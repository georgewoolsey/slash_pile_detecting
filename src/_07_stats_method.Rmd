# Statistical Testing of Method Settings{#stats_method}

In this prior [section](#allthetests) we performed sensitivity testing of our slash pile detection method using multiple CHM raster resolutions and ended up testing tens of thousands of possible parameterizations and input data combinations. using the tested parameter and data input combinations, we're going build statistical models to quantify the influence of these parameters and input data on pile detection and quantification accuracy. we'll utilize a Bayesian modelling framework which will allow us to probabilistically quantify parameter influence while accounting for uncertainty.

here are some of the hypotheses about the slash pile detection methodology that we'll be exploring:

1) does CHM resolution influences detection and quantification accuracy?
2) does the effect of CHM resolution change based on the inclusion of spectral data versus using only structural data?
3) does the use of spectral data have a meaningful impact on detection and quantification accuracy

our analysis data set will be the `param_combos_spectral_ranked` data which includes accuracy measurements at the parameter combination level using both structural data only as well as structural and spectral data in our data fusion approach. in this data, a row is unique by the full set of parameters and input data tested: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`, `chm_res_m`, `spectral_weight`. there are four structural parameters: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct` which are used to determine candidate slash piles from the CHM data alone, and the `chm_res_m` and `spectral_weight` parameters represent the input data with `spectral_weight` classifying if spectral data was not used (i.e. `spectral_weight` = 0), or if spectral data was used, what the weighting of that spectral data was on a 1-5 scale where the number represents the number of individual spectral index thresholds that must be met for a candidate pile detected from the structural data to be kept. for example, a value of "5" requires that all spectral criteria be met and will result in more candidate piles being filtered out than a value of "3". See [this section](#data_fusion) for full details on the data fusion approach.

we'll read in the sensitivity test result data which includes point estimates of detection and form quantification accuracy if it's not already in memory

```{r}
if( length(ls()[grep("param_combos_ranked",ls())])!=1 ){
  param_combos_ranked <- readr::read_csv(file.path("../data", "param_combos_ranked.csv"), progress = F, show_col_types = F)
}
if( length(ls()[grep("param_combos_spectral_ranked",ls())])!=1 ){
  param_combos_spectral_ranked <- readr::read_csv(file.path("../data", "param_combos_spectral_ranked.csv"), progress = F, show_col_types = F)
}
# convert spectral weight to factor for modelling
param_combos_spectral_ranked <- param_combos_spectral_ranked %>% 
  dplyr::mutate(
    spectral_weight = factor(spectral_weight)
    # replace 0 F-score with very small positive to run GLM models
    , f_score = ifelse(f_score==0,1e-4,f_score)
  )

# check out this data
param_combos_spectral_ranked %>% 
  dplyr::select(
    max_ht_m, max_area_m2, convexity_pct, circle_fit_iou_pct, chm_res_m
    , spectral_weight, spectral_weight_fact
    , f_score, precision, recall
    , tidyselect::ends_with("_mape")
  ) %>% 
  dplyr::glimpse()
```

a row is unique by the full set of parameters tested: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`, `chm_res_m`, `spectral_weight`

```{r}
# a row is unique by max_ht_m, max_area_m2, convexity_pct, circle_fit_iou_pct, chm_res_m, and spectral_weight
identical(
  param_combos_spectral_ranked %>% dplyr::distinct(max_ht_m, max_area_m2, convexity_pct, circle_fit_iou_pct, chm_res_m, spectral_weight) %>% nrow()
  , param_combos_spectral_ranked %>% nrow()
)
```

here are the number of records which returned valid predicted slash pile polygons by CHM resolution and data input setting (i.e. structural only versus data fusion). the number of records for the data fusion approach ("structural+spectral") should be roughly five times the number of records as the structural only approach because we tested five different settings of the `structural_weight` parameter from the lowest weighting of the spectral data of "1" (only one spectral index threshold must be met) to the highest weighting of spectral data "5" (all spectral index thresholds must be met)

```{r, fig.height=8.5}
param_combos_spectral_ranked %>% 
  dplyr::count(chm_res_m_desc,spectral_weight_fact) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x=n,y=spectral_weight_fact, color = spectral_weight_fact, fill = spectral_weight_fact)) +
    ggplot2::geom_col(width = 0.6) +
    ggplot2::geom_text(
      mapping = ggplot2::aes(label=scales::comma(n)) 
      , color = "black", size = 3
      , hjust = -0.1
    ) +
    ggplot2::facet_grid(rows = dplyr::vars(chm_res_m_desc)) +
    harrypotter::scale_fill_hp_d(option = "slytherin") +
    harrypotter::scale_color_hp_d(option = "slytherin") +
    ggplot2::scale_x_continuous(labels = scales::comma, expand = ggplot2::expansion(mult = c(0,.1))) +
    ggplot2::labs(y="") +
    ggplot2::theme_light() +
    ggplot2::theme(
      legend.position = "none"
      , strip.text.y = ggplot2::element_text(size = 9, color = "black", face = "bold")
    )
    
```

## Bayesian GLM - F-score

given that our data contains only one observation per parameter combination, we're going to use a Bayesian Beta generalized linear model (GLM) to ensure a statistically sound approach and interpretable relationships between each parameter and the dependent variable (e.g. F-score). our model will treat the parameters as a mix of continuous and nominal variables, preventing model saturation (where the model has as many parameters to estimate as data points, so the data perfectly explains the model). A Bayesian hierarchical model would not be appropriate for this structure, since it is designed for datasets with nested or grouped observations (e.g. if we had evaluated the method across different plots or study sites).

Our Bayesian Beta regression models the F-score with a Beta distribution because it is a proportion between 0 and 1, which ensures that the predictions and uncertainty estimates are always within the valid range. We're treating the four structural parameters (e.g. `max_ht_m` and `circle_fit_iou_pct`) and the CHM resolution (`chm_res_m`) as metric (i.e., continuous) variables, as this is statistically sound for our data and allows for a continuous interpretation where the model coefficient will represent the change in F-score for a one-unit change in the parameter value. The `spectral_weight` parameter, however, will be treated as nominal to capture its discrete effects without assuming a linear relationship. 

### Model selection

we're going to use a sub-sample of the data to perform model testing. our objective is to construct the model such that it faithfully represents the data.

we reviewed the main effect parameter trends against F-score [here](#detect_trends) and used these to guide our model design. we'll follow Kurz [2025](https://bookdown.org/content/3686/18.html) and compare our models with the LOO information criterion

> Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower estimates.

```{r}
# subsample data
set.seed(222)
ms_df_temp <- param_combos_spectral_ranked %>% dplyr::slice_sample(prop = 0.11)
# mcmc setup
iter_temp <- 2444
warmup_temp <- 1222
chains_temp <- 4
####################################################################
# base model with form selected based on main effect trends
####################################################################
fscore_mod1_temp <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Beta(link = "logit")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "fscore_mod1_temp")
)
fscore_mod1_temp <- brms::add_criterion(fscore_mod1_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by chm_res_m and vice-versa
####################################################################
fscore_mod2_temp <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Beta(link = "logit")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "fscore_mod2_temp")
)
fscore_mod2_temp <- brms::add_criterion(fscore_mod2_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by convexity_pct and vice-versa
####################################################################
fscore_mod3_temp <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    circle_fit_iou_pct:convexity_pct + # changed from base model
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Beta(link = "logit")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "fscore_mod3_temp")
)
fscore_mod3_temp <- brms::add_criterion(fscore_mod3_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
fscore_mod4_temp <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    circle_fit_iou_pct:convexity_pct + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m:convexity_pct + # changed from base model
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Beta(link = "logit")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "fscore_mod4_temp")
)
fscore_mod4_temp <- brms::add_criterion(fscore_mod4_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m. quadratic convexity_pct
####################################################################
fscore_mod5_temp <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    convexity_pct + I(convexity_pct^2) +  # changed from base model
    circle_fit_iou_pct:convexity_pct + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m:convexity_pct + # changed from base model
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Beta(link = "logit")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "fscore_mod5_temp")
)
fscore_mod5_temp <- brms::add_criterion(fscore_mod4_temp, criterion = "loo")
```

compare our models with the LOO information criterion. with the `brms::loo_compare()` function, we can compute a formal difference score between models with the output rank ordering the models such that the best fitting model appears on top. all models also receive a difference score relative to the best model and a standard error of the difference score

```{r}
brms::loo_compare(fscore_mod1_temp, fscore_mod2_temp, fscore_mod3_temp, fscore_mod4_temp, fscore_mod5_temp) %>% 
  kableExtra::kbl(caption = "F-score model selection with LOO information criterion") %>% 
  kableExtra::kable_styling()
```

we can also look at the AIC-type model weights

```{r,eval=F}
brms::model_weights(fscore_mod1_temp, fscore_mod2_temp, fscore_mod3_temp, fscore_mod4_temp, fscore_mod5_temp) %>% 
  round(digits = 4)
```

we can also quickly look at the Bayeisan $R^2$ returned from the `brms::bayes_R2()` function

```{r}
dplyr::bind_rows(
  brms::bayes_R2(fscore_mod1_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "fscore_mod1_temp")
  , brms::bayes_R2(fscore_mod2_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "fscore_mod2_temp")
  , brms::bayes_R2(fscore_mod3_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "fscore_mod3_temp")
  , brms::bayes_R2(fscore_mod4_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "fscore_mod4_temp")
  , brms::bayes_R2(fscore_mod5_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "fscore_mod5_temp")
) %>% 
  dplyr::mutate(mod = factor(mod)) %>% 
  ggplot2::ggplot(mapping=ggplot2::aes(y=R2, x = mod)) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
  ) +
  # ggplot2::facet_grid(cols = dplyr::vars(spectral_weight)) +
  # ggplot2::scale_fill_manual(values = pal_chm_res_m) +
  ggplot2::labs(x = "", y = "Bayesian R-squared") +
  ggplot2::theme_light()

```

the more complex models were selected as the best. while our model evaluation indicated that the more parsimonious model with fewer parameters was comparable to the most complex model tested, we'll the more complex model (`fscore_mod5_temp`) model for our based on the AIC-type model weights. because the selected model includes a quadratic term and multiple interactions parameter interpretation will be a challenge, so we will have to rely on plotting the modeled relationships rather than trying to interpret the coefficients.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Modeling

the four structural parameters (e.g. `max_ht_m` and `circle_fit_iou_pct`) and the CHM resolution (`chm_res_m`) as metric (i.e., continuous) variables. we include an interaction between `chm_res_m` and `spectral_weight` to directly compare the effect of CHM resolution with and without the use of spectral data.

we'll generally follow Kurz ([2023a](https://bookdown.org/content/3890/multivariate-linear-models.html); [2023b](https://bookdown.org/content/3890/interactions.html); [2025](https://bookdown.org/content/3686/18.html) for multiple linear regression model building using the `brms` Bayesian model framework based on [McElreath (2015, Ch. 5,7)](https://xcelab.net/rm/) and [Kruschke (2015, Ch. 18)](https://sites.google.com/site/doingbayesiandataanalysis/)

the fully factored Bayesian statistical model that details the likelihood, linear model, and priors used is:

\begin{align*}
\text{F-score}_i \sim & \operatorname{Beta}(\mu_{i}, \phi) \\
\operatorname{logit}(\mu_i) = & (\beta_1 \cdot \text{max_ht_m}_i) + (\beta_2 \cdot \text{max_area_m2}_i) \\
& + (\beta_3 \cdot \text{circle_fit_iou_pct}_i) + (\beta_4 \cdot (\text{circle_fit_iou_pct}_i)^2) \\
& + (\beta_5 \cdot \text{convexity_pct}_i) + (\beta_6 \cdot (\text{convexity_pct}_i)^2) \\
& + (\beta_7 \cdot \text{chm_res_m}_i) \\
& + \sum_{j=0}^{5} \left( \beta_{8, j} \cdot \mathbf{I}(\text{spectral_weight}_i = j) \right) \\
& + (\beta_9 \cdot \text{circle_fit_iou_pct}_i \cdot \text{convexity_pct}_i) \\
& + (\beta_{10} \cdot \text{circle_fit_iou_pct}_i \cdot \text{chm_res_m}_i) \\
& + (\beta_{11} \cdot \text{convexity_pct}_i \cdot \text{chm_res_m}_i) \\
& + \sum_{j=0}^{5} \left( \beta_{12, j} \cdot \text{chm_res_m}_i \cdot \mathbf{I}(\text{spectral_weight}_i = j) \right) \\
& + (\beta_{13} \cdot \text{circle_fit_iou_pct}_i \cdot \text{chm_res_m}_i \cdot \text{convexity_pct}_i) \\
\beta_k \sim & \operatorname{Normal}(0, \sigma_k) \quad \text{for } k = 0, \dots, 13 \\
\sigma_k \sim & \operatorname{Student T}(3,0,2.5) \quad \text{for } k = 0, \dots, 13 \\
\phi \sim & \operatorname{Gamma}(0.01,0.01) \\ 
\end{align*}

where, $i$ represents a single observation in the dataset which corresponds to a specific combination of the six parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`, `chm_res_m`, and `spectral_weight`) and its resulting F-score. Where `k` is used to index the different beta coefficients, which correspond to the intercept and the effects of each of the independent variables and their interactions and `j` denotes the specific level of the nominal (i.e. categorical) predictor `spectral_weight`

```{r, include = F, eval=FALSE, message = F, results = F, show.fig = T}
#install.packages("GGally", dependencies = T)
library(GGally)
GGally::ggpairs(
  data = param_combos_spectral_ranked %>% dplyr::slice_sample(prop = 0.2)
  , columns = c(
    "max_ht_m", "max_area_m2", "convexity_pct", "circle_fit_iou_pct", 
    "chm_res_m", "spectral_weight"
  )
) + 
theme(panel.grid = element_blank())

# interaction between convexity_pct, circle_fit_iou_pct?
param_combos_spectral_ranked %>% 
  dplyr::group_by(convexity_pct, circle_fit_iou_pct) %>% 
  dplyr::summarise(f_score = median(f_score)) %>% 
  ggplot(aes(x = convexity_pct, y = f_score, color = factor(circle_fit_iou_pct))) +
  geom_point() +
  geom_line()

# interaction between convexity_pct, circle_fit_iou_pct?
param_combos_spectral_ranked %>% 
  dplyr::group_by(convexity_pct, circle_fit_iou_pct) %>% 
  dplyr::summarise(f_score = median(f_score)) %>% 
  ggplot(aes(x = circle_fit_iou_pct, y = f_score, color = factor(convexity_pct))) +
  geom_point() +
  geom_line()

# interaction between convexity_pct, max_area_m2?
param_combos_spectral_ranked %>% 
  dplyr::group_by(convexity_pct, max_area_m2) %>% 
  dplyr::summarise(f_score = median(f_score)) %>% 
  ggplot(aes(x = max_area_m2, y = f_score, color = factor(convexity_pct))) +
  geom_point() +
  geom_line()

param_combos_spectral_ranked %>% 
  dplyr::slice_sample(prop = 0.22) %>% 
  ggplot(aes(x = convexity_pct, y = f_score, color = factor(chm_res_m))) +
  geom_point() +
  # facet_wrap(vars(circle_fit_iou_pct)) + # convexity_pct
  # geom_smooth(method = "loess", se = F)
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = F)

```

let's fit the model using the `brms` framework to fit Bayesian regression models using the Stan probabilistic programming language. if we want to set the prior for $\beta_0$ given a non-centered predictors, then we need to use the `0 + Intercept` syntax to fit the model (see Kurz [2025](https://solomonkurz.netlify.app/blog/2025-07-17-learn-stan-with-brms-part-iii/) for full discussion), but we'll just fit the model with the `brsm::brm()` default settings which automatically mean centers the predictors and also set the intercept to `0` so that we get explicit coefficient estimates for each level of our `spectral_weight` nominal variable which determines the intercept in this model

The table below details the terms used in our Bayesian GLM model defined in the `brms::brm()` call:

| Term in Formula | Type of Effect | Description of Relationship Tested |
| :--- | :--- | :--- |
| **`0 +`** | Zero Intercept | Specifies that the model is fit without a global intercept (baseline is determined by the combination of all factor levels). |
| **`max_ht_m`** | Main Effect (Linear) | Tests the direct, isolated linear influence of the maximum pile height threshold on the F-score. |
| **`max_area_m2`** | Main Effect (Linear) | Tests the direct, isolated linear influence of the maximum pile area threshold on the F-score. |
| **`chm_res_m`** | Main Effect (Linear) | Tests the direct, isolated linear influence of the input Canopy Height Model (CHM) resolution on the F-score. |
| **`spectral_weight`** | Main Effect (Factor) | The model estimates a separate coefficient for each of the six spectral weight levels (0 through 5). This coefficient represents the estimated mean F-score for that specific spectral weight level when all continuous variables are zero. |
| **`circle_fit_iou_pct`** | Main Effect (Linear) | Tests the direct linear influence of the pile's circular conformity threshold on the F-score. |
| **`convexity_pct`** | Main Effect (Linear) | Tests the direct linear influence of the pile's boundary smoothness (convexity) threshold on the F-score. |
| **`I(circle_fit_iou_pct^2)`** | Nonlinear (Quadratic) | Models a curved relationship where the F-score may peak or bottom out at an intermediate threshold for pile circularity. |
| **`I(convexity_pct^2)`** | Nonlinear (Quadratic) | Models a curved relationship where the F-score may peak or bottom out at an intermediate threshold for pile boundary smoothness. |
| **`circle_fit_iou_pct:convexity_pct`** | Two-Way Interaction | Captures how the optimal balance between pile circular conformity and boundary smoothness changes for the F-score. |
| **`chm_res_m:spectral_weight`** | Two-Way Interaction (Factor) | Captures how the effect of CHM resolution on the F-score changes across each of the six spectral weighting levels. |
| **`circle_fit_iou_pct:chm_res_m`** | Two-Way Interaction | Captures how the importance of the pile's circular conformity threshold changes as the input data resolution changes. |
| **`convexity_pct:chm_res_m`** | Two-Way Interaction | Captures how the sensitivity to the pile boundary smoothness threshold changes with the input data resolution. |
| **`circle_fit_iou_pct:chm_res_m:convexity_pct`** | Three-Way Interaction | The most complex term, showing how the combined effects of the circularity and convexity thresholds change simultaneously across different input CHM resolutions. |

```{r}
brms_f_score_mod <- brms::brm(
  formula = f_score ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) +
    convexity_pct + I(convexity_pct^2) +  # changed from base model
    circle_fit_iou_pct:convexity_pct + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m:convexity_pct + # changed from base model
    chm_res_m + spectral_weight + chm_res_m:spectral_weight
  , data = param_combos_spectral_ranked # %>% dplyr::slice_sample(prop = 0.33)
  , family = Beta(link = "logit")
  # , prior = c(
  #   brms::prior(student_t(3, 0, 5), class = "b")
  #   , brms::prior(gamma(0.01, 0.01), class = "phi")
  # )
  # mcmc
  , iter = 14000, warmup = 7000
  , chains = 4
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "brms_f_score_mod")
)
# brms::make_stancode(brms_f_score_mod)
# brms::prior_summary(brms_f_score_mod)
# print(brms_f_score_mod)
# brms::neff_ratio(brms_f_score_mod)
# brms::rhat(brms_f_score_mod)
# brms::nuts_params(brms_f_score_mod)
```

```{r, include=FALSE, eval=FALSE}
# check the prior distributions
# check priors
brms::prior_summary(brms_f_score_mod) %>%
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

```{r}
brms_f_score_mod %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | parameter == "phi"
  ) %>% 
  # dplyr::mutate(
  #   dplyr::across(
  #     dplyr::where(is.numeric)
  #     , ~ dplyr::case_when(
  #       stringr::str_ends(parameter,"_pct") ~ .x*0.01 # convert to percentage point change
  #       , T ~ .x
  #     )
  #   )
  # ) %>% 
  kableExtra::kbl(digits = 3, caption = "Bayesian model for F-score") %>% 
  kableExtra::kable_styling()
```

note the quadratic coefficients ending in `E2`, Kruschke (2015) provides some insight on how to interpret:

>A quadratic has the form $y = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$. When $\beta_{2}$ is zero, the form reduces to a line. Therefore, this extended model can produce any fit that the linear model can. When $\beta_{2}$ is positive, a plot of the curve is a parabola that opens upward. When $\beta_{2}$ is negative, the curve is a parabola that opens downward. We have no reason to think that the curvature in the family-income data is exactly a parabola, but the quadratic trend might describe the data much better than a line alone. (p. 496)

### Posterior Predictive Checks

Markov chain Monte Carlo (MCMC) simulations were conducted using the brms package (Bürkner 2017) to estimate posterior predictive distributions of the parameters of interest. We ran 4 chains of 14,000 iterations with the first 7,000 discarded as burn-in. Trace-plots were utilized to visually assess model convergence.

check the trace plots for problems with convergence of the Markov chains

```{r, include=T, eval=T, fig.height=6.6}
plot(brms_f_score_mod)
```

Sufficient convergence was checked with $\hat{R}$ values near 1 ([Brooks & Gelman, 1998](https://scholar.google.com/scholar?cluster=14209404114665352991&hl=en&as_sdt=0,6)). 

in the plot below, $\hat{R}$ values are colored using different shades (lighter is better). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: below 1.05 (good)
* mid: between 1.05 and 1.1 (ok)
* dark: above 1.1 (too high)

check our $\hat{R}$ values

```{r}
brms::mcmc_plot(brms_f_score_mod, type = "rhat_hist") +
  ggplot2::scale_x_continuous(breaks = scales::breaks_extended(n = 6)) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

and another check of our $\hat{R}$ values

```{r}
# and another check of our $\hat{R}$ values
brms_f_score_mod %>% 
  brms::rhat() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::rename(rhat = 2) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "phi"
  ) %>%
  dplyr::mutate(
    chk = (rhat <= 1*0.998 | rhat >= 1*1.002)
  ) %>% 
  ggplot(aes(x = rhat, y = parameter, color = chk, fill = chk)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray44", lwd = 1.2) +
  geom_vline(xintercept = 1*0.998, lwd = 1.5) +
  geom_vline(xintercept = 1*1.002, lwd = 1.5) +
  geom_vline(xintercept = 1*0.999, lwd = 1.2, color = "gray33") +
  geom_vline(xintercept = 1*1.001, lwd = 1.2, color = "gray33") +
  geom_point() +
  scale_fill_manual(values = c("navy", "firebrick")) +
  scale_color_manual(values = c("navy", "firebrick")) +
  scale_y_discrete(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\hat{R}$")
    , subtitle = latex2exp::TeX("MCMC chain convergence check for $\\hat{R}$ values")
    , title = "F-Score"
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , axis.text.y = element_text(size = 4)
    , panel.grid.major.x = element_blank()
    , panel.grid.minor.x = element_blank()
    , plot.subtitle = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )
```

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain **not** to the sample size of the data where acceptable values allow "for reasonably accurate and stable estimates of the limits of the 95% HDI...If accuracy of the HDI limits is not crucial for your application, then a smaller ESS may be sufficient" [(Kruschke 2015, p. 184)](https://sites.google.com/site/doingbayesiandataanalysis/)

Ratios of effective sample size (ESS) to total sample size with values are colored using different shades (lighter is better). A ratio close to "1" (no autocorrelation) is ideal, while a low ratio suggests the need for more samples or model re-parameterization. Efficiently mixing MCMC chains are important because they guarantee the resulting posterior samples accurately represent the true distribution of model parameters, which is necessary for reliable and precise estimation of parameter values and their associated uncertainties (credible intervals). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: between 0.5 and 1 (high)
* mid: between 0.1 and 0.5 (good)
* dark: below 0.1 (low)

```{r}
# and another effective sample size check
brms::mcmc_plot(brms_f_score_mod, type = "neff_hist") +
# brms::mcmc_plot(brms_f_score_mod, type = "neff") +
  ggplot2::scale_x_continuous(limits = c(0,NA), breaks = scales::breaks_extended(n = 9)) +
  # ggplot2::scale_color_discrete(drop = F) +
  # ggplot2::scale_fill_discrete(drop = F) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

our observed range of ESS to Total Sample Size ratios (~0.2 to ~0.8) are generally considered good to excellent, indicating the MCMC chains are performing well and mixing efficiently

```{r, include=FALSE, eval=FALSE}
# get ess values from model summary
summary(brms_f_score_mod) %>%
  purrr::pluck("fixed") %>%
  dplyr::as_tibble() %>%
  dplyr::rename(ess = Bulk_ESS) %>%
  dplyr::mutate(parameter = dplyr::row_number(), chk = ess<10000) %>%
  ggplot2::ggplot(ggplot2::aes(x = ess, y = parameter, color = chk, fill = chk)) +
  ggplot2::geom_vline(xintercept = 10000, linetype = "dashed", color = "gray44", lwd = 1.2) +
  ggplot2::geom_segment( ggplot2::aes(x = 0, xend=ess, yend=parameter), color="black") +
  ggplot2::geom_point() +
  ggplot2::scale_fill_manual(values = c("blue3","red3")) +
  ggplot2::scale_color_manual(values = c("blue3","red3")) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::scale_x_continuous(labels = scales::comma) +
  ggplot2::labs(
    x = "ESS"
    , subtitle = "MCMC chain resolution check for effective sample size (ESS) values"
    , y = ""
    , title = "F-Score"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.text.y = ggplot2::element_text(size = 4)
    , panel.grid.major.x = ggplot2::element_blank()
    , panel.grid.minor.x = ggplot2::element_blank()
    , plot.subtitle = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )

```

Posterior predictive checks were used to evaluate model goodness-of-fit by comparing data simulated from the model with the observed data used to estimate the model parameters ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)). Calculating the proportion of MCMC iterations in which the test statistic (i.e., mean and sum of squares) from the simulated data and observed data are more extreme than one another provides the Bayesian p-value. Lack of fit is indicated by a value close to 0 or 1 while a value of 0.5 indicates perfect fit ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)).

To learn more about this approach to posterior predictive checks, check out Gabry’s (2025) vignette, [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). 

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data. our objective is to construct the model such that it faithfully represents the data.

```{r}
# posterior predictive check
brms::pp_check(
    brms_f_score_mod
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  ggplot2::labs(subtitle = "posterior-predictive check (overlaid densities)") +
  ggplot2::theme_light() +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
    , plot.subtitle = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )
```

another way

```{r}
brms::pp_check(brms_f_score_mod, type = "ecdf_overlay", ndraws = 100) +
  ggplot2::labs(subtitle = "posterior-predictive check (ECDF: empirical cumulative distribution function)") + 
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
  )
```

```{r, include=FALSE, eval=F}
# and another posterior predictive check for the overall model combining mean and sd
brms::pp_check(brms_f_score_mod, type = "stat_2d", ndraws = 444) +
  ggplot2::theme_light() +
  ggplot2::labs(title = "F-Score") +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )
brms::pp_check(
    brms_f_score_mod
    , type = "stat"
    , stat = "mean"
    , ndraws = 888
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
brms::pp_check(
    brms_f_score_mod
    , type = "stat"
    , stat = "sd"
    , ndraws = 888
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sds") +
  theme_light()
# grouped
brms::pp_check(
    brms_f_score_mod
    , type = "dens_overlay_grouped" # ""
    , group = "spectral_weight"
    , ndraws = 111
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  theme_light()
# grouped
brms::pp_check(
    brms_f_score_mod
    , type = "dens_overlay_grouped" # ""
    , group = "chm_res_m"
    , ndraws = 111
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  theme_light()
# grouped
brms::pp_check(
    brms_f_score_mod
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "spectral_weight"
    , ndraws = 444
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
# grouped
brms::pp_check(
    brms_f_score_mod
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "chm_res_m"
    , ndraws = 888
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
```

### Conditional Effects

first, lets look at densities of the posterior samples per parameter

```{r}
brms::mcmc_plot(brms_f_score_mod, type = "dens") +
  # ggplot2::theme_light() +
  ggplot2::theme(
    strip.text = ggplot2::element_text(size = 7.5, face = "bold", color = "black")
  )
```

and we can look at the default coefficient plot that is commonly used in reporting coefficient "significance" in frequentist analysis

```{r}
# easy way to get the default coeff plot
brms::mcmc_plot(brms_f_score_mod, type = "intervals")
```

```{r, include=FALSE, eval=FALSE}
# easy way to get the default coeff plot
# brms::mcmc_plot(brms_f_score_mod)
# custom coeff plot
brms::as_draws_df(brms_f_score_mod) %>%
  dplyr::select(tidyselect::starts_with("b_")) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    name = stringr::str_remove(name, "\\bb_") %>% 
      stringr::str_remove("\\bI")
  ) %>% 
  dplyr::filter(
    name %in% c("max_ht_m" , "max_area_m2" , "convexity_pct" , "circle_fit_iou_pct" )
    | stringr::str_ends(name,"E2")
  ) %>% 
  dplyr::group_by(name) %>%
  dplyr::mutate(
    value = dplyr::case_when(
      stringr::str_ends(name,"_pct") ~ value*0.01 # convert to percentage point change
      , T ~ value # convert to percentage point change
    )
    , median_hdi_est = tidybayes::median_hdi(value)$y
    , median_hdi_lower = tidybayes::median_hdi(value)$ymin
    , median_hdi_upper = tidybayes::median_hdi(value)$ymax
  ) %>%
  dplyr::ungroup() %>%
  dplyr::select(-value) %>%
  dplyr::distinct() %>%
  dplyr::mutate(
    col_grp = dplyr::case_when(
      0>median_hdi_lower & 0<median_hdi_upper ~ "no effect"
      , median_hdi_est>0 ~ "positive effect"
      , median_hdi_est<0 ~ "negative effect"
    )
    , name = stringr::str_replace(name,"E2$$","^2")
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = median_hdi_est, y = name, color = col_grp)
    # mapping = ggplot2::aes(x = value, y = name)
  ) +
  ggplot2::geom_vline(xintercept = 0, color = "black") +
  ggplot2::geom_linerange(mapping = ggplot2::aes(xmin=median_hdi_lower, xmax=median_hdi_upper), lwd = 2) +
  ggplot2::geom_point(size = 4) +
  # tidybayes::stat_pointinterval(
  #   point_interval = median_hdi, .width = .95
  #   # , color = "firebrick"
  #   , point_size = 4, lwd = 7
  # ) +
  ggplot2::scale_color_manual(values = c("negative effect"="firebrick","no effect"="gray44","positive effect"="navy")) +
  ggplot2::labs(
    x = "estimate", y = ""
    , color = ""
    , caption = "*coefficient on variables in percent units represent the change in the dependent variable for a one percentage point increase"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  )
# easy way to get the default coeff plot
# brms::mcmc_plot(brms_f_score_mod)
################or
################or
################or
# custom coeff plot
struct_draws_temp <- 
  brms::as_draws_df(brms_f_score_mod) %>%
  # dplyr::select(b_circle_fit_iou_pct) %>% ggplot(aes(x=b_circle_fit_iou_pct)) + tidybayes::stat_halfeye()
  dplyr::select(tidyselect::starts_with("b_")) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    name = stringr::str_remove(name, "\\bb_") %>% 
      stringr::str_remove("\\bI")
  ) %>% 
  dplyr::filter(
    name %in% c("max_ht_m" , "max_area_m2" , "convexity_pct" , "circle_fit_iou_pct" )
    | stringr::str_ends(name,"E2")
  ) %>% 
  dplyr::group_by(name) %>%
  dplyr::mutate(
    value = dplyr::case_when(
      stringr::str_ends(name,"_pct") ~ value*0.01 # convert to percentage point change
      , T ~ value # convert to percentage point change
    )
    , median_hdi_est = tidybayes::median_hdi(value)$y
    , median_hdi_lower = tidybayes::median_hdi(value)$ymin
    , median_hdi_upper = tidybayes::median_hdi(value)$ymax
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    col_grp = dplyr::case_when(
      # 0>median_hdi_lower & 0<median_hdi_upper ~ "no effect"
      median_hdi_est==0 ~ "no effect"
      , median_hdi_est>0 ~ "positive effect"
      , median_hdi_est<0 ~ "negative effect"
    )
    , name = stringr::str_replace(name,"E2$$","^2")
  )
# struct_draws_temp %>% dplyr::glimpse()
# struct_draws_temp %>% dplyr::count(name)


# plot each one individually to allow for unique axes
plt_list_temp <- struct_draws_temp$name %>% 
  unique() %>% 
  sort() %>% 
  purrr::map(\(x)
    struct_draws_temp %>% 
      dplyr::filter(name==x) %>% 
      ggplot2::ggplot(
        mapping = ggplot2::aes(x = value, fill = col_grp)
      ) +
      ggplot2::geom_vline(xintercept = 0, color = "black") +
      tidybayes::stat_halfeye(
        point_interval = median_hdi, .width = .95
        , alpha = 0.8
        # , color = "firebrick"
        # , point_size = 4, lwd = 7
      ) +
      ggplot2::facet_wrap(facets = dplyr::vars(name), scales = "free_x") +
      ggplot2::scale_y_continuous(NULL, breaks = NULL) +
      ggplot2::scale_color_manual(values = c("negative effect"="firebrick","no effect"="gray44","positive effect"="navy")) +
      ggplot2::scale_fill_manual(values = c("negative effect"="firebrick","no effect"="gray44","positive effect"="navy")) +
      ggplot2::labs(
        x = "coefficient", y = ""
        , color = "", fill = ""
      ) +
      ggplot2::theme_light() +
      ggplot2::theme(
        legend.position = "top"
        , axis.title.x = ggplot2::element_text(size = 7) 
        , strip.text = ggplot2::element_text(size = 10, face = "bold", color = "black") 
      ) +
      ggplot2::guides(
        fill = ggplot2::guide_legend(override.aes = list(shape = NA, linetype = 0, size = 5, alpha = 1))
        , shape = "none"
        , color = "none"
      )
  )
patchwork::wrap_plots(
    plt_list_temp
    , ncol = 2
    , guides = "collect"
  ) &
  ggplot2::theme(legend.position = "bottom")
```

Regarding interactions and polynomial models like this, [McElreath (2015)](https://xcelab.net/rm/) notes:

>parameters are the linear and square components of the curve, respectively. But that doesn’t make them transparent. You have to plot these model fits to understand what they are saying. (p. 112-113)

all of the interactions and the quadradic trend of this model combine to make these coefficients by themselves uninterpretable as the coefficients are only meaningful in the context of the other terms in the interaction or by adding the quadratic component 

we can do this by checking for the main effects of the individual variables on F-score (averages across all other effects)

```{r}
brms::conditional_effects(brms_f_score_mod)
### ggplot version
# brms::conditional_effects(brms_f_score_mod) %>% 
#   purrr::pluck("max_ht_m") %>% 
#   ggplot(aes(x = max_ht_m)) +
#   geom_ribbon(aes(ymin = lower__, ymax = upper__), fill = "blue", alpha = 0.2) +
#   geom_line(aes(y = estimate__), color = "blue") +
#   labs(
#     x = "max_ht_m",
#     y = "F-score",
#     title = "Conditional Effects"
#   )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

```{r, include=FALSE, eval=FALSE}
# question 1: *does CHM resolution influences detection accuracy?*
brms::as_draws_df(brms_f_score_mod) %>%
  dplyr::select(tidyselect::starts_with("b_")) %>% 
  dplyr::mutate(
    dplyr::across(
      .cols = tidyselect::contains(":spectral_weight")
      , ~ .x + b_chm_res_m
    )
  ) %>% 
  dplyr::select(tidyselect::starts_with("b_chm_res_m")) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    name = stringr::str_remove(name, "\\bb_")
    , spectral_weight = 
      dplyr::case_when(
        # get the last character which is the spectral weight
        stringr::str_detect(name, ":spectral_weight") ~ stringr::str_sub(name, -1, -1)
        , T ~ "0"
      ) %>% 
      as.numeric() %>% 
      factor() %>% 
      forcats::fct_rev()
  ) %>% 
  dplyr::group_by(spectral_weight) %>%
  dplyr::mutate(
    median_hdi_est = tidybayes::median_hdi(value)$y
    , median_hdi_lower = tidybayes::median_hdi(value)$ymin
    , median_hdi_upper = tidybayes::median_hdi(value)$ymax
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    col_grp = dplyr::case_when(
      # 0>median_hdi_lower & 0<median_hdi_upper ~ "no effect"
       median_hdi_est==0 ~ "no effect"
      , median_hdi_est>0 ~ "positive effect"
      , median_hdi_est<0 ~ "negative effect"
    )
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = value, y = spectral_weight, fill = col_grp)
    # mapping = ggplot2::aes(x = value, y = name)
  ) +
  ggplot2::geom_vline(xintercept = 0, color = "black") +
  # ggplot2::geom_linerange(mapping = ggplot2::aes(xmin=median_hdi_lower, xmax=median_hdi_upper), lwd = 2) +
  tidybayes::stat_halfeye(
    point_interval = median_hdi, .width = .95
    , alpha = 0.8
    # , color = "firebrick"
    # , point_size = 4, lwd = 7
  ) +
  ggplot2::scale_color_manual(values = c("negative effect"="firebrick","no effect"="gray44","positive effect"="navy")) +
  ggplot2::scale_fill_manual(values = c("negative effect"="firebrick","no effect"="gray44","positive effect"="navy")) +
  ggplot2::labs(
    x = "slope of CHM resolution (`chm_res_m`)", y = "spectral_weight"
    , color = "", fill = ""
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "bottom"
    , axis.title.x = ggplot2::element_text(size = 7) 
  ) +
  ggplot2::guides(
    fill = ggplot2::guide_legend(override.aes = list(shape = NA, linetype = 0, size = 5, alpha = 1))
    , shape = "none"
    , color = "none"
  )
```

```{r, include=FALSE, eval=FALSE}
# question 3: *does the use of spectral data have a meaningful impact on detection and quantification accuracy*
brms::as_draws_df(brms_f_score_mod) %>%
  dplyr::select(tidyselect::starts_with("b_spectral_weight")) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    name = stringr::str_remove(name, "\\bb_")
    , spectral_weight = stringr::str_sub(name, -1, -1) %>% 
      as.numeric() %>% 
      factor() %>% 
      forcats::fct_rev()
  ) %>% 
  dplyr::group_by(spectral_weight) %>%
  dplyr::mutate(
    median_hdi_est = tidybayes::median_hdi(value)$y
    , median_hdi_lower = tidybayes::median_hdi(value)$ymin
    , median_hdi_upper = tidybayes::median_hdi(value)$ymax
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    col_grp = dplyr::case_when(
      # 0>median_hdi_lower & 0<median_hdi_upper ~ "no effect"
      median_hdi_est==0 ~ "no"
      , median_hdi_est>0 ~ "positive"
      , median_hdi_est<0 ~ "negative"
    )
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = value, y = spectral_weight, fill = col_grp)
    # mapping = ggplot2::aes(x = value, y = name)
  ) +
  ggplot2::geom_vline(xintercept = 0, color = "black") +
  tidybayes::stat_halfeye(
    point_interval = median_hdi, .width = .95
    , alpha = 0.8
    # , color = "firebrick"
    # , point_size = 4, lwd = 7
  ) +
  ggplot2::scale_color_manual(values = c("negative"="firebrick","no"="gray44","positive"="navy")) +
  ggplot2::scale_fill_manual(values = c("negative"="firebrick","no"="gray44","positive"="navy")) +
  ggplot2::labs(
    x = "intercept", y = "spectral_weight"
    , color = "", fill = ""
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.title.x = ggplot2::element_text(size = 7) 
  ) +
  ggplot2::guides(
    fill = ggplot2::guide_legend(override.aes = list(shape = NA, linetype = 0, size = 5, alpha = 1))
    , shape = "none"
    , color = "none"
  )
```

### Posterior Predictive Expectation{#f_score_epred}

we will test our hypotheses using the posterior distributions of the expected values (i.e., the posterior predictions of the mean) obtained via `tidybayes::add_epred_draws()`. our analysis will include two stages using parameter levels of the four structural parameters: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`. in practice, these values should be informed by the treatment and slash pile construction prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

```{r}
# let's fix the structural parameters based on expectations from the prescription
structural_params_settings <- 
  dplyr::tibble(
    max_ht_m = 2.3
    , max_area_m2 = 46
  )
  # dplyr::bind_rows(
  #   # structural only
  #   param_combos_ranked %>% dplyr::filter(is_top_overall) %>% dplyr::select(max_ht_m,max_area_m2,convexity_pct,circle_fit_iou_pct) 
  #   # fusion
  #   , param_combos_spectral_ranked %>% 
  #     dplyr::ungroup() %>% 
  #     dplyr::filter(is_top_overall & spectral_weight!=0) %>% 
  #     dplyr::arrange(ovrall_balanced_rank) %>% # same number of records as structural only
  #     dplyr::filter(dplyr::row_number()<=sum(param_combos_ranked$is_top_overall)) %>% 
  #     dplyr::select(max_ht_m,max_area_m2,convexity_pct,circle_fit_iou_pct)
  # ) %>% 
  #   tidyr::pivot_longer(
  #     cols = dplyr::everything()
  #     , names_to = "metric"
  #     , values_to = "value"
  #   ) %>% 
  #   dplyr::count(metric, value) %>% 
  #   dplyr::group_by(metric) %>% 
  #   dplyr::arrange(metric,desc(n),value) %>% 
  #   dplyr::slice(1) %>% 
  #   dplyr::select(-n) %>% 
  #   tidyr::pivot_wider(names_from = metric, values_from = value) %>% 
  #   dplyr::mutate(is_top_overall = T) %>% 
  # # we just need `max_ht_m`, `max_area_m2`
  # dplyr::select(max_ht_m, max_area_m2)
  
# huh?
structural_params_settings %>% dplyr::glimpse()
```

now we'll get the posterior predictive draws but over a range of `circle_fit_iou_pct` and `convexity_pct` including the best setting

```{r}
seq_temp <- seq(from = 0.05, to = 1.0, by = 0.1)
seq2_temp <- seq_temp[seq(1, length(seq_temp), by = 2)] # get every other element
# draws
draws_temp <- 
  # get the draws for levels of 
  # spectral_weight circle_fit_iou_pct convexity_pct
  tidyr::crossing(
    param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight)
    , circle_fit_iou_pct = seq_temp
    , convexity_pct = seq_temp
    , chm_res_m = seq(from = 0.1, to = 1.0, by = 0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  # dplyr::glimpse()
  tidybayes::add_epred_draws(brms_f_score_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(
    is_seq = (convexity_pct %in% seq_temp) & (circle_fit_iou_pct %in% seq_temp)
  )
# # huh?
draws_temp %>% dplyr::glimpse()
```

#### Geometric shape regularity

let's look at the influence of the parameters that control the geometric shape regularity filtering: `circle_fit_iou_pct` and `convexity_pct`. to do this, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

##### `circle_fit_iou_pct`

we need to look at the influence of `circle_fit_iou_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.1)
    , convexity_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(convexity_pct = factor(convexity_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct, y = value, color = convexity_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.95)
    , lwd = 1.1, fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "mako", begin = 0.6, end = 0.1) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `circle_fit_iou_pct` on F-score"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`circle_fit_iou_pct`"
    , y = "F-score"
    , color = "`convexity_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )

```

the non-linear relationship between the `circle_fit_iou_pct` parameter and the F-score is prominent, confirmed by the model's quadratic coefficient. this trend is consistent across all levels of spectral_weight and CHM resolution. this figure also demonstrates how, within the optimal range of `circle_fit_iou_pct`, the intermediate values of the `convexity_pct` parameter seem to result in the best detection accuracy across CHM resolution levels. Across all CHM resolutions and spectral weighting levels, setting `circle_fit_iou_pct` too high (e.g > 0.7) results in steep declines in detection accuracy.

##### `convexity_pct`

we need to look at the influence of `convexity_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.1)
    , circle_fit_iou_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(circle_fit_iou_pct = factor(circle_fit_iou_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct, y = value, color = circle_fit_iou_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.5,0.95)
    , lwd = 1.1
    , fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "magma", begin = 0.5, end = 0.1) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `convexity_pct` on F-score"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`convexity_pct`"
    , y = "F-score"
    , color = "`circle_fit_iou_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )

```

As expected, the model shows that the influence of the `convexity_pct` parameter on the F-score is conditional on the `circle_fit_iou_pct` setting. When `circle_fit_iou_pct` is set too high (requiring nearly perfectly circular pile perimeters), `convexity_pct` has a minimal impact on the F-score. When `circle_fit_iou_pct` is set to an optimal value near its vertex (e.g., ~0.25-0.55), the range of `convexity_pct` values below 0.5 result in the best pile detection for finer resolution CHM data (e.g. < 0.3 m) but for coarser resolution CHM data the optimal `convexity_pct` setting increases to ~0.37-0.62. Across all CHM resolutions and spectral weighting levels, setting `convexity_pct` too high (e.g > 0.7) results in steep declines in detection accuracy.

##### Optimizing geometric filtering

Given the complexity of our model, which includes a non-linear link function and parameter interactions, calculating the optimal parameter values by solving for them algebraically from the model's coefficients would be prone to error. instead, we can use a robust Bayesian approach that leverages the model's posterior predictive distribution. This method is powerful because it inherently accounts for all sources of model uncertainty.

first, we'll generate a large number of predictions across a fine grid of parameter values (e.g. in steps of 0.01) for each posterior draw of the model coefficients. we'll generate a large number (e.g. 1000+) of posterior predictive draws for each combination of parameter values. for each posterior predictive draw, we'll then identify the parameter combination that maximizes the F-score and we'll be left with a posterior distribution of optimal parameter combinations.

this approach demonstrates a key advantage of the Bayesian framework, allowing us to ask complex questions and find the most probable optimal parameter combination while fully accounting for uncertainty.

```{r}
# let's get the draws at a very granular level
vertex_draws_temp <- 
  tidyr::crossing(
    param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight, spectral_weight_fact)
    , circle_fit_iou_pct = seq(from = 0.0, to = 1, by = 0.01) # very granular to identify vertex
    , convexity_pct = seq(from = 0.0, to = 1, by = 0.01) # very granular to identify vertex
    , chm_res_m = seq(0.1,0.5,by=0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  tidybayes::add_epred_draws(brms_f_score_mod, ndraws = 1000, value = "value") %>% 
  dplyr::ungroup() %>% 
  # for each draw, get the highest f-score by chm_res_m, spectral_weight
  # which we'll use to identify the optimal circle_fit_iou_pct,convexity_pct settings
  # these are essentially "votes" based on likelihood
  dplyr::group_by(
    .draw
    , chm_res_m, spectral_weight
  ) %>% 
  dplyr::arrange(desc(value),circle_fit_iou_pct,convexity_pct) %>% 
  dplyr::slice(1)
# vertex_draws_temp %>% dplyr::glimpse() # this thing is huge
```

plot the posterior distribution of optimal parameter setting for `circle_fit_iou_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>%
  # dplyr::filter(chm_res_m==0.4) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  # ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1), breaks = scales::breaks_extended(6)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8)
  )
```

our model is very confident that the optimal `circle_fit_iou_pct` for maximizing detection accuracy is in a narrow range, just look at that x-axis scale

plot the posterior distribution of optimal parameter setting for `convexity_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  # ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1), breaks = scales::breaks_extended(6)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

when the `circle_fit_iou_pct` parameter is optimized, the influence of `convexity_pct` on detection accuracy is dependent on the CHM resolution. for coarser CHM data (>0.3 m), the model's predictions indicate with high certainty that the optimal `convexity_pct` is higher (i.e. requiring more smooth boundaries) than the optimal `convexity_pct` setting (i.e. requiring less smooth boundaries) for finer resoultion CHM data. this finding is consistent with our previous results and provides additional evidence that the smoothing effect of coarser resolution CHM data makes `convexity_pct` most effective at filtering irregular objects only when it is set to an intermediate level (e.g. ~0.5).

we can look at this another way, check it

```{r, fig.height=9}
vertex_draws_temp %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y = convexity_pct, x = circle_fit_iou_pct)) +
  # geom_point(alpha=0.2) +
  ggplot2::geom_jitter(alpha=0.2, height = .01, width = .01) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    # , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  # ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1), breaks = scales::breaks_extended(6)) +
  # ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
    , axis.text = ggplot2::element_text(size = 8)
  )
```

note, in the plot above, we slightly "jitter" the points so that they are visible where they would otherwise be stacked on top of each other and only look like a few points instead of the 1000 draws from the posterior we used

let's table the HDI of the optimal values

```{r}
# summarize it
table_vertex_draws_temp <- vertex_draws_temp %>% 
  dplyr::group_by(
    chm_res_m, spectral_weight
  ) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$y
    , median_hdi_lower_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymin
    , median_hdi_upper_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymax
    # get median_hdi
    , median_hdi_est_convexity_pct = tidybayes::median_hdci(convexity_pct)$y
    , median_hdi_lower_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymin
    , median_hdi_upper_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymax
  ) %>%
  dplyr::ungroup() 
# table it
table_vertex_draws_temp %>% 
  kableExtra::kbl(
    digits = 2
    , caption = ""
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , rep(c("median", "HDI low", "HDI high"),2)
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "circle_fit_iou_pct" = 3
    , "convexity_pct" = 3
  )) %>% 
  kableExtra::scroll_box(height = "8in")
```

to fix our structural parameter levels so that we can continue to explore the influence of the input data, we'll select the median of the optimal setting of `circle_fit_iou_pct` and `convexity_pct` by CHM resolution levels and spectral weighting. This will result in no one CHM resolution and spectral weighting combination using it's optimal `circle_fit_iou_pct` and `convexity_pct` setting but will provide us with values in the plausible range of optimal settings that work across the CHM resolutions tested with empirical data.

```{r}
structural_params_settings <- 
  vertex_draws_temp %>% 
  dplyr::ungroup() %>% 
  dplyr::summarise(
    # get median_hdi
    circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$y
    # , median_hdi_lower_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymin
    # , median_hdi_upper_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymax
    # get median_hdi
    , convexity_pct = tidybayes::median_hdci(convexity_pct)$y
    # , median_hdi_lower_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymin
    # , median_hdi_upper_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymax
  ) %>% 
  dplyr::bind_cols(
    structural_params_settings
  )
# what?
structural_params_settings %>% dplyr::glimpse()
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Input data{#in_dta_det_acc_pred}

to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`), we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`). the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

we'll make contrasts of the posterior predictions to probabilistically quantify the influence of the input data (e.g. inclusion of spectral data and it's weighting and CHM resolution). before we do this we're going to borrow code from ([Tinkham and Woolsey 2024](https://georgewoolsey.github.io/uas_sfm_tree_detection/stats_validation.html#beta_mod)) to make and plot the Bayesian contrasts

```{r}
############################################
# make the variables for the contrast
############################################
make_contrast_vars <- function(my_data){
  my_data %>%
    dplyr::mutate(
      # get median_hdi
      median_hdi_est = tidybayes::median_hdci(value)$y
      , median_hdi_lower = tidybayes::median_hdci(value)$ymin
      , median_hdi_upper = tidybayes::median_hdci(value)$ymax
      # check probability of contrast
      , pr_gt_zero = mean(value > 0) %>% 
          scales::percent(accuracy = 1)
      , pr_lt_zero = mean(value < 0) %>% 
          scales::percent(accuracy = 1)
      # check probability that this direction is true
      , is_diff_dir = dplyr::case_when(
        median_hdi_est >= 0 ~ value > 0
        , median_hdi_est < 0 ~ value < 0
      )
      , pr_diff = mean(is_diff_dir)
      # make a label
      , pr_diff_lab = dplyr::case_when(
          median_hdi_est > 0 ~ paste0(
            "Pr("
            , stringr::word(contrast, 1, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ">"
            , stringr::word(contrast, 2, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ")="
            , pr_diff %>% scales::percent(accuracy = 1)
          )
          , median_hdi_est < 0 ~ paste0(
            "Pr("
            , stringr::word(contrast, 2, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ">"
            , stringr::word(contrast, 1, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ")="
            , pr_diff %>% scales::percent(accuracy = 1)
          )
        )
      # make a SMALLER label
      , pr_diff_lab_sm = dplyr::case_when(
        median_hdi_est >= 0 ~ paste0(
          "Pr(>0)="
          , pr_diff %>% scales::percent(accuracy = 1)
        )
        , median_hdi_est < 0 ~ paste0(
          "Pr(<0)="
          , pr_diff %>% scales::percent(accuracy = 1)
        )
      )
      , pr_diff_lab_pos = dplyr::case_when(
        median_hdi_est > 0 ~ median_hdi_upper
        , median_hdi_est < 0 ~ median_hdi_lower
      ) * 1.075
      , sig_level = dplyr::case_when(
        pr_diff > 0.99 ~ 0
        , pr_diff > 0.95 ~ 1
        , pr_diff > 0.9 ~ 2
        , pr_diff > 0.8 ~ 3
        , T ~ 4
      ) %>% 
      factor(levels = c(0:4), labels = c(">99%","95%","90%","80%","<80%"), ordered = T)
    )
}

############################################
# plot the contrast
############################################
plt_contrast <- function(
    my_data
    , x = "value"
    , y = "contrast"
    , fill = "pr_diff"
    , label = "pr_diff_lab"
    , label_pos = "pr_diff_lab_pos"
    , label_size = 3
    , x_expand = c(0.1, 0.1)
    , facet = NA
    , y_axis_title = ""
    , x_axis_title = "difference"
    , caption_text = "" # form_temp
    , annotate_size = 2.2
    , annotate_which = "both" # "both", "left", "right"
    , include_zero = T
    ) {
  # df for annotation
  get_annotation_df <- function(
        my_text_list = c(
          "Bottom Left (h0,v0)","Top Left (h0,v1)"
          ,"Bottom Right h1,v0","Top Right h1,v1"
          )
        , hjust = c(0,0,1,1) # higher values = right, lower values = left 
        , vjust = c(0,1.3,0,1.3) # higher values = down, lower values = up
    ){
      df = data.frame(
        xpos = c(-Inf,-Inf,Inf,Inf)
        , ypos =  c(-Inf, Inf,-Inf,Inf)
        , annotate_text = my_text_list
        , hjustvar = hjust
        , vjustvar = vjust
      )  
      return(df)
  }
  
  if(annotate_which=="left"){
    text_list <- c(
      "","L.H.S. < R.H.S."
      ,"",""
    )
  }else if(annotate_which=="right"){
    text_list <- c(
      "",""
      ,"","L.H.S. > R.H.S."
    )
  }else{
    text_list <- c(
      "","L.H.S. < R.H.S."
      ,"","L.H.S. > R.H.S."
    )
  }
  
  # plot base
  plt <-  
    my_data %>%
    ggplot(aes(x = .data[[x]], y = .data[[y]])) 
  
  if(include_zero){
    plt <- plt + geom_vline(xintercept = 0, linetype = "solid", color = "gray33", lwd = 1.1)
  }
  # plot meat
  plt <- plt +
      tidybayes::stat_halfeye(
        mapping = aes(fill = .data[[fill]])
        , point_interval = median_hdi, .width = c(0.5,0.95)
        # , slab_fill = "gray22", slab_alpha = 1
        , interval_color = "black", point_color = "black", point_fill = "black"
        , point_size = 0.9
        , justification = -0.01
      ) +
      geom_text(
        data = get_annotation_df(
          my_text_list = text_list
        )
        , mapping = aes(
          x = xpos, y = ypos
          , hjust = hjustvar, vjust = vjustvar
          , label = annotate_text
          , fontface = "bold"
        )
        , size = annotate_size
        , color = "gray30" # "#2d2a4d" #"#204445"
      ) + 
      # scale_fill_fermenter(
      #   n.breaks = 5 # 10 use 10 if can go full range 0-1
      #   , palette = "PuOr" # "RdYlBu"
      #   , direction = 1
      #   , limits = c(0.5,1) # use c(0,1) if can go full range 0-1
      #   , labels = scales::percent
      # ) +
      scale_fill_stepsn(
        n.breaks = 5 # 10 use 10 if can go full range 0-1
        , colors = harrypotter::hp(n=5, option="ravenclaw", direction = -1) # RColorBrewer::brewer.pal(11,"PuOr")[c(3,4,8,10,11)]
        , limits = c(0.5,1) # use c(0,1) if can go full range 0-1
        , labels = scales::percent
      ) +
      scale_x_continuous(expand = expansion(mult = x_expand)) +
      labs(
        y = y_axis_title
        , x = x_axis_title
        , fill = "Pr(contrast)"
        , subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI"
        , caption = caption_text
      ) +
      theme_light() +
      theme(
        legend.text = element_text(size = 7)
        , legend.title = element_text(size = 8)
        , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1.05)
        , strip.text = element_text(color = "black", face = "bold")
      ) +
      guides(fill = guide_colorbar(theme = theme(
        legend.key.width  = unit(1, "lines"),
        legend.key.height = unit(12, "lines")
      )))
  
  # label or not
  if(!is.na(label) && !is.na(label_pos) && !is.na(label_size)){
    plt <- plt +
      geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos>=0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
              , facet
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = 0, size = label_size
        ) +
        geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos<0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
              , facet
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = +1, size = label_size
        )
  }
  
  # return facet or not
  if(max(is.na(facet))==0){
    return(
      plt +
        facet_grid(cols = vars(.data[[facet]]))
    )
  }
  else{return(plt)}
}


```

now get the posterior predictive draws

```{r}
draws_temp <- 
  tidyr::crossing(
    structural_params_settings
    , param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight_fact, spectral_weight)
    , param_combos_spectral_ranked %>% dplyr::distinct(chm_res_m)
  ) %>% 
  tidybayes::add_epred_draws(brms_f_score_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred)
# # huh?
draws_temp %>% dplyr::glimpse()
```

##### CHM resolution

first, we'll look at the impact of changing CHM resolution by the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

our questions regarding CHM resolution were: 

* question 1: *does CHM resolution influences detection accuracy?*
* question 2: *does the effect of CHM resolution change based on the inclusion of spectral data versus using only structural data?*

we can answer those questions using our model by considering the credible slope of the CHM resolution predictor as a function of the `spectral_weight` parameter (e.g. Kurz [2025](https://bookdown.org/content/3686/18.html); [Kruschke (2015, Ch. 18)](https://sites.google.com/site/doingbayesiandataanalysis/))

```{r}
pal_spectral_weight <- c("gray77", harrypotter::hp(n=5, option = "gryffindor", direction = 1)) # %>%  scales::show_col()
# brms::posterior_summary(brms_f_score_mod)
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = chm_res_m, y = value, color = spectral_weight)) +
  # tidybayes::stat_halfeye() +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  # ggplot2::facet_wrap(facets = dplyr::vars(spectral_weight))
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "CHM resolution", y = "F-score") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

there are a few takeaways from this plot:

* including spectral data and setting the `spectral_weight` to "1", "2", or "3" appears to be not much different than not including spectral data at all (but we'll probabilistically test this later on)
* including spectral data and setting the `spectral_weight` to "4" yields a larger negative impact of decreasing CHM resolution on detection accuracy
* including spectral data and setting the `spectral_weight` to "5" yields a smaller negative impact of decreasing CHM resolution on detection accuracy and beyond a CHM resolution of ~0.13 m, detection accuracy is maximized when setting the `spectral_weight` to "5". this makes intuitive sense because as the structural information about slash piles becomes less fine grain (i.e. more coarse), we should put more weight into the spectral data for detecting slash piles

all of this is to say that the impact of CHM resolution varies based on the `spectral_weight` setting and vice-versa

averaging across all other parameters to look at the main effect of including spectral data with the `spectral_weight` parameter, these results align with what we saw during our data summarization exploration: increasing the `spectral_weight` (where "5" requires all spectral index thresholds to be met) had minimal impact on metrics until a value of "3", at which point F-score saw a minimal increase; at a `spectral_weight` of "4", the F-score significantly improved, but at a value of "5" F-score was lower than not including the spectral data at all when CHM resolution was fine (e.g. <0.3) but for coarse resolution CHM data the spectral data became more important for detection accuracy.

let's test including predictions at CHM resolutions outside of the bounds of the data tested (e.g. > 0.5 m resolution)

```{r}
tidyr::crossing(
    structural_params_settings
    , param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight_fact, spectral_weight)
    , chm_res_m = seq(0.05,1,by = 0.05)
  ) %>% 
  tidybayes::add_epred_draws(brms_f_score_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = chm_res_m, y = value, color = spectral_weight)) +
  # tidybayes::stat_halfeye() +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  # ggplot2::facet_wrap(facets = dplyr::vars(spectral_weight))
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::scale_x_continuous(breaks = scales::breaks_extended(n=8)) +
  ggplot2::labs(x = "CHM resolution", y = "F-score") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

even at the coarse CHM resolutions not represented in the data, the model is still confident that coarser resolution CHM data decreases detection accuracy

we can look at the posterior distributions of the expected F-score at different CHM resolution levels by the inclusion (or exclusion) of spectral data and it's weighting

```{r}
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=chm_res_m)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = spectral_weight)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), labeller = "label_both") +
  ggplot2::scale_fill_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(limits = c(0,NA), labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "CHM resolution", y = "F-score") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

table that

```{r}
draws_temp %>% 
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(spectral_weight, chm_res_m) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::arrange(spectral_weight, chm_res_m) %>% 
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "F-score<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "spectral_weight", "CHM resolution"
      , c("F-score<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

now we'll probabilistically test the hypothesis that coarser resolution CHM data results in lower detection accuracy and quantify by how much. we'll look at the influence of CHM resolution based on the inclusion of spectral data (or exclusion) and it's weighting determined by the `spectral_weight` parameter

```{r, fig.height=8}
contrast_temp <- 
  draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::group_by(spectral_weight) %>% 
  tidybayes::compare_levels(
    value
    , by = chm_res_m
    , comparison = 
      # "control"
      # tidybayes::emmeans_comparison("revpairwise") 
      "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = chm_res_m) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(spectral_weight, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "CHM resolution contrast"
  , x_axis_title = "difference (F-score)"
  , facet = "spectral_weight"
  , label_size = NA
  , x_expand = c(0,0.1)
  , annotate_which = "left"
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `spectral_weight`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      spectral_weight, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      , pr_lt_zero # , pr_gt_zero
    ) %>% 
    dplyr::arrange(spectral_weight, contrast) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "spectral_weight", "CHM res. contrast"
        , "difference (F-score)"
        , "HDI low", "HDI high"
        , "Pr(diff<0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

it is clear from these contrasts that coarser CHM resolutions lead to a decrease in detection accuracy. without the inclusion of spectral data (i.e. `spectral_weight` = "0"), a 0.1 m increase in CHM resolution coarseness results in an approximate decrease in F-score of ~5-7 percentage points. for example, the expected F-score decreases from 72% to 66% when moving from a 0.2 m resolution CHM raster to a 0.3 m raster, respectively. 

when spectral data is included, this trend of decreasing accuracy with coarser resolution persists, but there is a greater improvement in detection accuracy at coarser resolutions when making the resolution more granular. for example, when including spectral data and setting the `spectral_weight` parameter to "4" (i.e., requiring four spectral thresholds to be met), a shift from a 0.5 m resolution raster to a 0.3 m raster results in a substantial F-score increase of 14.5 percentage points, from 57% to 71%.

##### Spectral data

now we'll look at the impact of including (or excluding) spectral data and the weighting of the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

```{r}
# viridis::rocket(5, begin = 0.9, end = 0.6) %>% scales::show_col()
# brms::posterior_summary(brms_f_score_mod)
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = spectral_weight, y = value, color = factor(chm_res_m))) +
  # tidybayes::stat_halfeye() +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  # ggplot2::facet_wrap(facets = dplyr::vars(spectral_weight))
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "`spectral_weight`", color = "CHM resolution", y = "F-score") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

there are a few takeaways from this plot:

* setting the `spectral_weight` to "3", "4", or "5" improves detection accuracy across CHM resolution levels whereas values of "1" or "2" appear to be similar to including no spectral data at all
* including spectral data and setting the `spectral_weight` to "5" results in a only a slight change in detection accuracy compared to a value of "4" for the finder resolution CHM data (e.g. 0.2 m or less) but beyond a CHM resolution of ~0.25 m detection accuracy is maximized when setting the `spectral_weight` to "5". this makes intuitive sense because as the structural information about slash piles becomes less fine grain (i.e. more coarse), we should put more weight into the spectral data for detecting slash piles

all of this is to say that the impact of `spectral_weight` varies based on the CHM resolution setting and vice-versa

we can look at the posterior distributions of the expected F-score at different `spectral_weight` settings by the input CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=spectral_weight)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = chm_res_m)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(chm_res_m)) +
  ggplot2::scale_fill_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(limits = c(0,NA), labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "`spectral_weight`", y = "F-score") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

we already saw this same data above in our CHM resolution testing, but we'll table that again but this time grouping by CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(chm_res_m, spectral_weight) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::arrange(chm_res_m,spectral_weight) %>% 
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "F-score<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , c("F-score<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

now we'll probabilistically test the hypothesis that the inclusion of spectral data improves detection accuracy and quantify by how much. we'll look at the influence of spectral data based on the CHM resolution

to actually compare the different levels of `spectral_weight`, we'll use the MCMC draws to contrast the posterior predictions at different levels of the parameter (see below)

```{r, fig.height=8.5}
contrast_temp <- 
  draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::group_by(chm_res_m) %>% 
  tidybayes::compare_levels(
    value
    , by = spectral_weight
    , comparison = 
      # tidybayes::emmeans_comparison("revpairwise") 
      "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = spectral_weight) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(chm_res_m, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "`spectral_weight` contrast"
  , x_axis_title = "difference (F-score)"
  , facet = "chm_res_m"
  , label_size = 2
  , x_expand = c(0.5,0.5)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `chm_res_m`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      chm_res_m, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      # , pr_lt_zero 
      , pr_gt_zero
    ) %>% 
    dplyr::arrange(chm_res_m, contrast) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "CHM resolution", "spectral_weight"
        , "difference (F-score)"
        , "HDI low", "HDI high"
        # , "Pr(diff<0)"
        , "Pr(diff>0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

We've previously observed that setting the `spectral_weight` to "1" or "2" appears to have minimal impact on detection accuracy, with improvements only becoming apparent at levels of "3," "4," or "5." These contrasts allow us to evaluate those hypotheses probabilistically. At a 0.2m CHM resolution, for example, there is no evidence that including spectral data but setting `spectral_weight` to "1" is better or worse than not including it at all (`spectral_weight` = "0"). Although setting `spectral_weight` to "3" provides a highly certain improvement in detection accuracy (~90% probability) compared to not using any spectral data, the improvement is small, at less than one percentage point. The model indicates with high certainty (>99% probability) that the optimal setting for 0.2m CHM data is `spectral_weight` = "5," which improves F-score by 6 percentage points compared to not including spectral data at all.

For coarser CHM data (e.g., >0.3m), we can be highly confident (>99% probability) that `spectral_weight` set to "5" is the optimal setting for detection accuracy. At this setting, F-score is increased by 10 percentage points for 0.4m resolution rasters and by 12 percentage points for 0.5m resolution rasters when compared to not including spectral data at all.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Bayesian GLM - Diameter MAPE{#mod_diam_mape}

### Model selection

we're going to use a sub-sample of the data to perform model testing. our objective is to construct the model such that it faithfully represents the data.

we reviewed the main effect parameter trends against MAPE [here](#mape_trends) and used these to guide our model design. we'll follow Kurz [2025](https://bookdown.org/content/3686/18.html) and compare our models with the LOO information criterion

> Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower estimates.

```{r}
# subsample data
set.seed(222)
ms_df_temp <- param_combos_spectral_ranked %>% dplyr::slice_sample(prop = 0.11)
# mcmc setup
iter_temp <- 2444
warmup_temp <- 1222
chains_temp <- 4
####################################################################
# base model with form selected based on main effect trends
####################################################################
diam_mape_mod1_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + circle_fit_iou_pct +
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod1_temp")
)
diam_mape_mod1_temp <- brms::add_criterion(diam_mape_mod1_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by chm_res_m and vice-versa
####################################################################
diam_mape_mod2_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod2_temp")
)
diam_mape_mod2_temp <- brms::add_criterion(diam_mape_mod2_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by convexity_pct and vice-versa
####################################################################
diam_mape_mod3_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod3_temp")
)
diam_mape_mod3_temp <- brms::add_criterion(diam_mape_mod3_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
diam_mape_mod4_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod4_temp")
)
diam_mape_mod4_temp <- brms::add_criterion(diam_mape_mod4_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
diam_mape_mod5_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod5_temp")
)
diam_mape_mod5_temp <- brms::add_criterion(diam_mape_mod5_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
diam_mape_mod6_temp <- brms::brm(
  formula = pct_diff_diameter_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "diam_mape_mod6_temp")
)
diam_mape_mod6_temp <- brms::add_criterion(diam_mape_mod6_temp, criterion = "loo")
```

compare our models with the LOO information criterion. with the `brms::loo_compare()` function, we can compute a formal difference score between models with the output rank ordering the models such that the best fitting model appears on top. all models also receive a difference score relative to the best model and a standard error of the difference score

```{r}
brms::loo_compare(
  diam_mape_mod1_temp, diam_mape_mod2_temp, diam_mape_mod3_temp
  , diam_mape_mod4_temp, diam_mape_mod5_temp, diam_mape_mod6_temp
) %>% 
  kableExtra::kbl(caption = "Diameter MAPE model selection with LOO information criterion") %>% 
  kableExtra::kable_styling()
```

we can also look at the AIC-type model weights

```{r,eval=F}
brms::model_weights(
  diam_mape_mod1_temp, diam_mape_mod2_temp, diam_mape_mod3_temp
  , diam_mape_mod4_temp, diam_mape_mod5_temp, diam_mape_mod6_temp
) %>% 
  round(digits = 4)
```

we can also quickly look at the Bayeisan $R^2$ returned from the `brms::bayes_R2()` function

```{r}
dplyr::bind_rows(
  brms::bayes_R2(diam_mape_mod1_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod1_temp")
  , brms::bayes_R2(diam_mape_mod2_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod2_temp")
  , brms::bayes_R2(diam_mape_mod3_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod3_temp")
  , brms::bayes_R2(diam_mape_mod4_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod4_temp")
  , brms::bayes_R2(diam_mape_mod5_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod5_temp")
  , brms::bayes_R2(diam_mape_mod6_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "diam_mape_mod6_temp")
) %>% 
  dplyr::mutate(mod = factor(mod)) %>% 
  ggplot2::ggplot(mapping=ggplot2::aes(y=R2, x = mod)) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
  ) +
  # ggplot2::facet_grid(cols = dplyr::vars(spectral_weight)) +
  # ggplot2::scale_fill_manual(values = pal_chm_res_m) +
  ggplot2::labs(x = "", y = "Bayesian R-squared") +
  ggplot2::theme_light()

```

the more complex models were selected as the best. because the selected model includes quadratic terms and multiple interactions parameter interpretation will be a challenge, so we will have to rely on plotting the modeled relationships rather than trying to interpret the coefficients.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Modeling{#mod_diam_mape2}

the selected MAPE model predicts diameter error using the four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) and data properties representing the CHM resolution (`chm_res_m`) and the use of spectral data (`spectral_weight`). The model includes quadratic terms for both `chm_res_m` and `circle_fit_iou_pct` to allow for non-linear relationships between these parameters and MAPE.

To account for how different parameters influence each other, the model includes several interaction terms. The `circle_fit_iou_pct:chm_res_m` and the `circle_fit_iou_pct:convexity_pct` interactions were included to explore how the effect of the geometric filtering parameters varies with CHM resolution and with each other. In addition, the model includes `spectral_weight` as a main effect and in an interaction with `chm_res_m`. This means that the predicted MAPE can vary by the `spectral_weight` setting even when all other variables are zero (i.e. the intercept), effectively giving each level of `spectral_weight` its own unique intercept. Finally, the `chm_res_m:spectral_weight` interaction allows the effect of CHM resolution (i.e. its slope) on MAPE to vary across each `spectral_weight` setting.

the fully factored Bayesian statistical model that details the likelihood, linear model, and priors used is:

\begin{align*}
\text{MAPE}_i \sim & \operatorname{Gamma}(\mu_i, \text{shape}) \\
\log(\mu_i) = & (\beta_1 \cdot \text{max_ht_m}_i) + (\beta_2 \cdot \text{max_area_m2}_i) \\
& + (\beta_3 \cdot \text{convexity_pct}_i) + (\beta_4 \cdot (\text{convexity_pct}_i)^2) \\
& + (\beta_5 \cdot \text{circle_fit_iou_pct}_i) + (\beta_6 \cdot (\text{circle_fit_iou_pct}_i)^2) \\
& + (\beta_7 \cdot \text{chm_res_m}_i) + (\beta_8 \cdot (\text{chm_res_m}_i)^2) \\
& + \sum_{j=0}^{5} \left( \beta_{9, j} \cdot \mathbf{I}(\text{spectral_weight}_i = j) \right) \\
& + (\beta_{10} \cdot \text{convexity_pct}_i \cdot \text{circle_fit_iou_pct}_i) \\
& + (\beta_{11} \cdot \text{convexity_pct}_i \cdot \text{chm_res_m}_i) \\
& + (\beta_{12} \cdot \text{circle_fit_iou_pct}_i \cdot \text{chm_res_m}_i) \\
& + \sum_{j=0}^{5} \left( \beta_{13, j} \cdot \text{chm_res_m}_i \cdot \mathbf{I}(\text{spectral_weight}_i = j) \right) \\
& + (\beta_{14} \cdot \text{convexity_pct}_i \cdot \text{circle_fit_iou_pct}_i \cdot \text{chm_res_m}_i) \\
\beta_k \sim & \operatorname{Student-t}(3, 0, 10) \quad \text{for } k = 1, \dots, 14 \\
\text{shape} \sim & \operatorname{Gamma}(0.01, 0.01)
\end{align*}

where, $i$ represents a single observation in the dataset which corresponds to a specific combination of the six parameters (, `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`, `chm_res_m`, and `spectral_weight`) and its resulting MAPE. Where `k` is used to index the different beta coefficients, which correspond to the intercept and the effects of each of the independent variables and their interactions and `j` denotes the specific level of the nominal (i.e. categorical) predictor `spectral_weight`

we reviewed the main effect parameter trends against MAPE [here](#mape_trends) and used these to guide our model design

The table below details the terms used in our Bayesian GLM model defined in the `brms::brm()` call:

| Term in Formula | Type of Effect | Description of Relationship Tested |
| :--- | :--- | :--- |
| **`0 +`** | Zero Intercept | Specifies that the model is fit without a global intercept. The effect of each factor level and the value of continuous variables at zero is estimated directly. |
| **`max_ht_m`** | Main Effect (Linear) | Tests the direct linear influence of the maximum pile height threshold on MAPE. |
| **`max_area_m2`** | Main Effect (Linear) | Tests the direct linear influence of the maximum pile area threshold on MAPE. |
| **`chm_res_m`** | Main Effect (Linear) | Tests the direct linear influence of the input CHM resolution on MAPE. |
| **`I(chm_res_m^2)`** | Nonlinear (Quadratic) | Models a curved relationship with CHM resolution, allowing MAPE to potentially decrease then increase (or vice versa) as resolution changes. |
| **`spectral_weight`** | Main Effect (Factor) | The model estimates a coefficient for each of the six spectral weight levels (0 through 5), representing the estimated mean MAPE for that specific level when all continuous variables are zero. |
| **`circle_fit_iou_pct`** | Main Effect (Linear) | Tests the direct linear influence of the pile's circular conformity threshold on MAPE. |
| **`convexity_pct`** | Main Effect (Linear) | Tests the direct linear influence of the pile's boundary smoothness (convexity) threshold on MAPE. |
| **`I(circle_fit_iou_pct^2)`** | Nonlinear (Quadratic) | Models a curved relationship where MAPE may peak or bottom out at an intermediate threshold for pile circularity. |
| **`I(convexity_pct^2)`** | Nonlinear (Quadratic) | Models a curved relationship where MAPE may peak or bottom out at an intermediate threshold for pile boundary smoothness. |
| **`convexity_pct:circle_fit_iou_pct`** | Two-Way Interaction | Captures how the optimal balance between pile circular conformity and boundary smoothness changes for MAPE. |
| **`chm_res_m:spectral_weight`** | Two-Way Interaction (Factor) | Captures how the effect of CHM resolution's slope on MAPE changes across each of the six spectral weighting levels. |
| **`convexity_pct:chm_res_m`** | Two-Way Interaction | Captures how the sensitivity to the pile boundary smoothness threshold changes with the input data resolution. |
| **`circle_fit_iou_pct:chm_res_m`** | Two-Way Interaction | Captures how the importance of the pile's circular conformity threshold changes as the input data resolution changes. |
| **`convexity_pct:circle_fit_iou_pct:chm_res_m`** | Three-Way Interaction | Shows how the combined effects of the convexity and circularity thresholds change simultaneously across different input CHM resolutions. |

```{r}
brms_diam_mape_mod <- brms::brm(
  formula = pct_diff_diameter_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = param_combos_spectral_ranked # %>% dplyr::slice_sample(prop = 0.33)
  , family = Gamma(link = "log")
  # mcmc
  , iter = 14000, warmup = 7000
  , chains = 4
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "brms_diam_mape_mod")
)
# brms::make_stancode(brms_diam_mape_mod)
# brms::prior_summary(brms_diam_mape_mod)
# print(brms_diam_mape_mod)
# brms::neff_ratio(brms_diam_mape_mod)
# brms::rhat(brms_diam_mape_mod)
# brms::nuts_params(brms_diam_mape_mod)
```

The `brms::brm` model summary

```{r}
brms_diam_mape_mod %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | parameter == "phi"
  ) %>% 
  kableExtra::kbl(digits = 3, caption = "Bayesian model for Diameter MAPE") %>% 
  kableExtra::kable_styling()
```

note the quadratic coefficients ending in `E2`, Kruschke (2015) provides some insight on how to interpret:

>A quadratic has the form $y = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$. When $\beta_{2}$ is zero, the form reduces to a line. Therefore, this extended model can produce any fit that the linear model can. When $\beta_{2}$ is positive, a plot of the curve is a parabola that opens upward. When $\beta_{2}$ is negative, the curve is a parabola that opens downward. We have no reason to think that the curvature in the family-income data is exactly a parabola, but the quadratic trend might describe the data much better than a line alone. (p. 496)

### Posterior Predictive Checks

Markov chain Monte Carlo (MCMC) simulations were conducted using the brms package (Bürkner 2017) to estimate posterior predictive distributions of the parameters of interest. We ran 4 chains of 10,000 iterations with the first 5,000 discarded as burn-in. Trace-plots were utilized to visually assess model convergence.

check the trace plots for problems with convergence of the Markov chains

```{r, include=T, eval=T, fig.height=6.6}
plot(brms_diam_mape_mod)
```

Sufficient convergence was checked with $\hat{R}$ values near 1 ([Brooks & Gelman, 1998](https://scholar.google.com/scholar?cluster=14209404114665352991&hl=en&as_sdt=0,6)). 

in the plot below, $\hat{R}$ values are colored using different shades (lighter is better). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: below 1.05 (good)
* mid: between 1.05 and 1.1 (ok)
* dark: above 1.1 (too high)

check our $\hat{R}$ values

```{r}
brms::mcmc_plot(brms_diam_mape_mod, type = "rhat_hist") +
  ggplot2::scale_x_continuous(breaks = scales::breaks_extended(n = 6)) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain **not** to the sample size of the data where acceptable values allow "for reasonably accurate and stable estimates of the limits of the 95% HDI...If accuracy of the HDI limits is not crucial for your application, then a smaller ESS may be sufficient" [(Kruschke 2015, p. 184)](https://sites.google.com/site/doingbayesiandataanalysis/)

Ratios of effective sample size (ESS) to total sample size with values are colored using different shades (lighter is better). A ratio close to "1" (no autocorrelation) is ideal, while a low ratio suggests the need for more samples or model re-parameterization. Efficiently mixing MCMC chains are important because they guarantee the resulting posterior samples accurately represent the true distribution of model parameters, which is necessary for reliable and precise estimation of parameter values and their associated uncertainties (credible intervals). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: between 0.5 and 1 (high)
* mid: between 0.1 and 0.5 (good)
* dark: below 0.1 (low)

```{r}
# and another effective sample size check
brms::mcmc_plot(brms_diam_mape_mod, type = "neff_hist") +
# brms::mcmc_plot(brms_diam_mape_mod, type = "neff") +
  ggplot2::scale_x_continuous(limits = c(0,NA), breaks = scales::breaks_extended(n = 9)) +
  # ggplot2::scale_color_discrete(drop = F) +
  # ggplot2::scale_fill_discrete(drop = F) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

Posterior predictive checks were used to evaluate model goodness-of-fit by comparing data simulated from the model with the observed data used to estimate the model parameters ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)). Calculating the proportion of MCMC iterations in which the test statistic (i.e., mean and sum of squares) from the simulated data and observed data are more extreme than one another provides the Bayesian p-value. Lack of fit is indicated by a value close to 0 or 1 while a value of 0.5 indicates perfect fit ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)).

To learn more about this approach to posterior predictive checks, check out Gabry’s (2025) vignette, [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). 

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data. our objective is to construct the model such that it faithfully represents the data.

```{r}
# posterior predictive check
brms::pp_check(
    brms_diam_mape_mod
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  ggplot2::labs(subtitle = "posterior-predictive check (overlaid densities)") +
  ggplot2::theme_light() +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
    , plot.subtitle = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )
```

another way

```{r}
brms::pp_check(brms_diam_mape_mod, type = "ecdf_overlay", ndraws = 100) +
  ggplot2::labs(subtitle = "posterior-predictive check (ECDF: empirical cumulative distribution function)") + 
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
  )
```

```{r, include=FALSE, eval=F}
# and another posterior predictive check for the overall model combining mean and sd
brms::pp_check(brms_diam_mape_mod, type = "stat_2d", ndraws = 555) +
  ggplot2::theme_light() +
  ggplot2::labs(title = "F-Score") +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )
brms::pp_check(
    brms_diam_mape_mod
    , type = "stat"
    , stat = "mean"
    , ndraws = 444
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
brms::pp_check(
    brms_diam_mape_mod
    , type = "stat"
    , stat = "sd"
    , ndraws = 888
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sds") +
  theme_light()
# grouped
brms::pp_check(
    brms_diam_mape_mod
    , type = "dens_overlay_grouped" # ""
    , group = "spectral_weight"
    , ndraws = 111
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  theme_light()
# grouped
brms::pp_check(
    brms_diam_mape_mod
    , type = "dens_overlay_grouped" # ""
    , group = "chm_res_m"
    , ndraws = 111
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  theme_light()
# grouped
brms::pp_check(
    brms_diam_mape_mod
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "spectral_weight"
    , ndraws = 333
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
# grouped
brms::pp_check(
    brms_diam_mape_mod
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "chm_res_m"
    , ndraws = 333
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
```

### Conditional Effects

first, lets look at densities of the posterior samples per parameter

```{r}
brms::mcmc_plot(brms_diam_mape_mod, type = "dens") +
  # ggplot2::theme_light() +
  ggplot2::theme(
    strip.text = ggplot2::element_text(size = 7.5, face = "bold", color = "black")
  )
```

and we can look at the default coefficient plot that is commonly used in reporting coefficient "significance" in frequentist analysis

```{r}
# easy way to get the default coeff plot
brms::mcmc_plot(brms_diam_mape_mod, variable = "\\bb_", regex = T, type = "intervals")
```

Regarding interactions and polynomial models like the one we use, [McElreath (2015)](https://xcelab.net/rm/) notes:

>parameters are the linear and square components of the curve, respectively. But that doesn’t make them transparent. You have to plot these model fits to understand what they are saying. (p. 112-113)

all of the interactions and the quadradic trend of this model combine to make these coefficients by themselves uninterpretable as the coefficients are only meaningful in the context of the other terms in the interaction or by adding the quadratic component 

we can do this by checking for the main effects of the individual variables on F-score (averages across all other effects)

```{r}
brms::conditional_effects(brms_diam_mape_mod)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Posterior Predictive Expectation

we will test our hypotheses using the posterior distributions of the expected values (i.e., the posterior predictions of the mean) obtained via `tidybayes::add_epred_draws()`. our analysis will include two stages using parameter levels of the four structural parameters: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`.  in practice, these values should be informed by the treatment prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` are fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

as a reminder, here are those parameter levels

```{r}
structural_params_settings %>% dplyr::glimpse()
```

now we'll get the posterior predictive draws but over a range of `circle_fit_iou_pct` and `convexity_pct` including the best setting

```{r}
seq_temp <- seq(from = 0.05, to = 1.0, by = 0.1)
seq2_temp <- seq_temp[seq(1, length(seq_temp), by = 2)] # get every other element
# draws
draws_temp <- 
  # get the draws for levels of 
  # spectral_weight circle_fit_iou_pct convexity_pct
  tidyr::crossing(
    param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight)
    , circle_fit_iou_pct = seq_temp %>% unique()
    , convexity_pct = seq_temp %>% unique()
    , chm_res_m = seq(from = 0.1, to = 1.0, by = 0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  # dplyr::glimpse()
  tidybayes::add_epred_draws(brms_diam_mape_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(
    is_seq = (convexity_pct %in% seq_temp) & (circle_fit_iou_pct %in% seq_temp)
  )
# # huh?
draws_temp %>% dplyr::glimpse()
```

#### Geometric shape regularity

let's look at the influence of the parameters that control the geometric shape regularity filtering: `circle_fit_iou_pct` and `convexity_pct`. to do this, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

##### `circle_fit_iou_pct`

we need to look at the influence of `circle_fit_iou_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.2)
    , convexity_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(convexity_pct = factor(convexity_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct, y = value, color = convexity_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.95)
    , lwd = 1.1, fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both", scales = "free_y") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "mako", begin = 0.6, end = 0.1) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `circle_fit_iou_pct` on Diameter MAPE"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`circle_fit_iou_pct`"
    , y = "Diameter MAPE"
    , color = "`convexity_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )

```

For all of these faceted plots, take note of the narrow range of the y-axis (Predicted Diameter MAPE), which is particularly narrow for finer resolution CHM data (e.g., <0.3m). The non-linear relationship between the `circle_fit_iou_pct` parameter and the Diameter MAPE is prominent, especially for coarser resolution CHM data. This is confirmed by the statistical model's negative quadratic coefficient for the squared term of `circle_fit_iou_pct` (i.e., `I(circle_fit_iou_pct^2)`), which indicates a downward-opening parabolic shape. Across all CHM resolutions and irrespective of the `spectral_weight` setting, diameter quantification accuracy is highest (MAPE is lowest) at the highest values of the `circle_fit_iou_pct` parameter (e.g., greater than 0.6). This finding is intuitive: diameter error is calculated only for True Positive matches, and when a successful match is made between a prediction and a ground truth pile at these high circularity thresholds, it strongly suggests the predicted pile perimeter accurately conforms to the ground truth pile shape.

The influence of the `circle_fit_iou_pct` parameter on diameter accuracy is conditional on the `convexity_pct` parameter setting. The relationship between these parameters is most evident with coarser resolution CHM data (e.g., >0.3m) and looking at cases when `convexity_pct` parameter is set low (e.g., less than 0.4) the best diameter quantification accuracy occurs when the `circle_fit_iou_pct` parameter is set high (e.g., greater than 0.5), and vice-versa. This effect occurs because both parameters function as geometric shape filters in the rules-based methodology. When the piles in the treatment area are generally uniform (e.g., mostly circular), the parameters become somewhat redundant. However, when pile shapes are heterogeneous, these two filtering parameters must be balanced to fine-tune the retention of objects. For instance, if piles are expected to be rectangular, the optimal setting would be to effectively disable the `circle_fit_iou_pct` filter (i.e., set it to '0') and rely on the `convexity_pct` filter to retain objects with the expected perimeter regularity.

Finally, the `circle_fit_iou_pct` parameter has a smaller influence on diameter quantification accuracy with finer resolution CHM data (e.g., <0.3m) and a larger influence on Diameter MAPE for coarser CHM data (e.g., >0.3m). For the coarser CHM data, increasing values of the `circle_fit_iou_pct` parameter on the right side of the vertex increased diameter quantification accuracy (decreased MAPE) at an increasing rate as the parameter approached '1'.

##### `convexity_pct`

we need to look at the influence of `convexity_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.2)
    , circle_fit_iou_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(circle_fit_iou_pct = factor(circle_fit_iou_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct, y = value, color = circle_fit_iou_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.95)
    , lwd = 1.1
    , fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both", scales = "free_y") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "magma", begin = 0.5, end = 0.1) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `convexity_pct` on Diameter MAPE"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`convexity_pct`"
    , y = "Diameter MAPE"
    , color = "`circle_fit_iou_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )

```

The model shows that the influence of the `convexity_pct` parameter on the Diameter MAPE is conditional on the CHM resolution. The magnitude of this influence varies significantly: for finer resolution CHM data (e.g., less than 0.3m), the `convexity_pct` parameter has a minimal impact on the Diameter MAPE, as evidenced by the narrow range of the y-axis; conversely, its influence is considerably larger at coarser resolutions. Across all CHM resolutions tested, it is clear that the influence of the `circle_fit_iou_pct` parameter on diameter accuracy is conditional on the `convexity_pct` parameter setting. This relationship is most pronounced with coarser resolution CHM data (e.g., greater than 0.3m). For example, using 0.5m CHM data, the vertex of the downward-opening parabola (which indicates the largest diameter quantification error) shifts across the `convexity_pct` range: it is located at lower `convexity_pct` values (i.e., less than 0.5) when the `circle_fit_iou_pct` parameter is set low (e.g., less than 0.4), but shifts to higher `convexity_pct` values (i.e., greater than 0.6) when the `circle_fit_iou_pct` parameter is set high (e.g., greater than 0.6). This inverse relationship occurs because both parameters function as geometric shape filters in the rules-based methodology. For the test data, where piles generally exhibit regular, circular footprints, the two filters become somewhat redundant. Therefore, the lowest Diameter MAPE (highest accuracy) is achieved when the parameters are complementary: when `circle_fit_iou_pct` is high (near 1) and `convexity_pct` is low (near 0), or vice-versa. Additionally, if `circle_fit_iou_pct` is at an intermediate level (near 0.5), `convexity_pct` should be set either high or low to maintain diameter quantification accuracy. It is important to note though, that these relationships are specific to regularly shaped circular pile footprints; if pile bases are expected to be square or rectangular, the parameters should be rebalanced. For instance, in a scenario predicting rectangular piles, the optimal setting would be to effectively disable the `circle_fit_iou_pct` filter (set to '0') and instead rely on the `convexity_pct` filter to enforce the expected shape regularity filtering.

##### Optimizing geometric filtering

Given the complexity of our model, which includes a non-linear link function and parameter interactions, calculating the optimal parameter values by solving for them algebraically from the model's coefficients would be prone to error. instead, we can use a robust Bayesian approach that leverages the model's posterior predictive distribution. This method is powerful because it inherently accounts for all sources of model uncertainty.

first, we'll generate a large number of predictions across a fine grid of parameter values (e.g. in steps of 0.01) for each posterior draw of the model coefficients. we'll generate a large number (e.g. 1000+) of posterior predictive draws for each combination of parameter values. for each posterior predictive draw, we'll then identify the parameter combination that maximizes the Diameter MAPE and we'll be left with a posterior distribution of optimal parameter combinations.

this approach demonstrates a key advantage of the Bayesian framework, allowing us to ask complex questions and find the most probable optimal parameter combination while fully accounting for uncertainty.

note, we only extract draws based on not using any spectral data (i.e. `spectral_weight` = 0) to save on plotting space and because we expect form quantification to only be minimally influenced by the inclusion of spectral data

```{r}
# let's get the draws at a very granular level
vertex_draws_temp <- 
  tidyr::crossing(
    param_combos_spectral_ranked %>% 
      dplyr::filter(spectral_weight=="0") %>% 
      dplyr::distinct(spectral_weight, spectral_weight_fact)
    , circle_fit_iou_pct = seq(from = 0.0, to = 1, by = 0.01) # very granular to identify vertex
    , convexity_pct = seq(from = 0.0, to = 1, by = 0.02) # very granular to identify vertex
    , chm_res_m = seq(0.1,0.5,by=0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  tidybayes::add_epred_draws(brms_diam_mape_mod, ndraws = 1000, value = "value") %>% 
  dplyr::ungroup() %>% 
  # for each draw, get the highest Diameter MAPE by chm_res_m, spectral_weight
  # which we'll use to identify the optimal circle_fit_iou_pct,convexity_pct settings
  # these are essentially "votes" based on likelihood
  dplyr::group_by(
    .draw
    , chm_res_m, spectral_weight
  ) %>% 
  dplyr::arrange(value,circle_fit_iou_pct,convexity_pct) %>% # notice the ascending sort of value (mape) here to take the lowest 
  dplyr::slice(1)
# vertex_draws_temp %>% dplyr::glimpse() # this thing is huge
```

plot the posterior distribution of optimal parameter setting for `circle_fit_iou_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>%
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  # ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(limits = c(0,1), breaks = scales::breaks_extended(6)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8)
  )
```

The optimal `circle_fit_iou_pct` setting for maximizing diameter quantification accuracy is predicted to be near it's highest setting of '1'. This finding is likely not very applicable in real-world scenarios but it is intuitive: diameter error is calculated only for True Positive matches, and when a successful match is made between a prediction and a ground truth pile at these high circularity thresholds, it strongly suggests the predicted pile perimeter accurately conforms to the ground truth pile shape.

plot the posterior distribution of optimal parameter setting for `convexity_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>% 
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  ggplot2::scale_x_continuous(limits = c(0,1), breaks = scales::breaks_extended(6)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

when the `circle_fit_iou_pct` parameter is optimized (i.e. set near '1'), the influence of `convexity_pct` on form quantification accuracy is dependent on the CHM resolution. only for the finest resolution CHM data (i.e. 0.1m) tested, the model's predictions indicate with high certainty that the optimal `convexity_pct` is its maximum value of '1'. for coarser resolution CHM data (e.g. 0.3m), the 95% HDI for the optimal `convexity_pct` spans the entire 0-1 range, indicating the model is not confident in any specific setting. an important distinction is that this finding of irrelavance for the `convexity_pct` parameter on diameter quantification is only for cases when `circle_fit_iou_pct` is already optimized, which, as shown above, was at a level near 1 for this coarser-resolution CHM data. at these higher `circle_fit_iou_pct` levels , the filtering for irregularly shaped objects is accomplished solely by the `circle_fit_iou_pct` parameter so the `convexity_pct` is less important for accurate quantification of predicted diameter. however, when the `circle_fit_iou_pct` parameter is *not* optimized or is turned off (e.g., set to 0), `convexity_pct` becomes a crucial parameter for filtering irregularly shaped objects.

we can look at this another way, check it

```{r, fig.height=8.5}
vertex_draws_temp %>% 
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y = convexity_pct, x = circle_fit_iou_pct)) +
  # geom_point(alpha=0.2) +
  ggplot2::geom_jitter(alpha=0.2, height = .01, width = 0.0) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    # , scales = "free_y"
    , labeller = "label_both"
  ) +
  # ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  # ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

note, in the plot above, we slightly "jitter" the points so that they are visible where they would otherwise be stacked on top of each other and only look like a few points instead of the 1000 draws from the posterior we used

let's table the HDI of the optimal values

```{r}
# summarize it
vertex_draws_temp <- 
  vertex_draws_temp %>% 
  dplyr::group_by(
    chm_res_m, spectral_weight
  ) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$y
    , median_hdi_lower_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymin
    , median_hdi_upper_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymax
    # get median_hdi
    , median_hdi_est_convexity_pct = tidybayes::median_hdci(convexity_pct)$y
    , median_hdi_lower_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymin
    , median_hdi_upper_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymax
  ) %>%
  dplyr::ungroup() 
# table it
vertex_draws_temp %>% 
  kableExtra::kbl(
    digits = 2
    , caption = ""
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , rep(c("median", "HDI low", "HDI high"),2)
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "circle_fit_iou_pct" = 3
    , "convexity_pct" = 3
  ))
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Input data

to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`), we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`). the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

we'll make contrasts of the posterior predictions to probabilistically quantify the influence of the input data (e.g. inclusion of spectral data and it's weighting and CHM resolution)

let's get the posterior predictive draws

```{r}
# structural_params_settings
draws_temp <- 
  tidyr::crossing(
    structural_params_settings
    , param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight_fact, spectral_weight)
    , param_combos_spectral_ranked %>% dplyr::distinct(chm_res_m)
  ) %>% 
  tidybayes::add_epred_draws(brms_diam_mape_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred)
# # huh?
# draws_temp %>% dplyr::glimpse()
```

##### CHM resolution

first, we'll look at the impact of changing CHM resolution by the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

our questions regarding CHM resolution were: 

* question 1: *does CHM resolution influence quantification accuracy?*
* question 2: *does the effect of CHM resolution change based on the inclusion of spectral data versus using only structural data?*

we can answer those questions using our model by considering the credible slope of the CHM resolution predictor as a function of the `spectral_weight` parameter (e.g. Kurz [2025](https://bookdown.org/content/3686/18.html); [Kruschke (2015, Ch. 18)](https://sites.google.com/site/doingbayesiandataanalysis/))

```{r}
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = chm_res_m, y = value, color = spectral_weight)) +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "CHM resolution", y = "Diameter MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

the primary takeaway from this plot is that coarser resolution CHM data produces significantly worse diameter quantification accuracy (i.e. higher MAPE) irrespective of the inclusion of spectral data or it's weighting. if the objective is to quantify slash pile form with any sort of accuracy, a CHM resolution of 0.2m or finer is needed. based on the vertex of the parabola, the optimal CHM resolution for accurately quantifying diameter is between 0.1m and 0.2m. remember that the inclusion of spectral data in the data fusion detection methodology does not alter candidate pile form nor add new piles, so we don't expect much variability in quantification accuracy at a given CHM resolution based on the inclusion of spectral data. any changes in overall pile quantification accuracy (e.g. when aggregated to the stand level) from the inclusion of spectral data is a result of the spectral data filtering out candidate piles that would otherwise alter the overall, aggregated accuracy.

we can look at the posterior distributions of the expected Diameter MAPE at different CHM resolution levels by the inclusion (or exclusion) of spectral data and it's weighting

```{r}
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=chm_res_m)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = spectral_weight)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), labeller = "label_both") +
  ggplot2::scale_fill_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "CHM resolution", y = "Diameter MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

those are some tight posterior distributions and HDI's, meaning our model is very confident in the expected diameter accuracy at the CHM resolutions tested. put another way, the output from our pile detection methodology is consistent in terms of quantification of detected pile diameter and it's error relative to field-measured values.

table that

```{r}
draws_temp %>% 
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(spectral_weight, chm_res_m) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(spectral_weight, chm_res_m) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "Diameter MAPE<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "spectral_weight", "CHM resolution"
      , c("Diameter MAPE<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

now we'll probabilistically test the hypothesis that coarser resolution CHM data results in lower diameter quantification accuracy and determine by how much. we'll look at the influence of CHM resolution based on the inclusion of spectral data (or exclusion) and it's weighting determined by the `spectral_weight` parameter

we'll only make contrasts against the lowest CHM resolution tested which will be treated as a "control" in the `tidybayes::compare_levels()` call

```{r, fig.height=8}
contrast_temp <- 
  draws_temp %>% 
  dplyr::group_by(spectral_weight) %>% 
  tidybayes::compare_levels(
    value
    , by = chm_res_m
    , comparison = 
      "control"
      # tidybayes::emmeans_comparison("revpairwise") 
      # "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = chm_res_m) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(spectral_weight, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "CHM resolution contrast"
  , x_axis_title = "difference (Diameter MAPE)"
  , facet = "spectral_weight"
  , label_size = 2.5
  , x_expand = c(0.1,0.6)
  # , annotate_which = "right"
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `spectral_weight`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      spectral_weight, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      , pr_gt_zero # , pr_lt_zero
    ) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    dplyr::arrange(spectral_weight, contrast) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "spectral_weight", "CHM res. contrast"
        , "difference (Diameter MAPE)"
        , "HDI low", "HDI high"
        , "Pr(diff>0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

it is clear from these contrasts that coarser CHM resolutions lead to a decrease in diameter quantification accuracy and the pattern and difference in accuracy between resolution levels is consistent whether or not spectral data is included and it's weighting if included. taking the method that does not use spectral data as an example (i.e. `spectral_weight` = 0), the lowest diameter MAPE of 10.6% was achieved using a 0.1m CHM and the diameter quantification accuracy decreased (MAPE increased) steadily as CHM resolution became more coarse with a 41.8% diameter MAPE expected fore the 0.5 m resolution data. we can be certain (>99% probability) that there is a significant decrease in diameter quantification accuracy (i.e. increase in MAPE) for coarser resolution CHM data compared to finer resolution data.

##### Spectral data

now we'll look at the impact of including (or excluding) spectral data and the weighting of the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

*Note*: 
we expect that the spectral data does not significantly alter the quantification of slash pile form. this is because spectral information is used solely to filter candidate piles, meaning it neither reshapes existing ones nor introduces new detections.

```{r}
# viridis::rocket(5, begin = 0.9, end = 0.6) %>% scales::show_col()
# brms::posterior_summary(brms_diam_mape_mod)
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = spectral_weight, y = value, color = factor(chm_res_m))) +
  # tidybayes::stat_halfeye() +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  # ggplot2::facet_wrap(facets = dplyr::vars(spectral_weight))
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "`spectral_weight`", color = "CHM resolution", y = "Diameter MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

while there is a slight decrease in diameter quantification accuracy (increase in MAPE) at the highest spectral weighting, the inclusion of spectral data and the setting of the `spectral_weight` parameter does not significantly alter the form quantification accuracy irrespective of the CHM resolution

we can look at the posterior distributions of the expected Diameter MAPE at different `spectral_weight` settings by the input CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=spectral_weight)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = chm_res_m)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(chm_res_m)) +
  ggplot2::scale_fill_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(limits = c(0,NA), labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "`spectral_weight`", y = "Diameter MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

we already saw this same data above in our CHM resolution testing, but we'll table that again but this time grouping by CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(chm_res_m, spectral_weight) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(chm_res_m,spectral_weight) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "Diameter MAPE<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , c("Diameter MAPE<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

now we'll probabilistically test the hypothesis that the inclusion of spectral data improves detection accuracy and quantify by how much. we'll look at the influence of spectral data based on the CHM resolution

to actually compare the different levels of `spectral_weight`, we'll use the MCMC draws to contrast the posterior predictions at different levels of the parameter (see below)

```{r, fig.height=8.5}
contrast_temp <- 
  draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::group_by(chm_res_m) %>% 
  tidybayes::compare_levels(
    value
    , by = spectral_weight
    , comparison = 
      # tidybayes::emmeans_comparison("revpairwise") 
      "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = spectral_weight) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(chm_res_m, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "`spectral_weight` contrast"
  , x_axis_title = "difference (Diameter MAPE)"
  , facet = "chm_res_m"
  , label_size = 2
  , x_expand = c(0.5,0.5)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `chm_res_m`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      chm_res_m, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      # , pr_lt_zero 
      , pr_gt_zero
    ) %>% 
    dplyr::arrange(chm_res_m, contrast) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "CHM resolution", "spectral_weight"
        , "difference (Diameter MAPE)"
        , "HDI low", "HDI high"
        # , "Pr(diff<0)"
        , "Pr(diff>0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

although we can be very certain that there are differences in diameter quantification accuracy (>99% probability across most CHM resolutions) when the `spectral_weight` is set to "5" (i.e. requiring that all spectral criteria be met) compared to the other settings of `spectral_weight` or not including spectral data, these differences are so small with nearly all of them showing a difference in diameter MAPE of <1%.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Bayesian GLM - Height MAPE

### Model selection

we're going to use a sub-sample of the data to perform model testing. our objective is to construct the model such that it faithfully represents the data.

we reviewed the main effect parameter trends against MAPE [here](#mape_trends) and used these to guide our model design. we'll follow Kurz [2025](https://bookdown.org/content/3686/18.html) and compare our models with the LOO information criterion

> Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower estimates.

```{r}
# subsample data
set.seed(222)
ms_df_temp <- param_combos_spectral_ranked %>% dplyr::slice_sample(prop = 0.11)
# mcmc setup
iter_temp <- 2444
warmup_temp <- 1222
chains_temp <- 4
####################################################################
# base model with form selected based on main effect trends
####################################################################
height_mape_mod1_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + circle_fit_iou_pct +
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod1_temp")
)
height_mape_mod1_temp <- brms::add_criterion(height_mape_mod1_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by chm_res_m and vice-versa
####################################################################
height_mape_mod2_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + convexity_pct + 
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod2_temp")
)
height_mape_mod2_temp <- brms::add_criterion(height_mape_mod2_temp, criterion = "loo")
####################################################################
# allows slope and curvature of circle_fit_iou_pct to vary by convexity_pct and vice-versa
####################################################################
height_mape_mod3_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod3_temp")
)
height_mape_mod3_temp <- brms::add_criterion(height_mape_mod3_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
height_mape_mod4_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~ 
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod4_temp")
)
height_mape_mod4_temp <- brms::add_criterion(height_mape_mod4_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
height_mape_mod5_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m +
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod5_temp")
)
height_mape_mod5_temp <- brms::add_criterion(height_mape_mod5_temp, criterion = "loo")
####################################################################
# a three-way interaction of circle_fit_iou_pct, convexity_pct and chm_res_m
####################################################################
height_mape_mod6_temp <- brms::brm(
  formula = pct_diff_height_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = ms_df_temp
  , family = Gamma(link = "log")
  # mcmc
  , iter = iter_temp, warmup = warmup_temp
  , chains = chains_temp
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "height_mape_mod6_temp")
)
height_mape_mod6_temp <- brms::add_criterion(height_mape_mod6_temp, criterion = "loo")
```

compare our models with the LOO information criterion. with the `brms::loo_compare()` function, we can compute a formal difference score between models with the output rank ordering the models such that the best fitting model appears on top. all models also receive a difference score relative to the best model and a standard error of the difference score

```{r}
brms::loo_compare(
  height_mape_mod1_temp, height_mape_mod2_temp, height_mape_mod3_temp
  , height_mape_mod4_temp, height_mape_mod5_temp, height_mape_mod6_temp
) %>% 
  kableExtra::kbl(caption = "Height MAPE model selection with LOO information criterion") %>% 
  kableExtra::kable_styling()
```

we can also look at the AIC-type model weights

```{r,eval=F}
brms::model_weights(
  height_mape_mod1_temp, height_mape_mod2_temp, height_mape_mod3_temp
  , height_mape_mod4_temp, height_mape_mod5_temp, height_mape_mod6_temp
) %>% 
  round(digits = 4)
```

we can also quickly look at the Bayeisan $R^2$ returned from the `brms::bayes_R2()` function

```{r}
dplyr::bind_rows(
  brms::bayes_R2(height_mape_mod1_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod1_temp")
  , brms::bayes_R2(height_mape_mod2_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod2_temp")
  , brms::bayes_R2(height_mape_mod3_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod3_temp")
  , brms::bayes_R2(height_mape_mod4_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod4_temp")
  , brms::bayes_R2(height_mape_mod5_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod5_temp")
  , brms::bayes_R2(height_mape_mod6_temp,summary=F) %>% dplyr::as_tibble() %>% dplyr::mutate(mod = "height_mape_mod6_temp")
) %>% 
  dplyr::mutate(mod = factor(mod)) %>% 
  ggplot2::ggplot(mapping=ggplot2::aes(y=R2, x = mod)) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
  ) +
  # ggplot2::facet_grid(cols = dplyr::vars(spectral_weight)) +
  # ggplot2::scale_fill_manual(values = pal_chm_res_m) +
  ggplot2::labs(x = "", y = "Bayesian R-squared") +
  ggplot2::theme_light()

```

the more complex models were selected as the best. because the selected model includes quadratic terms and multiple interactions parameter interpretation will be a challenge, so we will have to rely on plotting the modeled relationships rather than trying to interpret the coefficients.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Modeling

the fully factored Bayesian statistical model that details the likelihood, linear model, and priors used is the same as above for [Diameter MAPE](#mod_diam_mape2)

we reviewed the main effect parameter trends against MAPE [here](#mape_trends) and used these to guide our model design

```{r}
brms_height_mape_mod <- brms::brm(
  formula = pct_diff_height_m_mape ~
    0 + # no intercept to allow all values of spectral_weight to be shown instead of set as the baseline 
    max_ht_m + max_area_m2 + 
    convexity_pct + I(convexity_pct^2) + # changed from base model
    circle_fit_iou_pct + I(circle_fit_iou_pct^2) + # changed from base model
    convexity_pct:circle_fit_iou_pct + # changed from base model
    convexity_pct:chm_res_m + # changed from base model
    circle_fit_iou_pct:chm_res_m + # changed from base model
    convexity_pct:circle_fit_iou_pct:chm_res_m + # changed from base model
    chm_res_m + I(chm_res_m^2) + # changed from base model
    spectral_weight + chm_res_m:spectral_weight
  , data = param_combos_spectral_ranked # %>% dplyr::slice_sample(prop = 0.33)
  , family = Gamma(link = "log")
  # mcmc
  , iter = 14000, warmup = 7000
  , chains = 4
  # , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = lasR::half_cores()
  , file = paste0("../data/", "brms_height_mape_mod")
)
# brms::make_stancode(brms_height_mape_mod)
# brms::prior_summary(brms_height_mape_mod)
# print(brms_height_mape_mod)
# brms::neff_ratio(brms_height_mape_mod)
# brms::rhat(brms_height_mape_mod)
# brms::nuts_params(brms_height_mape_mod)
```

The `brms::brm` model summary

```{r}
brms_height_mape_mod %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | parameter == "phi"
  ) %>% 
  kableExtra::kbl(digits = 3, caption = "Bayesian model for Height MAPE") %>% 
  kableExtra::kable_styling()
```

note the quadratic coefficients ending in `E2`, Kruschke (2015) provides some insight on how to interpret:

>A quadratic has the form $y = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$. When $\beta_{2}$ is zero, the form reduces to a line. Therefore, this extended model can produce any fit that the linear model can. When $\beta_{2}$ is positive, a plot of the curve is a parabola that opens upward. When $\beta_{2}$ is negative, the curve is a parabola that opens downward. We have no reason to think that the curvature in the family-income data is exactly a parabola, but the quadratic trend might describe the data much better than a line alone. (p. 496)

### Posterior Predictive Checks

Markov chain Monte Carlo (MCMC) simulations were conducted using the brms package (Bürkner 2017) to estimate posterior predictive distributions of the parameters of interest. We ran 4 chains of 10,000 iterations with the first 5,000 discarded as burn-in. Trace-plots were utilized to visually assess model convergence.

check the trace plots for problems with convergence of the Markov chains

```{r, include=T, eval=T, fig.height=6.6}
plot(brms_height_mape_mod)
```

Sufficient convergence was checked with $\hat{R}$ values near 1 ([Brooks & Gelman, 1998](https://scholar.google.com/scholar?cluster=14209404114665352991&hl=en&as_sdt=0,6)). 

in the plot below, $\hat{R}$ values are colored using different shades (lighter is better). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: below 1.05 (good)
* mid: between 1.05 and 1.1 (ok)
* dark: above 1.1 (too high)

check our $\hat{R}$ values

```{r}
brms::mcmc_plot(brms_height_mape_mod, type = "rhat_hist") +
  ggplot2::scale_x_continuous(breaks = scales::breaks_extended(n = 6)) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain **not** to the sample size of the data where acceptable values allow "for reasonably accurate and stable estimates of the limits of the 95% HDI...If accuracy of the HDI limits is not crucial for your application, then a smaller ESS may be sufficient" [(Kruschke 2015, p. 184)](https://sites.google.com/site/doingbayesiandataanalysis/)

Ratios of effective sample size (ESS) to total sample size with values are colored using different shades (lighter is better). A ratio close to "1" (no autocorrelation) is ideal, while a low ratio suggests the need for more samples or model re-parameterization. Efficiently mixing MCMC chains are important because they guarantee the resulting posterior samples accurately represent the true distribution of model parameters, which is necessary for reliable and precise estimation of parameter values and their associated uncertainties (credible intervals). The chosen thresholds are somewhat arbitrary, but can be useful guidelines in practice ([Gabry and Mahr 2025](https://mc-stan.org/bayesplot/reference/MCMC-diagnostics.html)):

* light: between 0.5 and 1 (high)
* mid: between 0.1 and 0.5 (good)
* dark: below 0.1 (low)

```{r}
# and another effective sample size check
brms::mcmc_plot(brms_height_mape_mod, type = "neff_hist") +
# brms::mcmc_plot(brms_height_mape_mod, type = "neff") +
  ggplot2::scale_x_continuous(limits = c(0,NA), breaks = scales::breaks_extended(n = 9)) +
  # ggplot2::scale_color_discrete(drop = F) +
  # ggplot2::scale_fill_discrete(drop = F) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

Posterior predictive checks were used to evaluate model goodness-of-fit by comparing data simulated from the model with the observed data used to estimate the model parameters ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)). Calculating the proportion of MCMC iterations in which the test statistic (i.e., mean and sum of squares) from the simulated data and observed data are more extreme than one another provides the Bayesian p-value. Lack of fit is indicated by a value close to 0 or 1 while a value of 0.5 indicates perfect fit ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)).

To learn more about this approach to posterior predictive checks, check out Gabry’s (2025) vignette, [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). 

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data. our objective is to construct the model such that it faithfully represents the data.

```{r}
# posterior predictive check
brms::pp_check(
    brms_height_mape_mod
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  ggplot2::labs(subtitle = "posterior-predictive check (overlaid densities)") +
  ggplot2::theme_light() +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
    , plot.subtitle = ggplot2::element_text(size = 8)
    , plot.title = ggplot2::element_text(size = 9)
  )
```

another way

```{r}
brms::pp_check(brms_height_mape_mod, type = "ecdf_overlay", ndraws = 100) +
  ggplot2::labs(subtitle = "posterior-predictive check (ECDF: empirical cumulative distribution function)") + 
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = ggplot2::element_text(size = 14)
  )
```

### Conditional Effects

first, lets look at densities of the posterior samples per parameter

```{r}
brms::mcmc_plot(brms_height_mape_mod, type = "dens") +
  # ggplot2::theme_light() +
  ggplot2::theme(
    strip.text = ggplot2::element_text(size = 7.5, face = "bold", color = "black")
  )
```

and we can look at the default coefficient plot that is commonly used in reporting coefficient "significance" in frequentist analysis

```{r}
# easy way to get the default coeff plot
brms::mcmc_plot(brms_height_mape_mod, variable = "\\bb_", regex = T, type = "intervals")
```

Regarding interactions and polynomial models like the one we use, [McElreath (2015)](https://xcelab.net/rm/) notes:

>parameters are the linear and square components of the curve, respectively. But that doesn’t make them transparent. You have to plot these model fits to understand what they are saying. (p. 112-113)

all of the interactions and the quadradic trend of this model combine to make these coefficients by themselves uninterpretable as the coefficients are only meaningful in the context of the other terms in the interaction or by adding the quadratic component 

we can do this by checking for the main effects of the individual variables on Height MAPE (averages across all other effects)

```{r}
brms::conditional_effects(brms_height_mape_mod)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Posterior Predictive Expectation

we will test our hypotheses using the posterior distributions of the expected values (i.e., the posterior predictions of the mean) obtained via `tidybayes::add_epred_draws()`. our analysis will include two stages using parameter levels of the four structural parameters: `max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`.  in practice, these values should be informed by the treatment prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` are fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

as a reminder, here are those parameter levels

```{r}
structural_params_settings %>% dplyr::glimpse()
```

now we'll get the posterior predictive draws but over a range of `circle_fit_iou_pct` and `convexity_pct` including the best setting

```{r}
seq_temp <- seq(from = 0.05, to = 1.0, by = 0.1)
seq2_temp <- seq_temp[seq(1, length(seq_temp), by = 2)] # get every other element
# draws
draws_temp <- 
  # get the draws for levels of 
  # spectral_weight circle_fit_iou_pct convexity_pct
  tidyr::crossing(
    param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight)
    , circle_fit_iou_pct = seq_temp %>% unique()
    , convexity_pct = seq_temp %>% unique()
    , chm_res_m = seq(from = 0.1, to = 1.0, by = 0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  # dplyr::glimpse()
  tidybayes::add_epred_draws(brms_height_mape_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(
    is_seq = (convexity_pct %in% seq_temp) & (circle_fit_iou_pct %in% seq_temp)
  )
# # huh?
draws_temp %>% dplyr::glimpse()
```

#### Geometric shape regularity

let's look at the influence of the parameters that control the geometric shape regularity filtering: `circle_fit_iou_pct` and `convexity_pct`. to do this, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground.

In the first stage, we will fix the `max_ht_m` and `max_area_m2` parameters at levels expected based on the treatment and slash pile construction prescription implemented on the ground. We will then explore the influence of the two geometric shape filtering parameters (`circle_fit_iou_pct` and `convexity_pct`) over different levels of the `spectral_weight` parameter and CHM resolution data.

In the second stage, we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`) to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`). As in the first stage, the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

##### `circle_fit_iou_pct`

we need to look at the influence of `circle_fit_iou_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.2)
    , convexity_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(convexity_pct = factor(convexity_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct, y = value, color = convexity_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.95)
    , lwd = 1.1
    , fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both", scales = "free_y") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "mako", begin = 0.6, end = 0.1) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `circle_fit_iou_pct` on Height MAPE"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`circle_fit_iou_pct`"
    , y = "Height MAPE"
    , color = "`convexity_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

For all of these faceted plots, note the variation in the y-axis (Predicted Height MAPE) range across different CHM resolution levels. The influence of the `circle_fit_iou_pct` parameter on the Height MAPE is dependent on the CHM resolution but, unlike diameter quantification, its trend does not change based on the `convexity_pct` parameter setting. The magnitude of this influence decreases with finer data: for the finest resolution CHM data (0.1m), changes in `circle_fit_iou_pct` only shift the MAPE by approximately 6 percentage points, while for the coarsest resolution CHM data (0.5m), the shift in MAPE is approximately 11 percentage points as `circle_fit_iou_pct` is varied. There is a notable shift in the influence of the `circle_fit_iou_pct` parameter as CHM resolution moves from fine to coarse. With fine resolution CHM data (0.1m), height quantification accuracy improves (MAPE decreases) as the `circle_fit_iou_pct` parameter is increased toward its highest setting of '1', with improvements stabilizing around 0.6. For moderate resolution CHM data (0.3m), the parabolic relationship between the `circle_fit_iou_pct` parameter and the Height MAPE becomes evident. Height accuracy is optimized at low-intermediate levels of the `circle_fit_iou_pct` parameter (e.g., 0.25–0.5), and height quantification accuracy steeply declines (MAPE increases) for values set above this level. For coarse resolution CHM data (0.5m), there is a consistent decrease in height quantification accuracy (increase in MAPE) as the `circle_fit_iou_pct` parameter is increased toward its highest setting of '1'; this trend is opposite of what was found for the fine resolution CHM data. With coarse resolution CHM data, the best height accuracy is achieved at the lowest `circle_fit_iou_pct` parameter settings, near '0'. These seemingly conflicting trends likely occur because the coarser resolution CHM data smoothes out variation in height from the aerial point cloud, while the variability in the elevation profile within a detected slash pile is retained when using finer resolution data.

##### `convexity_pct`

we need to look at the influence of `convexity_pct` in the context of the other terms in the interaction

```{r, fig.height=9.3}
draws_temp %>%
  dplyr::ungroup() %>% 
  dplyr::filter(
    is_seq
    , chm_res_m %in% seq(0.1,0.5,by=0.2)
    , circle_fit_iou_pct %in% seq2_temp
  ) %>% 
  dplyr::mutate(circle_fit_iou_pct = factor(circle_fit_iou_pct, ordered = T)) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct, y = value, color = circle_fit_iou_pct)) +
  tidybayes::stat_lineribbon(
    point_interval = "median_hdi", .width = c(0.95)
    , lwd = 1.1
    , fill = NA
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), rows = dplyr::vars(chm_res_m), labeller = "label_both", scales = "free_y") +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "magma", begin = 0.5, end = 0.1) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(
    title = "conditional effect of `convexity_pct` on Height MAPE"
    # , subtitle = "Faceted by spectral_weight and chm_res_m"
    , x = "`convexity_pct`"
    , y = "Height MAPE"
    , color = "`circle_fit_iou_pct`"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
    , strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 10, color = "black", face = "bold")
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )

```

The model shows that the influence of the `convexity_pct` parameter on the Height MAPE is primarily conditional on the CHM resolution. The `circle_fit_iou_pct` parameter further influences the relationship between `convexity_pct` and height quantification accuracy but only at intermediate CHM resolutions (0.3m). Conversely, at the finest (0.1m) and coarsest (0.5m) CHM data levels, the `circle_fit_iou_pct` parameter does not alter the relationship between `convexity_pct` and Height MAPE but alters the magnitude of the influence. The magnitude of the `convexity_pct` parameter's influence on Height MAPE is lowest for intermediate CHM resolutions (0.3m), with changes in the parameter only resulting in approximately 1.5 percentage point shifts in MAPE. The influence is significantly larger for other resolutions, causing a 6 percentage point shift over the `convexity_pct` range at finer resolutions (e.g. 0.1m) and a 10 percentage point influence at coarser resolutions (e.g. 0.5m). There is a notable shift in the optimal trend of the `convexity_pct` parameter as CHM resolution changes. With fine resolution CHM data (0.1m), height quantification accuracy improves (MAPE decreases) in a very linear trend as the `convexity_pct` parameter is decreased toward its lowest setting of '0'. This indicates that allowing for less regular (more concave/less smooth) segments yields better height accuracy at the finest resolution. This trend reverses for coarse resolution CHM data (0.5m), where there is a consistent decrease in height quantification accuracy (increase in MAPE) as the `convexity_pct` parameter is decreased toward its lowest setting of '0'. With coarse resolution CHM data, the best height accuracy is achieved at the highest `convexity_pct` parameter settings, near '1'. This suggests that at coarser resolutions, a very strict shape requirement is necessary to minimize errors in height estimation. Additionally, for coarser resolution data, the penalty in height quantification accuracy for decreasing `convexity_pct` toward 0 is steeper when the `circle_fit_iou_pct` parameter is set to higher levels (e.g., greater than 0.65) than when `circle_fit_iou_pct` is set to lower values (e.g., less than 0.25). This heightened penalty occurs because simultaneously requiring a pile to be highly circular (`circle_fit_iou_pct` high) and allowing it to be highly irregular (`convexity_pct` low) introduces high geometric uncertainty into the segmented area used for height calculation, making such a parameter combination unlikely to be applicable for optimizing height quantification accuracy using this pile detection method.

##### Optimizing geometric filtering

Given the complexity of our model, which includes a non-linear link function and parameter interactions, calculating the optimal parameter values by solving for them algebraically from the model's coefficients would be prone to error. instead, we can use a robust Bayesian approach that leverages the model's posterior predictive distribution. This method is powerful because it inherently accounts for all sources of model uncertainty.

first, we'll generate a large number of predictions across a fine grid of parameter values (e.g. in steps of 0.01) for each posterior draw of the model coefficients. we'll generate a large number (e.g. 1000+) of posterior predictive draws for each combination of parameter values. for each posterior predictive draw, we'll then identify the parameter combination that maximizes the Height MAPE and we'll be left with a posterior distribution of optimal parameter combinations.

this approach demonstrates a key advantage of the Bayesian framework, allowing us to ask complex questions and find the most probable optimal parameter combination while fully accounting for uncertainty.

note, we only extract draws based on not using any spectral data (i.e. `spectral_weight` = 0) to save on plotting space and because we expect form quantification to only be minimally influenced by the inclusion of spectral data

```{r}
# let's get the draws at a very granular level
vertex_draws_temp <- 
  tidyr::crossing(
    param_combos_spectral_ranked %>% 
      dplyr::filter(spectral_weight=="0") %>% 
      dplyr::distinct(spectral_weight, spectral_weight_fact)
    , circle_fit_iou_pct = seq(from = 0.0, to = 1, by = 0.01) # very granular to identify vertex
    , convexity_pct = seq(from = 0.0, to = 1, by = 0.02) # very granular to identify vertex
    , chm_res_m = seq(0.1,0.5,by=0.1)
    , max_ht_m = structural_params_settings$max_ht_m
    , max_area_m2 = structural_params_settings$max_area_m2
  ) %>% 
  tidybayes::add_epred_draws(brms_height_mape_mod, ndraws = 1000, value = "value") %>% 
  dplyr::ungroup() %>% 
  # for each draw, get the highest Height MAPE by chm_res_m, spectral_weight
  # which we'll use to identify the optimal circle_fit_iou_pct,convexity_pct settings
  # these are essentially "votes" based on likelihood
  dplyr::group_by(
    .draw
    , chm_res_m, spectral_weight
  ) %>% 
  dplyr::arrange(value,circle_fit_iou_pct,convexity_pct) %>% # notice the ascending sort of value (mape) here to take the lowest 
  dplyr::slice(1)
# vertex_draws_temp %>% dplyr::glimpse() # this thing is huge
```

plot the posterior distribution of optimal parameter setting for `circle_fit_iou_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>%
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = circle_fit_iou_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  # ggplot2::scale_x_continuous(limits = c(0,1), breaks = scales::breaks_extended(6)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8)
  )
```

like we saw above, the optimal `circle_fit_iou_pct` for maximizing Height quantification accuracy shifts from higher (e.g. ~0.6-0.9) for fine resolution CHM data (e.g. <=0.2m) to lower (e.g. <0.25) for coarse resolution CHM data (e.g >=0.4m)

plot the posterior distribution of optimal parameter setting for `convexity_pct`

```{r, fig.height=8.5}
vertex_draws_temp %>% 
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = convexity_pct)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    , scales = "free_y"
    , labeller = "label_both"
    , axes = "all_x"
  ) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

when the `circle_fit_iou_pct` parameter is optimized, the influence of `convexity_pct` on form quantification accuracy is dependent on the CHM resolution. only for the finest resolution CHM data (i.e. 0.1m) tested, the model's predictions indicate with high certainty that the optimal `convexity_pct` is its minimum value of '0'. for intermediate resolution CHM data (e.g. 0.3m), the 95% HDI for the optimal `convexity_pct` spans the entire 0-1 range, indicating the model is not confident in any specific setting. for the coarsest resolution CHM data (i.e. 0.5m) tested, the model's predictions indicate with high certainty that the optimal `convexity_pct` is its maximum value of '1'

we can look at this another way, check it

```{r, fig.height=8.5}
vertex_draws_temp %>% 
  dplyr::filter(
    chm_res_m %in% seq(0.1,0.5,by=0.1)
  ) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y = convexity_pct, x = circle_fit_iou_pct)) +
  # geom_point(alpha=0.2) +
  ggplot2::geom_jitter(alpha=0.2, height = .01, width = .01) +
  ggplot2::facet_grid(
    cols = dplyr::vars(spectral_weight)
    , rows = dplyr::vars(chm_res_m)
    # , scales = "free_y"
    , labeller = "label_both"
  ) +
  ggplot2::scale_y_continuous(limits = c(0,1)) +
  ggplot2::scale_x_continuous(limits = c(0,1)) +
  ggplot2::theme_light() +
  ggplot2::theme(
    strip.text.x = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

note, in the plot above, we slightly "jitter" the points so that they are visible where they would otherwise be stacked on top of each other and only look like a few points instead of the 1000 draws from the posterior we used

let's table the HDI of the optimal values

```{r}
# summarize it
vertex_draws_temp <- 
  vertex_draws_temp %>% 
  dplyr::group_by(
    chm_res_m, spectral_weight
  ) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$y
    , median_hdi_lower_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymin
    , median_hdi_upper_circle_fit_iou_pct = tidybayes::median_hdci(circle_fit_iou_pct)$ymax
    # get median_hdi
    , median_hdi_est_convexity_pct = tidybayes::median_hdci(convexity_pct)$y
    , median_hdi_lower_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymin
    , median_hdi_upper_convexity_pct = tidybayes::median_hdci(convexity_pct)$ymax
  ) %>%
  dplyr::ungroup() 
# table it
vertex_draws_temp %>% 
  kableExtra::kbl(
    digits = 2
    , caption = ""
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , rep(c("median", "HDI low", "HDI high"),2)
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "circle_fit_iou_pct" = 3
    , "convexity_pct" = 3
  ))
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Input data

to explore the influence of the input data, which includes the presence or absence of spectral data and its weighting (i.e. `spectral_weight`) as well as the CHM resolution (`chm_res_m`), we will fix all four structural parameters (`max_ht_m`, `max_area_m2`, `convexity_pct`, `circle_fit_iou_pct`). the `max_ht_m`, `max_area_m2` parameters will be fixed at expected levels based on the slash pile construction prescription while the `convexity_pct`, `circle_fit_iou_pct` will be fixed at the optimal levels determined based on the Bayesian posterior predictive distribution of [detection accuracy](#f_score_epred)

we'll make contrasts of the posterior predictions to probabilistically quantify the influence of the input data (e.g. inclusion of spectral data and it's weighting and CHM resolution)

let's get the posterior predictive draws

```{r}
draws_temp <- 
  tidyr::crossing(
    structural_params_settings
    , param_combos_spectral_ranked %>% dplyr::distinct(spectral_weight_fact, spectral_weight)
    , param_combos_spectral_ranked %>% dplyr::distinct(chm_res_m)
  ) %>% 
  tidybayes::add_epred_draws(brms_height_mape_mod, ndraws = 1111) %>% 
  dplyr::rename(value = .epred)
# # huh?
# draws_temp %>% dplyr::glimpse()
```

##### CHM resolution

first, we'll look at the impact of changing CHM resolution by the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

our questions regarding CHM resolution were: 

* question 1: *does CHM resolution influence quantification accuracy?*
* question 2: *does the effect of CHM resolution change based on the inclusion of spectral data versus using only structural data?*

we can answer those questions using our model by considering the credible slope of the CHM resolution predictor as a function of the `spectral_weight` parameter (e.g. Kurz [2025](https://bookdown.org/content/3686/18.html); [Kruschke (2015, Ch. 18)](https://sites.google.com/site/doingbayesiandataanalysis/))

```{r}
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = chm_res_m, y = value, color = spectral_weight)) +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "CHM resolution", y = "Height MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

this trend demonstrates that increasing the coarseness of the CHM data has a minimal impact of about 1 percentage point on Height quantification accuracy for CHM data with resolutions in the range of 0.1m to 0.4m. however, increasing the CHM resolution coarseness beyond this level results in a more significant reduction in Height quantification accuracy (i.e. increase in height MAPE) and height accuracy degrades at an increasing rate the more coarse the CHM data...don't do it

we can look at the posterior distributions of the expected Height MAPE at different CHM resolution levels by the inclusion (or exclusion) of spectral data and it's weighting...make sure to note the y-axis range (it's fairly narrow)

```{r}
draws_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=chm_res_m)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = spectral_weight)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(spectral_weight), labeller = "label_both") +
  ggplot2::scale_fill_manual(values = pal_spectral_weight) +
  ggplot2::scale_y_continuous(limits = c(0,NA), labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "CHM resolution", y = "Height MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

table that

```{r}
draws_temp %>% 
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(spectral_weight, chm_res_m) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(spectral_weight, chm_res_m) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "Height MAPE<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "spectral_weight", "CHM resolution"
      , c("Height MAPE<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

note that even though these figures make it seem that height quantification accuracy is much worse for the coarser resolution CHM data compared to the finer resolution data, there is only a ~3 percentage point increase in height MAPE from the finest resolution 0.1m to the coarsest resolution 0.5m tested. for example, when spectral data is not included (i.e. `spectral_weight` = 0) the predicted Height MAPE is 15.3% using 0.1m CHM data and 18.1% when using 0.5m CHM data.

now we'll probabilistically test the hypothesis that coarser resolution CHM data results in lower Height quantification accuracy and determine by how much. we'll look at the influence of CHM resolution based on the inclusion of spectral data (or exclusion) and it's weighting determined by the `spectral_weight` parameter

```{r, fig.height=9}
contrast_temp <- 
  draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>%
  dplyr::group_by(spectral_weight) %>% 
  tidybayes::compare_levels(
    value
    , by = chm_res_m
    , comparison = 
      # "control"
      # tidybayes::emmeans_comparison("revpairwise") 
      "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = chm_res_m) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(spectral_weight, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(spectral_weight = forcats::fct_relabel(spectral_weight,~paste0("spectral_weight: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "CHM resolution contrast"
  , x_axis_title = "difference (Height MAPE)"
  , facet = "spectral_weight"
  , label_size = 0
  , x_expand = c(0.3,0.3)
  , annotate_which = "both"
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `spectral_weight`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      spectral_weight, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      , pr_gt_zero # , pr_lt_zero
    ) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    dplyr::arrange(spectral_weight, contrast) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "spectral_weight", "CHM res. contrast"
        , "difference (Height MAPE)"
        , "HDI low", "HDI high"
        , "Pr(diff>0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

these contrasts confirm what we saw by looking at the predicted relationship between CHM resolution and height MAPE, that increasing the coarseness of the CHM data has a minimal impact of about 1 percentage point on Height quantification accuracy for CHM data with resolutions in the range of 0.1m to 0.4m. however, increasing the CHM resolution coarseness beyond this level results in a more significant reduction in Height quantification accuracy (i.e. increase in height MAPE) and accuracy degrades at an increasing rate the more coarse the CHM data. for example, when spectral data is not included (i.e. `spectral_weight` = 0) the predicted difference in Height MAPE is ~1% when going from 0.1m to 0.2m-0.4m CHM data but a decrease in height quantification accuracy of ~4 percentage points is predicted going from 0.2m to 0.5m CHM data, for example.

##### Spectral data

now we'll look at the impact of including (or excluding) spectral data and the weighting of the `spectral_weight` parameter where a value of "0" indicates no spectral data was used (i.e. structural only), the lowest weighting of the spectral data is "1" (only one spectral index threshold must be met), and the highest weighting of spectral data is "5" (all spectral index thresholds must be met). 

*Note*: 
we expect that the spectral data does not significantly alter the quantification of slash pile form. this is because spectral information is used solely to filter candidate piles, meaning it neither reshapes existing ones nor introduces new detections.

```{r}
# viridis::rocket(5, begin = 0.9, end = 0.6) %>% scales::show_col()
# brms::posterior_summary(brms_height_mape_mod)
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = spectral_weight, y = value, color = factor(chm_res_m))) +
  # tidybayes::stat_halfeye() +
  tidybayes::stat_lineribbon(point_interval = "median_hdi", .width = c(0.95)) +
  # ggplot2::facet_wrap(facets = dplyr::vars(spectral_weight))
  ggplot2::scale_fill_brewer(palette = "Greys") +
  ggplot2::scale_color_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(labels = scales::percent) +
  ggplot2::labs(x = "`spectral_weight`", color = "CHM resolution", y = "Height MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  ) +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, lwd = 8, fill = NA))
    , fill = "none"
  )
```

while there is a slight change in height quantification accuracy at the highest spectral weighting, the inclusion of spectral data and the setting of the `spectral_weight` parameter does not significantly alter the height quantification accuracy irrespective of the CHM resolution

we can look at the posterior distributions of the expected Height MAPE at different `spectral_weight` settings by the input CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y=value, x=spectral_weight)) +
  tidybayes::stat_eye(
    mapping = ggplot2::aes(fill = chm_res_m)
    , point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "gray44", linewidth = 1
    , point_color = "gray44", point_fill = "gray44", point_size = 1
  ) +
  ggplot2::facet_grid(cols = dplyr::vars(chm_res_m)) +
  ggplot2::scale_fill_viridis_d(option = "rocket", begin = 0.9, end = 0.6) +
  ggplot2::scale_y_continuous(limits = c(0,NA), labels = scales::percent, breaks = scales::breaks_extended(16)) +
  ggplot2::labs(x = "`spectral_weight`", y = "Height MAPE") +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 8, color = "black", face = "bold")
  )
```

we already saw this same data above in our CHM resolution testing, but we'll table that again but this time grouping by CHM resolution

```{r}
draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T))) %>% 
  dplyr::group_by(chm_res_m, spectral_weight) %>%
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  # format pcts
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi_")
    , ~ scales::percent(.x, accuracy = 0.1)
  )) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(chm_res_m,spectral_weight) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "Height MAPE<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "CHM resolution", "spectral_weight"
      , c("Height MAPE<br>median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::scroll_box(height = "8in")
  
```

now we'll probabilistically test the hypothesis that the inclusion of spectral data improves detection accuracy and quantify by how much. we'll look at the influence of spectral data based on the CHM resolution

to actually compare the different levels of `spectral_weight`, we'll use the MCMC draws to contrast the posterior predictions at different levels of the parameter (see below)

```{r, fig.height=8.5}
contrast_temp <- 
  draws_temp %>% 
  dplyr::filter(
    round(chm_res_m,2) == round(chm_res_m,1) # let's just look at every 0.1 m (10 cm)
  ) %>% 
  dplyr::group_by(chm_res_m) %>% 
  tidybayes::compare_levels(
    value
    , by = spectral_weight
    , comparison = 
      # tidybayes::emmeans_comparison("revpairwise") 
      "pairwise"
  ) %>% 
  # dplyr::glimpse()
  dplyr::rename(contrast = spectral_weight) %>% 
  # group the data before calculating contrast variables %>% 
  dplyr::group_by(chm_res_m, contrast) %>% 
  make_contrast_vars() %>% 
  # relabel the label for the facets
  dplyr::mutate(chm_res_m = chm_res_m %>% factor() %>% forcats::fct_relabel(~paste0("CHM resolution: ", .x, recycle0 = T)))
# huh?
# contrast_temp %>% dplyr::glimpse()

# plot it
plt_contrast(
  contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "`spectral_weight` contrast"
  , x_axis_title = "difference (Height MAPE)"
  , facet = "chm_res_m"
  , label_size = 1.7
  , x_expand = c(0.4,0.3)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby `chm_res_m`"
  )
```

let's table it

```{r}
contrast_temp %>% 
    dplyr::distinct(
      chm_res_m, contrast
      , median_hdi_est, median_hdi_lower, median_hdi_upper
      # , pr_lt_zero 
      , pr_gt_zero
    ) %>% 
    dplyr::arrange(chm_res_m, contrast) %>% 
    # format pcts
    dplyr::mutate(dplyr::across(
      tidyselect::starts_with("median_hdi_")
      , ~ scales::percent(.x, accuracy = 0.1)
    )) %>% 
    kableExtra::kbl(
      digits = 2
      , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
      , col.names = c(
        "CHM resolution", "spectral_weight"
        , "difference (Height MAPE)"
        , "HDI low", "HDI high"
        # , "Pr(diff<0)"
        , "Pr(diff>0)"
      )
      , escape = F
    ) %>% 
    kableExtra::kable_styling() %>% 
    kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
    kableExtra::scroll_box(height = "8in")
```

although we can be very certain that there are differences in Height quantification accuracy (>99% probability across most CHM resolutions) when the `spectral_weight` is set to "5" (i.e. requiring that all spectral criteria be met) compared to the other settings of `spectral_weight` or not including spectral data for the finer resolution CHM data (e.g. <0.4m), these differences are so small with nearly all of them showing a difference in Height MAPE of <1%

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Balanced Accuracy Bayesian Optimization{#hey_balanced_acc}

now we're going to find the optimal parameter settings for our pile detection methodology by using the Bayesian models we developed for F-score (`brms_f_score_mod`) and MAPE for height (`brms_height_mape_mod`) and diameter (`brms_diam_mape_mod`). we'll use `tidybayes::add_epred_draws()` to obtain the posterior predictions of the expected value (e.g. F-score) from each model across a common grid of parameter settings (`newdata`). the predictions from each model will be joined on the parameter settings and draw number (`.draw`). then we'll use a customized ranking filter on each posterior draw to find the parameter combination that best balances detection and quantification accuracy. This approach allows us to weight the accuracies according to specific objectives, such as sacrificing some form quantification accuracy to gain better detection accuracy. this methodology is statistically sound because it accounts for uncertainty by using the full posterior distribution from all models throughout the optimization process. unlike methods that rely on single point estimates (like our initial sensitivity testing), this approach can handle complex, non-linear, and interactive relationships. The final output is not a single optimal solution but a *posterior distribution of optimal settings*, which quantifies the range of credible parameter values that satisfy the optimization criteria.

first, let's generate the grid to see how many different parameter combinations we're testing. note, we hold the `max_ht_m` (`r structural_params_settings$max_ht_m` m) and `max_area_m2` (`r structural_params_settings$max_area_m2` m^2^) parameters constant based on our expectations from the treatment prescription implemented on the ground. if we didn't do this, we would risk over-fitting the optimization based on the training data used.

```{r}
# grid of parameter settings for use in each tidybayes::add_epred_draws() call
newdata_temp <- tidyr::crossing(
  param_combos_spectral_ranked %>% 
    # dplyr::filter(spectral_weight %in% c("0","4","5")) %>% 
    dplyr::distinct(spectral_weight)
  , circle_fit_iou_pct = seq(from = 0, to = 1.0, by = 0.02)
  , convexity_pct = seq(from = 0, to = 1.0, by = 0.02)
  , chm_res_m = seq(from = 0.1, to = 0.5, by = 0.05)
  , max_ht_m = structural_params_settings$max_ht_m # seq(from = 2.1, to = 4.3, length.out = 5)
  , max_area_m2 = structural_params_settings$max_area_m2 # seq(from = 20, to = 55, length.out = 5)
)
# huh?
newdata_temp %>% dplyr::glimpse()
# nrow(newdata_temp)/nrow(param_combos_spectral_ranked)
```

the number of records in our `newdata` is the number of different parameter combinations we're testing for each draw. modelling allows us to test so many more possible combinations (`r scales::comma(nrow(newdata_temp)/nrow(param_combos_spectral_ranked),accuracy = 0.1)` times more here) than getting point estimates as we did with our sensitivity testing!

get the posterior predictions from the detection accuracy model F-score (`brms_f_score_mod`) and MAPE for height (`brms_height_mape_mod`) and diameter (`brms_diam_mape_mod`) for quantification accuracy

```{r}
# names of variables to join the draws on 
join_names_temp <- c(names(newdata_temp), ".draw")
ndraws_temp <- 333
# draws
full_draws_temp <- 
  # brms_f_score_mod
  tidybayes::add_epred_draws(newdata = newdata_temp, object = brms_f_score_mod, ndraws = ndraws_temp, value = "f_score") %>% 
  # join brms_diam_mape_mod
  dplyr::inner_join(
    tidybayes::add_epred_draws(newdata = newdata_temp, object = brms_diam_mape_mod, ndraws = ndraws_temp, value = "diameter_mape")  
    , by = join_names_temp
  ) %>% 
  # join brms_height_mape_mod
  dplyr::inner_join(
    tidybayes::add_epred_draws(newdata = newdata_temp, object = brms_height_mape_mod, ndraws = ndraws_temp, value = "height_mape")  
    , by = join_names_temp
  ) %>% 
  # select
  dplyr::select(dplyr::all_of(
    c(join_names_temp, "f_score", "diameter_mape", "height_mape")
  ))
# huh?
full_draws_temp %>% dplyr::glimpse()
```

the number of records should be equal to the number of records in our `newdata` multiplied by the number of draws (`ndraws`)

```{r}
identical(
  as.numeric(nrow(newdata_temp)*ndraws_temp)
  , nrow(full_draws_temp) %>% as.numeric()
)
```

### Balanced Accuracy Selection

now we'll use a customized ranking filter on each posterior draw to find the parameter combination that best balances detection and quantification accuracy. This approach allows us to weight the accuracies according to specific objectives, such as sacrificing some form quantification accuracy to gain better detection accuracy

```{r}
# weights need to add to 1
weight_detection_temp <- 0.7
weight_quantification_temp <- 0.3
# rank the draws
best_balanced_accuracy_combos <- 
  full_draws_temp %>% 
  dplyr::group_by(.draw) %>% 
  dplyr::mutate(
    pct_rank_detection = dplyr::percent_rank(f_score) 
    , dplyr::across(
      .cols = tidyselect::ends_with("_mape")
      , .fn = ~dplyr::percent_rank(-.x) # -mape so largest gets highest pct rank
      , .names = "{.col}_pct_rank"
    )
  ) %>% 
  # average of mapes to get quantification mape average
  dplyr::mutate(
    pct_rank_quantification = (height_mape_pct_rank+diameter_mape_pct_rank)/2
    # apply weights to get balanced_accuracy (0-1) with 1 being best
    , balanced_accuracy = (weight_detection_temp*pct_rank_detection) + (weight_quantification_temp*pct_rank_quantification)
  ) %>% 
  # dplyr::filter(.draw==1) %>% dplyr::arrange(desc(balanced_accuracy)) %>% View()
  # for each draw, find the setting that maximizes balanced_accuracy
  dplyr::group_by(.draw) %>%
  dplyr::slice_max(order_by = balanced_accuracy, n = 1, with_ties = FALSE) %>%
  dplyr::ungroup() %>% 
  dplyr::select(-tidyselect::starts_with("pct_rank_"), -tidyselect::ends_with("_pct_rank"), -balanced_accuracy)
# add on the structural only data
best_balanced_accuracy_combos <- 
  full_draws_temp %>% 
  dplyr::filter(spectral_weight=="0") %>% 
  dplyr::group_by(.draw) %>% 
  dplyr::mutate(
    pct_rank_detection = dplyr::percent_rank(f_score) 
    , dplyr::across(
      .cols = tidyselect::ends_with("_mape")
      , .fn = ~dplyr::percent_rank(-.x) # -mape so largest gets highest pct rank
      , .names = "{.col}_pct_rank"
    )
  ) %>% 
  # average of mapes to get quantification mape average
  dplyr::mutate(
    pct_rank_quantification = (height_mape_pct_rank+diameter_mape_pct_rank)/2
    # apply weights to get balanced_accuracy (0-1) with 1 being best
    , balanced_accuracy = (weight_detection_temp*pct_rank_detection) + (weight_quantification_temp*pct_rank_quantification)
  ) %>% 
  # dplyr::filter(.draw==1) %>% dplyr::arrange(desc(balanced_accuracy)) %>% View()
  # for each draw, find the setting that maximizes balanced_accuracy
  dplyr::group_by(.draw) %>%
  dplyr::slice_max(order_by = balanced_accuracy, n = 1, with_ties = FALSE) %>%
  dplyr::ungroup() %>% 
  dplyr::select(-tidyselect::starts_with("pct_rank_"), -tidyselect::ends_with("_pct_rank"), -balanced_accuracy) %>% 
  # add on data fusion combos
  dplyr::mutate(method_input_data = 0) %>% 
  dplyr::bind_rows(
    best_balanced_accuracy_combos %>% dplyr::mutate(method_input_data = 1)
  ) %>% 
  dplyr::mutate(
    method_input_data = factor(method_input_data, levels = 0:1, labels = c("structural only", "structural+spectral"), ordered = T)
  )

# huh?  
best_balanced_accuracy_combos %>% dplyr::glimpse()
# best_balanced_accuracy_combos %>% dplyr::select(max_area_m2) %>% summary()
```

the number of records `r scales::comma(nrow(best_balanced_accuracy_combos %>% dplyr::filter(method_input_data=="structural+spectral")),accuracy=1)` corresponds to the number of draws we got from the posterior predictive distribution, each one selected as the parameter combination that best balanced detection and quantification accuracy from the `r scales::comma(nrow(newdata_temp),accuracy=1)` tested for the data fusion approach

this output is not a single optimal solution but a *posterior distribution of optimal settings*, which quantifies the range of credible parameter values that satisfy the optimization criteria.

we'll have data for both the structural only method and the data fusion method (i.e. "structural+spectral")

```{r}
best_balanced_accuracy_combos %>% 
  dplyr::count(method_input_data)
```

we also tested different parameter combinations in the absence of spectral data (i.e. `spectral_weight` = 0) so that we get optimal settings for cases when we only have structural data to attempt to detect piles from

the number of parameter combinations tested per draw with structural data only is equivalent to the number of combinations tested per draw for the data fusion approach divided by six (for `spectral_weight` setting "1"-"5" and without spectral data "0"): 

```{r}
full_draws_temp %>% 
  dplyr::filter(spectral_weight=="0") %>% 
  nrow() %>% 
  `/`(ndraws_temp) %>% 
  scales::comma(accuracy=1)
```

pivot to long

```{r}
# pivot to long
best_balanced_accuracy_combos_long <- 
  best_balanced_accuracy_combos %>% 
  # get rid of vars we fixed
  dplyr::select(-c(max_ht_m, max_area_m2)) %>% 
  dplyr::mutate(
    spectral_weight = spectral_weight %>% as.character() %>% as.numeric()
  ) %>% 
  tidyr::pivot_longer(cols = -c(.draw,method_input_data))
```

let's get the same data but by different CHM resolution levels tested to represent the optimal setting if CHM resolution is fixed and cannot be selected

```{r}
# let's get the same data but by different CHM resolution levels tested to represent the optimal setting if CHM resolution is fixed and cannot be selected
# rank the draws
best_balanced_accuracy_combos_chm <- 
  full_draws_temp %>% 
  dplyr::group_by(.draw, chm_res_m) %>% 
  dplyr::mutate(
    pct_rank_detection = dplyr::percent_rank(f_score) 
    , dplyr::across(
      .cols = tidyselect::ends_with("_mape")
      , .fn = ~dplyr::percent_rank(-.x) # -mape so largest gets highest pct rank
      , .names = "{.col}_pct_rank"
    )
  ) %>% 
  # average of mapes to get quantification mape average
  dplyr::mutate(
    pct_rank_quantification = (height_mape_pct_rank+diameter_mape_pct_rank)/2
    # apply weights to get balanced_accuracy (0-1) with 1 being best
    , balanced_accuracy = (weight_detection_temp*pct_rank_detection) + (weight_quantification_temp*pct_rank_quantification)
  ) %>% 
  # dplyr::filter(.draw==1) %>% dplyr::arrange(desc(balanced_accuracy)) %>% View()
  # for each draw, find the setting that maximizes balanced_accuracy
  dplyr::group_by(.draw, chm_res_m) %>%
  dplyr::slice_max(order_by = balanced_accuracy, n = 1, with_ties = FALSE) %>%
  dplyr::ungroup() %>% 
  dplyr::select(-tidyselect::starts_with("pct_rank_"), -tidyselect::ends_with("_pct_rank"), -balanced_accuracy)
# add on the structural only data
best_balanced_accuracy_combos_chm <- 
  full_draws_temp %>% 
  dplyr::filter(spectral_weight=="0") %>% 
  dplyr::group_by(.draw, chm_res_m) %>% 
  dplyr::mutate(
    pct_rank_detection = dplyr::percent_rank(f_score) 
    , dplyr::across(
      .cols = tidyselect::ends_with("_mape")
      , .fn = ~dplyr::percent_rank(-.x) # -mape so largest gets highest pct rank
      , .names = "{.col}_pct_rank"
    )
  ) %>% 
  # average of mapes to get quantification mape average
  dplyr::mutate(
    pct_rank_quantification = (height_mape_pct_rank+diameter_mape_pct_rank)/2
    # apply weights to get balanced_accuracy (0-1) with 1 being best
    , balanced_accuracy = (weight_detection_temp*pct_rank_detection) + (weight_quantification_temp*pct_rank_quantification)
  ) %>% 
  # dplyr::filter(.draw==1) %>% dplyr::arrange(desc(balanced_accuracy)) %>% View()
  # for each draw, find the setting that maximizes balanced_accuracy
  dplyr::group_by(.draw, chm_res_m) %>%
  dplyr::slice_max(order_by = balanced_accuracy, n = 1, with_ties = FALSE) %>%
  dplyr::ungroup() %>% 
  dplyr::select(-tidyselect::starts_with("pct_rank_"), -tidyselect::ends_with("_pct_rank"), -balanced_accuracy) %>% 
  # add on data fusion combos
  dplyr::mutate(method_input_data = 0) %>% 
  dplyr::bind_rows(
    best_balanced_accuracy_combos_chm %>% dplyr::mutate(method_input_data = 1)
  ) %>% 
  dplyr::mutate(
    method_input_data = factor(method_input_data, levels = 0:1, labels = c("structural only", "structural+spectral"), ordered = T)
  )
# huh?  
best_balanced_accuracy_combos_chm %>% dplyr::glimpse()
# best_balanced_accuracy_combos_chm %>% 
#   dplyr::ungroup() %>% 
#   dplyr::count(chm_res_m,method_input_data)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
# # best_balanced_accuracy_combos_chm %>% readr::write_csv("c:/Users/georg/Downloads/best_balanced_accuracy_combos_chm.csv")
# # best_balanced_accuracy_combos %>% readr::write_csv("c:/Users/georg/Downloads/best_balanced_accuracy_combos.csv")
# best_balanced_accuracy_combos_chm <- readr::read_csv("c:/Users/georg/Downloads/best_balanced_accuracy_combos_chm.csv")
# best_balanced_accuracy_combos <- readr::read_csv("c:/Users/georg/Downloads/best_balanced_accuracy_combos.csv")
```

### Overall (across CHM resolution)

these recommendations are for users who can generate a CHM from the original point cloud (e.g. using `cloud2trees::cloud2raster()`). This is a critical distinction because creating a new CHM at the desired resolution is a fundamentally different process than simply disaggregating an existing, coarser raster.

#### Posterior distribution of optimal settings

let's check out the posterior distribution of optimal settings for each parameter. remember, it is these parameter settings *combined* that are expected to yield the best balance between detection and quantification accuracy based on our weighting

note, we distinguish the optimal settings based on the availability of spectral data which determines whether or not we can use a data fusion (i.e. "structural+spectral") approach

```{r}
# plot it
best_balanced_accuracy_combos_long %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape")) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(method_input_data)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  # ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1.2)) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "predicted optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text.x = ggplot2::element_text(size = 9, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 11, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8, angle = 90)
  )
```

let's table the HDI of the optimal values

```{r}
# let's save these optimal settings for later use
# summarize it
optimal_param_settings <- best_balanced_accuracy_combos_long %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape")) %>% 
  dplyr::group_by(method_input_data,name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup()
# table it
optimal_param_settings %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~dplyr::case_when(
      name == "spectral_weight" ~ scales::comma(.x,accuracy=1)
      , T ~scales::comma(.x,accuracy=.01)
    )
  )) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
     "predicted optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      ".", "parameter"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "optimal setting" = 3
  ))
```

based on the training data and our model, we have a high degree of certainty that these optimal parameter settings will return the best balanced accuracy. this is evidenced by the 95% highest density intervals (HDIs) of the posterior distribution of optimal solutions, which are either very narrow or centered on a single parameter setting, indicating low uncertainty in the model's recommendation. However, it is crucial to remember that these settings should be refined based on the *actual, on-the-ground treatment prescription and implementation*. For example, if a prescription called for piles to be constructed as half-frustum of a cone with rounded ends (lolwut; [Hardy 1996](https://scholar.google.com/scholar?cluster=3292666862612204856&hl=en&as_sdt=0,6)), these settings would be inappropriate since they assume mostly circular pile footprints which the method (based on the `convexity_pct` and `circle_fit_iou_pct` settings) would not be suited for

#### Posterior distribution of accuracy

what can we expect from a detection and form quantification accuracy perspective?

note, we distinguish the predicted accuracies based on the availability of spectral data which determines whether or not we can use a data fusion (i.e. "structural+spectral") approach

```{r}
# plot it
best_balanced_accuracy_combos_long %>% 
  dplyr::ungroup() %>% 
  # filter for accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(method_input_data)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::scale_x_continuous(labels = scales::percent) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "predicted pile detection and form quantification accuracy metrics"
     , "\nusing optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8, angle = 90)
  )
```

let's table the HDI of the predicted accuracy metrics

```{r}
# summarize it
best_balanced_accuracy_combos_long %>% 
  dplyr::ungroup() %>% 
  # filter for accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  dplyr::group_by(method_input_data, name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~ scales::percent(.x,accuracy=.1)
  )) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
     "predicted pile detection and form quantification accuracy metrics"
     , "<br>using optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      ".", "metric"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "predicted value" = 3
  ))
```

The narrow 95% highest density intervals (HDIs) of the posterior distribution for the accuracy metrics indicate a high degree of confidence that the method is capable of high detection and form quantification accuracy. This predicted performance, however, is *contingent on using the specific optimal parameter settings* identified in this analysis. These expectations are specific to the training data, which consisted of piles that were generally circular with few irregularities in their footprint. As such, these results should not be anticipated if the actual on-the-ground pile prescription and implementation differ from those of this training data. Similarly, the same level of accuracy should not be expected if the parameterization of the detection method is altered, even when applied to piles with similar shapes and structures.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(best_balanced_accuracy_combos_long)
gc()
```

### by CHM resolution

these recommendations are for users who *cannot* generate a CHM from the original point cloud and only have access to a CHM at a provided resolution (assuming that resolution is <= `r max(best_balanced_accuracy_combos_chm$chm_res_m)`)

```{r}
# pivot to long
best_balanced_accuracy_combos_long <- 
  best_balanced_accuracy_combos_chm %>% 
  # get rid of vars we fixed
  dplyr::select(-c(max_ht_m, max_area_m2)) %>% 
  dplyr::mutate(
    spectral_weight = spectral_weight %>% as.character() %>% as.numeric()
    , chm_res_m_desc = paste0(chm_res_m, "m CHM") %>% factor() %>% forcats::fct_reorder(chm_res_m)
  ) %>% 
  tidyr::pivot_longer(cols = -c(.draw,method_input_data,chm_res_m,chm_res_m_desc))
# best_balanced_accuracy_combos_long %>% dplyr::glimpse()
# best_balanced_accuracy_combos_long %>% dplyr::ungroup() %>% dplyr::count(name)
```

#### Posterior distribution of optimal settings

let's check out the posterior distribution of optimal settings for each parameter. remember, it is these parameter settings *combined* that are expected to yield the best balance between detection and quantification accuracy based on our weighting

##### Data Fusion

let's start with the data fusion approach which assumes users have RGB data to complement the structural CHM data

```{r, fig.height=9.6}
# plot it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural+spectral") %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape")) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(chm_res_m_desc)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  # ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1.2)) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "Data Fusion (structural+spectral)"
     , "\npredicted optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text.x = ggplot2::element_text(size = 9, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 7, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 7, angle = 90)
  )
```

let's table the HDI of the optimal values

```{r}
# summarize it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural+spectral") %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape")) %>% 
  dplyr::group_by(chm_res_m_desc,method_input_data,name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~dplyr::case_when(
      name == "spectral_weight" ~ scales::comma(.x,accuracy=1)
      , T ~scales::comma(.x,accuracy=.01)
    )
  )) %>% 
  dplyr::select(-method_input_data) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
      "Data Fusion (structural+spectral)"
      , "<br>predicted optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      "CHM resolution", "parameter"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "optimal setting" = 3
  )) %>% 
  kableExtra::scroll_box(height = "8in")
```

##### Structural only

now we'll consider an approach that only uses structural data without the benefit of supplemental spectral data

```{r, fig.height=9.6}
# plot it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural only") %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape"), name!="spectral_weight") %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(chm_res_m_desc)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  # ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = 1.2)) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "Structural only"
     , "\npredicted optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text.x = ggplot2::element_text(size = 9, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 7, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 7, angle = 90)
  )
```

let's table the HDI of the optimal values

```{r}
# summarize it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural only") %>% 
  # filter out accuracy metrics
  dplyr::filter(name!="f_score", !stringr::str_ends(name, "_mape")) %>% 
  dplyr::group_by(chm_res_m_desc,method_input_data,name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~dplyr::case_when(
      name == "spectral_weight" ~ scales::comma(.x,accuracy=1)
      , T ~scales::comma(.x,accuracy=.01)
    )
  )) %>% 
  dplyr::select(-method_input_data) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
      "Structural only"
      , "<br>predicted optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      "CHM resolution", "parameter"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "optimal setting" = 3
  )) %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Posterior distribution of accuracy

what can we expect from a detection and form quantification accuracy perspective?

##### Data Fusion

let's start with the data fusion approach which assumes users have RGB data to complement the structural CHM data

```{r, fig.height=9.6}
# plot it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural+spectral") %>% 
  # filter out accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(chm_res_m_desc)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::scale_x_continuous(labels = scales::percent) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "Data Fusion (structural+spectral)"
     , "\npredicted pile detection and form quantification accuracy metrics"
     , "\nusing optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text.x = ggplot2::element_text(size = 9, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 7, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 7, angle = 90)
  )
```

let's table the HDI of the predicted accuracy metrics

```{r}
# summarize it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural+spectral") %>% 
  dplyr::ungroup() %>% 
  # filter for accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  dplyr::group_by(chm_res_m_desc, method_input_data, name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~ scales::percent(.x,accuracy=.1)
  )) %>% 
  dplyr::select(-method_input_data) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
     "Data Fusion (structural+spectral)"
     , "<br>predicted pile detection and form quantification accuracy metrics"
     , "<br>using optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      "CHM Resolution", "metric"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "predicted value" = 3
  )) %>% 
  kableExtra::scroll_box(height = "8in")
```

##### Structural only

now we'll consider an approach that only uses structural data without the benefit of supplemental spectral data

```{r, fig.height=9.6}
# plot it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural only") %>% 
  # filter out accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.95)
    , quantiles = 100
    , point_size = 3 
  ) +
  ggplot2::facet_grid(
    rows = dplyr::vars(chm_res_m_desc)
    , cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::scale_x_continuous(labels = scales::percent) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
     "Structural only"
     , "\npredicted pile detection and form quantification accuracy metrics"
     , "\nusing optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text.x = ggplot2::element_text(size = 9, color = "black", face = "bold")
    , strip.text.y = ggplot2::element_text(size = 7, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 7, angle = 90)
  )
```

let's table the HDI of the predicted accuracy metrics

```{r}
# summarize it
best_balanced_accuracy_combos_long %>% 
  dplyr::filter(method_input_data=="structural only") %>% 
  dplyr::ungroup() %>% 
  # filter for accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  dplyr::group_by(chm_res_m_desc, method_input_data, name) %>% 
  dplyr::summarise(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdci(value)$y
    , median_hdi_lower = tidybayes::median_hdci(value)$ymin
    , median_hdi_upper = tidybayes::median_hdci(value)$ymax
  ) %>%
  dplyr::ungroup() %>% 
  dplyr::mutate(dplyr::across(
    tidyselect::starts_with("median_hdi")
    , ~ scales::percent(.x,accuracy=.1)
  )) %>% 
  dplyr::select(-method_input_data) %>% 
  # table it
  kableExtra::kbl(
    caption = paste0(
     "Structural only"
     , "<br>predicted pile detection and form quantification accuracy metrics"
     , "<br>using optimal parameter settings based on both detection and quantification accuracy"
    )
    , col.names = c(
      "CHM Resolution", "metric"
      , c("median", "HDI low", "HDI high")
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>% 
  kableExtra::add_header_above(c(
    " "=2
    , "predicted value" = 3
  )) %>% 
  kableExtra::scroll_box(height = "8in")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Balanced Accuracy Validation{#training_detect_final}

let's test these optimal settings on the actual data to see how close our model came to properly predicting the accuracies. we'll test using the data fusion method since we have the RGB data

### Detection

remember the optimal parameter settings we identified assuming we have spectral data for a data fusion approach? we'll use those settings for the structural parameters and set the `spectral_weight` parameter to the lower end of it's 95% HDI to be less restrictive with the spectral filtering (e.g. if HDI includes '4' and '5', use '4') while maintaining high anticipated accuracy because these HDI's include the full range of optimal settings based on our balanced accuracy. 

```{r}
# remember the optimal_param_settings!
optimal_temp <- 
  optimal_param_settings %>%
  dplyr::filter(method_input_data == "structural+spectral") %>% 
  dplyr::mutate(
    median_hdi_est = dplyr::case_when(
      name == "spectral_weight" ~ median_hdi_lower
      , T ~ median_hdi_est
    )
  ) %>% 
  dplyr::select(name,median_hdi_est) %>% 
  tidyr::pivot_wider(names_from = name, values_from = median_hdi_est) %>% 
  # add on the fixed values
  dplyr::bind_cols(
    structural_params_settings %>% dplyr::select(max_ht_m,max_area_m2)
  )
# huh?
optimal_temp %>% dplyr::glimpse()
```

first, we need to read in the CHM data at the optimal resolution as predicted by the model

```{r}
# set chm res
chm_res_m_temp <- optimal_temp$chm_res_m
dir_temp <- paste0("../data/point_cloud_processing_delivery_chm",chm_res_m_temp,"m")
# do it
if(!dir.exists(dir_temp)){
  # cloud2trees
  cloud2raster_ans <- cloud2trees::cloud2raster(
    output_dir = "../data"
    , input_las_dir = "f:\\PFDP_Data\\p4pro_images\\P4Pro_06_17_2021_half_half_optimal\\2_densification\\point_cloud"
    , accuracy_level = 2
    , keep_intrmdt = T
    , dtm_res_m = 0.25
    , chm_res_m = chm_res_m_temp
    , min_height = 0 # effectively generates a DSM based on non-ground points
  )
  # rename
  file.rename(from = "../data/point_cloud_processing_delivery", to = dir_temp)
}else{
  dtm_temp <- terra::rast( file.path(dir_temp, "dtm_0.25m.tif") )
  chm_temp <- terra::rast( file.path(dir_temp, paste0("chm_", chm_res_m_temp,"m.tif")) )
  
  cloud2raster_ans <- list(
    "dtm_rast" = dtm_temp
    , "chm_rast" = chm_temp
  )
}
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(dtm_temp,chm_temp,chm_res_m_temp,dir_temp)
gc()
```

we'll only work with the CHM in the study unit boundary plus a buffer to limit the amount of data we process

```{r}
chm_rast_temp <- cloud2raster_ans$chm_rast %>% 
  terra::crop(
    stand_boundary %>% 
      sf::st_buffer(2) %>% 
      terra::vect() %>% 
      terra::project(terra::crs(cloud2raster_ans$chm_rast))
  ) %>% 
  terra::mask(
    stand_boundary %>% 
      sf::st_buffer(2) %>% 
      terra::vect() %>% 
      terra::project(terra::crs(cloud2raster_ans$chm_rast))
  )
# # huh?
# chm_rast_temp %>%
#   terra::aggregate(fact = 2, na.rm=T) %>%  #, fun = "median", cores = lasR::half_cores(), na.rm = T) %>%
#   terra::plot(col = viridis::plasma(100), axes = F)
#   terra::plot(
#     stand_boundary %>%
#       terra::vect() %>%
#       terra::project(terra::crs(cloud2raster_ans$chm_rast))
#     , add = T, border = "black", col = NA, lwd = 1.2
#   )
#   terra::plot(
#     slash_piles_polys %>%
#       terra::vect() %>%
#       terra::project(terra::crs(cloud2raster_ans$chm_rast))
#     , add = T, border = "blue", col = NA, lwd = 1.2
#   )
```

we're going to use our handy-dandy `slash_pile_detect_watershed()` function we defined in [this earlier section](#slash_pile_detect_watershed). 

```{r}
outdir_temp <- "../data/PFDP_Data/PFDP_SlashPiles/"
fnm_temp <- file.path(outdir_temp,"structural_candidate_segments.gpkg")
if(!file.exists(fnm_temp)){
  set.seed(77)
  slash_pile_detect_watershed_ans <- slash_pile_detect_watershed(
    chm_rast = chm_rast_temp
    #### height and area thresholds for the detected piles
    # these should be based on data from the literature or expectations based on the prescription
    , max_ht_m = optimal_temp$max_ht_m # set the max expected pile height
    , min_ht_m = 0.5 # set the min expected pile height
    , min_area_m2 = 2 # set the min expected pile area # (5*0.3048)^2 = prescription minimum = 2.322 # ((5*0.95)*0.3048)^2 = 2.1 = 5% less than minimum
    , max_area_m2 = optimal_temp$max_area_m2 # set the max expected pile area
    #### irregularity filtering
    # 1 = perfectly convex (no inward angles); 0 = so many inward angles
    # values closer to 1 remove more irregular segments; 
      # values closer to 0 keep more irregular segments (and also regular segments)
    # these will all be further filtered for their circularity and later smoothed to remove blocky edges
    # and most inward angles by applying a convex hull to the original detected segment
    , convexity_pct = optimal_temp$convexity_pct # min required overlap between the predicted pile and the convex hull of the predicted pile
    #### circularity filtering
    # 1 = perfectly circular; 0 = not circular (e.g. linear) but also circular
    # min required IoU between the predicted pile and the best fit circle of the predicted pile
    , circle_fit_iou_pct = optimal_temp$circle_fit_iou_pct
    #### shape refinement & overlap removal
    ## smooth_segs = T ... convex hulls of raster detected segments are returned, any that overlap are removed
    ## smooth_segs = F ... raster detected segments are returned (blocky) if they meet all prior rules
    , smooth_segs = T
  )
  # save
  slash_pile_detect_watershed_ans %>% sf::st_write(fnm_temp, append = F)
}else{
  slash_pile_detect_watershed_ans <- sf::st_read(fnm_temp, quiet=T)
}
# what did we get?
slash_pile_detect_watershed_ans %>% dplyr::glimpse()
```

```{r, include=F,eval=F}
# CHM with the predicted piles (brown) and the ground-truth piles (blue)
chm_rast_temp %>% 
  terra::aggregate(fact = 2, na.rm=T) %>%  #, fun = "median", cores = lasR::half_cores(), na.rm = T) %>% 
  terra::plot(col = viridis::plasma(100, alpha = 0.7), axes = F)
  terra::plot(
    stand_boundary %>% 
      terra::vect() %>% 
      terra::project(terra::crs(cloud2raster_ans$chm_rast))
    , add = T, border = "black", col = NA, lwd = 1.2
  )
  terra::plot(
    slash_piles_polys %>% 
      terra::vect() %>% 
      terra::project(terra::crs(cloud2raster_ans$chm_rast))
    , add = T, border = "blue", col = NA, lwd = 1.6
  )
  terra::plot(
    slash_pile_detect_watershed_ans %>% 
      terra::vect() %>% 
      terra::project(terra::crs(cloud2raster_ans$chm_rast))
    , add = T, border = "brown", col = NA, lwd = 2
  )
```

Now we'll filter the structurally-detected candidate slash piles using the RGB spectral data with the `polygon_spectral_filtering()` function we defined in [this earlier section](#polygon_spectral_filtering). if you were wondering, yes, this function is also handy-dandy.

```{r, results='hide', message=FALSE}
final_predicted_slash_piles <- polygon_spectral_filtering(
  sf_data = slash_pile_detect_watershed_ans
  , rgb_rast = ortho_rast
  # define the band index
  , red_band_idx = 1
  , green_band_idx = 2
  , blue_band_idx = 3
  # spectral weighting
  , spectral_weight = optimal_temp$spectral_weight
)
# add is_in_stand
final_predicted_slash_piles <- final_predicted_slash_piles %>% 
  dplyr::mutate(
    is_in_stand = pred_id %in% (
      final_predicted_slash_piles %>% 
      sf::st_intersection(stand_boundary %>% sf::st_transform(sf::st_crs(final_predicted_slash_piles))) %>% 
      sf::st_drop_geometry() %>% 
      dplyr::pull(pred_id)
    )
  )
```

what did we get?

```{r}
# huh?
final_predicted_slash_piles %>% dplyr::glimpse()
# final_predicted_slash_piles %>%
#   sf::st_drop_geometry() %>% 
#   dplyr::count(inrange_th_votes)
```

how many piles were removed?

```{r}
# how many piles were removed?
nrow(slash_pile_detect_watershed_ans)-nrow(final_predicted_slash_piles)
# what proportion were removed?
scales::percent(
  (nrow(slash_pile_detect_watershed_ans)-nrow(final_predicted_slash_piles))/nrow(slash_pile_detect_watershed_ans)
  , accuracy=0.1
)
```

### Instance Match & Accuracy Assessment

now apply the instance matching process we outlined in this [earlier section](#iou_match) to establish True Positives (TP), False Positives (FP, commissions), and False Negatives (FN, omissions)

```{r}
# ground truth and prediction matching process
ground_truth_prediction_match_ans <- ground_truth_prediction_match(
  ground_truth = 
    slash_piles_polys %>% 
    dplyr::filter(is_in_stand) %>% 
    dplyr::arrange(desc(field_diameter_m)) %>% 
    sf::st_transform(sf::st_crs(final_predicted_slash_piles))
  , gt_id = "pile_id"
  , predictions = final_predicted_slash_piles %>% dplyr::filter(is_in_stand)
  , pred_id = "pred_id"
  , min_iou_pct = 0.05
)
# add data from gt and pred piles
ground_truth_prediction_match_ans <-
  ground_truth_prediction_match_ans %>% 
  # add area of gt
  dplyr::left_join(
    slash_piles_polys %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(is_in_stand) %>% 
      dplyr::select(
        pile_id
        , image_gt_area_m2
        , image_gt_diameter_m
        , field_gt_volume_m3
        , height_m
        , field_diameter_m
      ) %>% 
      dplyr::rename(
        gt_height_m = height_m
        , gt_diameter_m = field_diameter_m
        , gt_area_m2 = image_gt_area_m2
        , gt_volume_m3 = field_gt_volume_m3
      ) %>% 
      dplyr::mutate(pile_id=as.numeric(pile_id))
    , by = "pile_id"
  ) %>% 
  # add info from predictions
  dplyr::left_join(
    slash_pile_detect_watershed_ans %>%
      sf::st_drop_geometry() %>%
      dplyr::select(
        pred_id
        , area_m2, volume_m3, max_height_m, diameter_m
      ) %>% 
      dplyr::rename(
        pred_area_m2 = area_m2, pred_volume_m3 = volume_m3
        , pred_height_m = max_height_m, pred_diameter_m = diameter_m
      )
    , by = dplyr::join_by(pred_id)
  ) %>%
  dplyr::mutate(
    ### calculate these based on the formulas below...agg_ground_truth_match() depends on those formulas
    # ht diffs
    diff_height_m = pred_height_m-gt_height_m
    , pct_diff_height_m = (gt_height_m-pred_height_m)/gt_height_m
    # diameter
    , diff_field_diameter_m = pred_diameter_m-gt_diameter_m
    , pct_diff_field_diameter_m = (gt_diameter_m-pred_diameter_m)/gt_diameter_m
    # image diameter
    , diff_image_diameter_m = pred_diameter_m-image_gt_diameter_m
    , pct_diff_image_diameter_m = (image_gt_diameter_m-pred_diameter_m)/image_gt_diameter_m
    # area diffs
    , diff_area_m2 = pred_area_m2-gt_area_m2
    , pct_diff_area_m2 = (gt_area_m2-pred_area_m2)/gt_area_m2
    
  )
# huh?
ground_truth_prediction_match_ans %>% dplyr::glimpse()
# diff_diameter_m > diff_field_diameter_m
# pct_diff_diameter_m > pct_diff_field_diameter_m
```

Now we'll aggregate the instance matching results to calculate overall performance assessment metrics. Here, we take the counts of True Positives (TP), False Positives (FP, commissions), and False Negatives (FN, omissions), to determine overall accuracy. This aggregation will give us two types of results: 

* [detection accuracy metrics](#detect_metrics_form): such as Recall, Precision, and F-score, are calculated directly by aggregating these raw TP, FP, and FN counts and quantifies the method's ability to find the piles
* [quantification accuracy metrics](#quant_metrics_form): such as RMSE, MAPE, and Mean Error of pile form measurements (e.g. height, diameter) are calculated by aggregating the differences between the estimated pile attributes and the ground truth values for instances classified as True Positives. These metrics tell us about the method's ability to accurately quantify the form of the piles it successfully identified

```{r}
agg_ground_truth_match_ans <- agg_ground_truth_match(ground_truth_prediction_match_ans = ground_truth_prediction_match_ans)
# huh?
# agg_ground_truth_match_ans %>% dplyr::glimpse()
```

let's table the relevant accuracy metrics

```{r}
kbl_agg_gt_match <- function(
  agg_ground_truth_match_df
  , caption = "pile detection and form quantification accuracy metrics"
) {
  # let's table the most relevant metrics
  agg_ground_truth_match_df %>% 
  # first select to arrange eval_metric
  dplyr::select(
    # detection cnt
    tp_n, fn_n, fp_n
    # detection
    , f_score, recall, precision
    # quantification
    , tidyselect::ends_with("_mean")
    , tidyselect::ends_with("_rmse")
    # , tidyselect::ends_with("_rrmse")
    , tidyselect::ends_with("_mape")
  ) %>% 
  # second select to arrange pile_metric
  dplyr::select(
    # detection cnt
    tp_n, fn_n, fp_n
    # detection
    , f_score, recall, precision
    # quantification
    , c(tidyselect::contains("volume") & !tidyselect::contains("paraboloid"))
    , tidyselect::contains("area")
    , tidyselect::contains("height")
    , tidyselect::contains("diameter")
  ) %>% 
  dplyr::mutate(
    dplyr::across(
      .cols = c(f_score, recall, precision, tidyselect::ends_with("_mape"))
      , .fn = ~ scales::percent(.x, accuracy = 1)
    )
    , dplyr::across(
      .cols = c(tidyselect::ends_with("_mean"))
      , .fn = ~ scales::comma(.x, accuracy = 0.01)
    )
    , dplyr::across(
      .cols = c(tidyselect::ends_with("_rmse"))
      , .fn = ~ scales::comma(.x, accuracy = 0.1)
    )
    , dplyr::across(
      .cols = c(tidyselect::ends_with("_n"))
      , .fn = ~ scales::comma(.x, accuracy = 1)
    )
  ) %>% 
  tidyr::pivot_longer(
    cols = c(
      tidyselect::ends_with("_n")
      , f_score, recall, precision
      , tidyselect::ends_with("_rmse")
      , tidyselect::ends_with("_rrmse")
      , tidyselect::ends_with("_mean")
      , tidyselect::ends_with("_mape")
    )
    , names_to = "metric"
    , values_to = "value"
  ) %>% 
  dplyr::mutate(
    eval_metric = metric %>% 
      stringr::str_extract("(_rmse|_rrmse|_mean|_mape|f_score|recall|precision|tp_n|fn_n|fp_n)$") %>% 
      stringr::str_remove_all("_n$") %>% 
      stringr::str_remove_all("_") %>% 
      stringr::str_replace_all("mean","me") %>% 
      toupper() %>% 
      factor(
        ordered = T
        , levels = c("TP","FN","FP", "FSCORE","RECALL","PRECISION", "ME","RMSE","RRMSE","MAPE")
        , labels = c("TP","FN","FP", "F-score","Recall","Precision", "ME","RMSE","RRMSE","MAPE")
      )
    , pile_metric = metric %>% 
      stringr::str_remove("(_rmse|_rrmse|_mean|_mape)$") %>% 
      stringr::str_extract("(paraboloid_volume|volume|area|height|diameter)") %>% 
      dplyr::coalesce("detection") %>% 
      stringr::str_c(
        dplyr::case_when(
          stringr::str_detect(metric,"(field|image)") ~ paste0(" (", stringr::str_extract(metric,"(field|image)"), ")")
          , T ~ ""
        )
      ) %>% 
      stringr::str_replace("area", "area m<sup>2</sup>") %>% 
      stringr::str_replace("volume", "volume m<sup>3</sup>") %>% 
      stringr::str_replace("diameter", "diameter m") %>% 
      stringr::str_replace("height", "height m") %>% 
      stringr::str_to_sentence()
  ) %>% 
  dplyr::mutate(
    pile_metric = dplyr::case_when(
      pile_metric == "Detection" & eval_metric %in% c("TP","FN","FP") ~ "Detection Count"
      , T ~ pile_metric
    )
    , sorter = dplyr::case_when(
      pile_metric=="Detection Count" ~ 1
      , pile_metric=="Detection" ~ 2
      , T ~ 3
    )
  ) %>% 
  dplyr::arrange(sorter, pile_metric, eval_metric) %>% 
  dplyr::select(pile_metric,eval_metric,value) %>% 
  kableExtra::kbl(
    caption = caption
    , col.names = c(
      ".", ""
      , "value"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling(font_size = 12) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
}
```

use the function we just defined to make a nice summary table

```{r}
# do it
kbl_agg_gt_match(
  agg_ground_truth_match_ans
  , caption = "pile detection and form quantification accuracy metrics<br>data fusion PSINF ponderosa pine training site"
)
# save the table for full comparison at the very end
all_agg_ground_truth_match_ans_fp <- file.path("../data/","all_agg_ground_truth_match_ans.gpkg")
# make a function to convert the sf data to the same format for all
all_agg_ground_truth_sf_format <- function(stand_boundary, site = "you didn't name this site") {
  ret <- stand_boundary %>% 
    sf::st_union() %>% 
    sf::st_sf() %>% 
    dplyr::mutate(site_area_m2 = sf::st_area(.) %>% as.numeric()) %>% 
    sf::st_centroid() %>% 
    sf::st_sf() %>% 
    sf::st_transform(crs = 5070) %>% 
    dplyr::mutate(site = site)
  return(ret)
}
# write data
all_agg_ground_truth_sf_format(
  stand_boundary = stand_boundary
  , site = "PSINF ponderosa pine training site"
) %>% 
dplyr::bind_cols(
  agg_ground_truth_match_ans
  # join on aggregated form quantifications that we have for all
  , ground_truth_prediction_match_ans %>% 
    dplyr::ungroup() %>% 
    dplyr::summarise(
      dplyr::across(
        c(image_gt_diameter_m, pred_diameter_m, gt_area_m2, pred_area_m2, pred_volume_m3, pred_height_m)
        , ~ sum(.x, na.rm = TRUE)
      )
    )
  ) %>% 
  # dplyr::glimpse()
  # readr::write_csv(file = all_agg_ground_truth_match_ans_fp, append = F, progress = F)
  sf::st_write(dsn = all_agg_ground_truth_match_ans_fp, append = F, quiet = T)
```

let's compare these with the predicted accuracies from the model

```{r}
best_balanced_accuracy_combos %>% 
  # get rid of vars we fixed
  dplyr::select(-c(max_ht_m, max_area_m2)) %>% 
  dplyr::mutate(
    spectral_weight = spectral_weight %>% as.character() %>% as.numeric()
  ) %>% 
  tidyr::pivot_longer(cols = -c(.draw,method_input_data)) %>% 
  dplyr::ungroup() %>% 
  # filter for accuracy metrics
  dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("_mape", "_MAPE") %>% 
      stringr::str_replace_all("f_score", "F-score") %>% 
      stringr::str_replace_all("_", " ") %>% 
      stringr::str_squish() %>% 
      factor(ordered = T) %>% 
      forcats::fct_relevel("F-score")
  ) %>% 
  dplyr::filter(method_input_data == "structural+spectral") %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value)) +
  tidybayes::stat_dotsinterval(
    point_interval = "median_hdci", .width = c(0.99)
    , quantiles = 100
    , point_size = 3
  ) + 
  ggplot2::geom_vline(
    data = 
      agg_ground_truth_match_ans %>% 
      dplyr::select(f_score,pct_diff_field_diameter_m_mape,pct_diff_height_m_mape) %>% 
      dplyr::rename(
        diameter_mape=pct_diff_field_diameter_m_mape
        , height_mape=pct_diff_height_m_mape
      ) %>% 
      tidyr::pivot_longer(cols = dplyr::everything()) %>% 
      dplyr::ungroup() %>% 
      # filter for accuracy metrics
      dplyr::filter(name=="f_score" | stringr::str_ends(name, "_mape")) %>% 
      dplyr::mutate(
        name = name %>% 
          stringr::str_replace_all("_mape", "_MAPE") %>% 
          stringr::str_replace_all("f_score", "F-score") %>% 
          stringr::str_replace_all("_", " ") %>% 
          stringr::str_squish() %>% 
          factor(ordered = T) %>% 
          forcats::fct_relevel("F-score")
      )
    , mapping = ggplot2::aes(xintercept=value)
    , color = "navy", lwd = 2, alpha = 0.7
  ) +
  ggplot2::facet_grid(
    cols = dplyr::vars(name)
    , scales = "free_x"
    , axes = "all_x"
  ) +
  ggplot2::scale_y_continuous(NULL, breaks = NULL) +
  ggplot2::scale_x_continuous(labels = scales::percent, expand = ggplot2::expansion(mult = c(1.1,1.1))) +
  ggplot2::labs(
    x=""
    , subtitle = paste0(
      "Data Fusion predictions with actual results in blue"
     , "\npredicted pile detection and form quantification accuracy metrics"
     , "\nusing optimal parameter settings based on both detection and quantification accuracy"
    )
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 10, color = "black", face = "bold")
    , axis.text.x = ggplot2::element_text(size = 8, angle = 90)
  )

```

pretty close!

let's now look at the summary stats of ground truth piles

```{r}
kbl_form_sum_stats(
  slash_piles_polys %>% dplyr::filter(is_in_stand) %>% dplyr::select(!tidyselect::contains("volume_m3"))
  , caption = "Ground Truth Piles: summary statistics for form measurements<br>PSINF ponderosa pine training site"
)
```

and let's look at the summary stats of the predicted piles

```{r}
kbl_form_sum_stats(
  final_predicted_slash_piles %>% dplyr::filter(is_in_stand)
  , caption = "Predicted Piles: summary statistics for form measurements<br>PSINF ponderosa pine training site"
)
```

let's look at these on the RGB

```{r, fig.width = 10.5 , fig.height = 9}
# plot it
ortho_plt_fn(my_ortho_rast = ortho_rast, stand = stand_boundary %>% sf::st_transform(sf::st_crs(ortho_rast)), buffer = 10) +
# ggplot2::ggplot() +
  ggplot2::geom_sf(data = stand_boundary %>% sf::st_transform(sf::st_crs(ortho_rast)), fill = NA, color = "black", lwd = 0.8) +
  ggplot2::geom_sf(
    data = 
      slash_piles_polys %>% 
      dplyr::filter(is_in_stand) %>% 
      dplyr::left_join(
        ground_truth_prediction_match_ans %>% 
          dplyr::select(pile_id,match_grp)
        , by = "pile_id"
      ) %>% 
      sf::st_transform(sf::st_crs(ortho_rast))
    , mapping = ggplot2::aes(fill = match_grp)
    , color = NA ,alpha=0.6
  ) + 
  ggplot2::geom_sf(
    data =
      final_predicted_slash_piles %>% 
      dplyr::filter(is_in_stand) %>% 
      dplyr::left_join(
        ground_truth_prediction_match_ans %>%
          dplyr::select(pred_id,match_grp)
        , by = "pred_id"
      ) %>% 
      sf::st_transform(sf::st_crs(ortho_rast))
    , mapping = ggplot2::aes(fill = match_grp, color = match_grp)
    , alpha = 0
    , lwd = 0.3
  ) +
  ggplot2::scale_fill_manual(values = pal_match_grp, name = "") +
  ggplot2::scale_color_manual(values = pal_match_grp, name = "") +
  ggplot2::theme(legend.position = "top") +
  ggplot2::guides(
    fill = ggplot2::guide_legend(override.aes = list(color = c(NA,NA,pal_match_grp["commission"])))
    , color = "none"
  )
```

there are many TP matches there!

```{r}
agg_ground_truth_match_ans %>% 
  dplyr::select(tidyselect::ends_with("_n"))
```

let's look at some examples on our RGB image

**commissions (false positives)**

predicted pile outlined in brown

```{r, fig.height=10, fig.width=8.5}
plts_temp <-
  which(ground_truth_prediction_match_ans$match_grp %in% c("commission")) %>% 
  sample( min(16,agg_ground_truth_match_ans$fp_n) ) %>% 
  purrr::map(function(x){
    dta <- ground_truth_prediction_match_ans %>% dplyr::slice(x)
    pr <- final_predicted_slash_piles %>% dplyr::filter(pred_id==dta$pred_id)
    #plt
    ortho_plt_fn(my_ortho_rast=ortho_rast, stand=sf::st_union(pr), buffer=6) +
      ggplot2::geom_sf(data = pr, fill = NA, color = "brown", lwd = 0.5)
  })
# combine
patchwork::wrap_plots(
  plts_temp
  , ncol = 4
)
```

there are rocks, shadows, root bundles, downed trees with branches...

**omissions (false negatives)**

actual piles outlined in blue

```{r, fig.height=8, fig.width=8.5}
plts_temp <-
  which(ground_truth_prediction_match_ans$match_grp %in% c("omission")) %>% 
  sample( min(16,agg_ground_truth_match_ans$fn_n) ) %>% 
  purrr::map(function(x){
    dta <- ground_truth_prediction_match_ans %>% dplyr::slice(x)
    gt <- slash_piles_polys %>% dplyr::filter(pile_id==dta$pile_id)

    #plt
    ortho_plt_fn(my_ortho_rast=ortho_rast, stand=sf::st_union(gt), buffer=6) +
      ggplot2::geom_sf(data = gt, fill = NA, color = "blue", lwd = 0.6)
  })
# combine
patchwork::wrap_plots(
  plts_temp
  , ncol = 4
)
```

for the most part, those are irregularly shaped or have unexpected spectral signatures (e.g. very white or entirely in dark shadows). however, the machine piles might have been larger than our expected area threshold `max_area_m2` or taller than our expected height threshold `max_ht_m`

### Volume Comparison{#training_detect_final_volcomp}

The ground truth dataset used for developing and tuning all slash pile detection parameters includes direct data for field-measured height, field-measured diameter, and image-annotated area (based on pile perimeters). Accuracy and error metrics, such as ME, RMSE, and MAPE, will be only calculated for these direct measurements.

We excluded quantification accuracy metrics for derived volume because the resulting value would not constitute a true "error". Comparing our predicted volume to a volume that was *not* directly measured, but instead calculated using a geometric assumption (like assuming a perfectly circular base and paraboloid shape) would be inappropriate. This is because any resulting difference between the prediction and the ground truth would be a blend of three inseparable factors: the error of the remote-sensing prediction method, the error in the direct field measurements (diameter/height), and the error introduced by the geometric shape assumption. Reporting such combined errors would be misleading, as it would be impossible to isolate the true performance of our remote-sensing method alone.

Instead, data involving derived values of volume based on field measurements and a shape assumption and its comparison to our irregularly shaped CHM-derived volume will be treated simply as data points for insight into the differences. Using geometric shape assumptions for estimating pile volume is the standard practice when implementing prescriptions or preparing for slash pile burning ([Hardy 1996](https://permanent.fdlp.gov/gpo45282/index.htm); [Long & Boston 2014](https://doi.org/10.5849/forsci.13-501)). This comparison will help us understand the discrepancy between our irregularly shaped CHM-derived volume and the volume calculated assuming a perfectly circular base and paraboloid shape with field-measured height and diameter. This approach will still provide valuable context about the impact of the perfectly circular base and paraboloid geometric assumptions without falsely attributing the error of the simplified model to the remote-sensing method itself.

let's do that now

* **field-measured piles** 
  - **volume** assumes a paraboloid shape, with volume calculated using the field-measured diameter (as the width) and height. we'll refer to this as "Allometric Field Volume" to indicate the field measurement is derived using a shape assumption.
* **predicted piles** 
  - **volume** calculated from the elevation profile of the irregular predicted pile footprint, without assuming a specific geometric shape. we'll refer to this as "Predicted Volume" to indicate the predicted measurement is from our CHM-based detection methodology
  
We would generally expect that the allometric field volume is larger than the predicted volume because the allometric calculation assumes a perfectly regular geometric shape (circular base and paraboloid) based on maximum field dimensions (height and diameter). this process effectively encloses the actual, irregular pile form within a simplified geometric dome which inherently neglects and sits above the actual irregularities and voids in the pile structure, likely leading to an overestimation of the volume.
  
we already added volume measurements to the TP matches for both the ground truth and predicted piles, summary of that data

```{r}
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::select(gt_volume_m3, pred_volume_m3) %>% 
  summary()
```

those don't really look like they match up well...let's explore

```{r}
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::mutate(diff_volume_m3 = gt_volume_m3 - pred_volume_m3) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y = gt_volume_m3, x = pred_volume_m3)) +
  ggplot2::geom_abline(lwd = 1.5) +
  # ggplot2::geom_point(ggplot2::aes(color = diff_volume_m3)) +
  ggplot2::geom_point(color = "navy") +
  ggplot2::geom_smooth(method = "lm", se=F, color = "tomato", linetype = "dashed") +
  ggplot2::scale_color_viridis_c(option = "mako", direction = -1, alpha = 0.8) +
  ggplot2::scale_x_continuous(limits = c(0, max( max(ground_truth_prediction_match_ans$pred_volume_m3,na.rm=T), max(ground_truth_prediction_match_ans$gt_volume_m3,na.rm=T) ) )) +
  ggplot2::scale_y_continuous(limits = c(0, max( max(ground_truth_prediction_match_ans$pred_volume_m3,na.rm=T), max(ground_truth_prediction_match_ans$gt_volume_m3,na.rm=T) ) )) +
  ggplot2::labs(
    y = latex2exp::TeX("allometric field volume $m^3$")
    , x = latex2exp::TeX("predicted volume $m^3$")
    # , color = "image-field\ndiameter diff."
    , subtitle = latex2exp::TeX("bulk volume ($m^3$) comparison")
  ) +
  ggplot2::theme_light()
```

this is exactly what we expected: for true positive matches, there is a clear systematic difference with the plot showing that the volume calculated using the idealized, regular shape assumption (allometric field volume) is consistently larger than the predicted volume derived from the CHM

let's check these using `lm()`

```{r}
lm_temp <- lm(gt_volume_m3 ~ pred_volume_m3, data = ground_truth_prediction_match_ans %>% dplyr::filter(match_grp=="true positive"))
summary(lm_temp)
```

These linear model results (intercept = `r scales::comma(lm_temp$coefficients[1], accuracy = 0.01)`, slope = `r scales::comma(lm_temp$coefficients[2], accuracy = 0.01)`) indicate a strong proportional bias that significantly increases with pile size. The high slope (`r scales::comma(lm_temp$coefficients[2], accuracy = 0.01)`) coupled with the negative intercept (`r scales::comma(lm_temp$coefficients[1], accuracy = 0.01)`) indicate that the volume difference is not a simple constant offset (e.g. slope of ~1.0 and intercept of >0 if our hypothesis of consistently higher allometric field volume is true), but rather a scaling issue that is driven by the largest piles. The much larger allometric field volume estimates relative to the CHM-predicted volumes for the largest piles exert a strong influence on the predicted form of the liner model, pulling the slope steeply upward and forcing the intercept below zero as a mathematical artifact. Despite the predicted negative intercept, visual inspection of the data shows that most allometric field volumes are larger than the CHM-predicted volumes, even for smaller piles. The slope value indicates that for every 1 m^3^ increase in predicted volume, the allometric field volume increases by nearly `r scales::comma(lm_temp$coefficients[2], accuracy = 0.01)` m^3^. This data suggests that the geometric assumptions of the allometric model potentially introduce substantial scaling error which may limit its reliability (especially for larger piles) for accurately estimating the volume of real-world piles which have heterogeneous footprints and elevation profiles.

before we compare the volume measurements in aggregate, let's look at the distributions

```{r}
vol_df_temp <- 
  ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::select(pile_id,gt_volume_m3,pred_volume_m3) %>% 
  tidyr::pivot_longer(cols = -c(pile_id)) %>% 
  dplyr::mutate(
    name = factor(
      name
      , ordered = T
      , levels = c("gt_volume_m3","pred_volume_m3")
      , labels = c(
        "allometric field volume"
        , "predicted volume"
      )
    )
  ) 
# plot dist
vol_df_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value, color = name, fill = name)) +
  ggplot2::geom_density(mapping = ggplot2::aes(y=ggplot2::after_stat(scaled)), alpha = 0.7) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  harrypotter::scale_fill_hp_d(option = "lunalovegood") +
  ggplot2::scale_y_continuous(NULL,breaks=NULL) +
  ggplot2::labs(
    color="",fill="",x=latex2exp::TeX("volume $m^3$")
    , subtitle = latex2exp::TeX("bulk volume ($m^3$) comparison of distributions")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  )
```

slope plots are neat too

```{r}
vol_df_temp %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = name, y = value, group = pile_id)
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_point(mapping = ggplot2::aes(color = name), alpha = 0.7, size = 2.5) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  ggplot2::labs(
    color=""
    , y = latex2exp::TeX("volume $m^3$")
    , x = ""
    , subtitle = latex2exp::TeX("bulk volume ($m^3$) comparison at the pile level")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.title = ggplot2::element_text(size = 10)
    , axis.text = ggplot2::element_text(size = 10)
  )
```

what if we only look at the smaller piles?

```{r}
vol_df_temp %>% 
  dplyr::filter(
    value < quantile(vol_df_temp$value, probs = 0.938)
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = name, y = value, group = pile_id)
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_point(mapping = ggplot2::aes(color = name), alpha = 0.7, size = 2.5) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  ggplot2::labs(
    color=""
    , y = latex2exp::TeX("volume $m^3$")
    , x = ""
    , subtitle = latex2exp::TeX("bulk volume ($m^3$) comparison at the pile level for the smaller piles")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.title = ggplot2::element_text(size = 10)
    , axis.text = ggplot2::element_text(size = 10)
  )
```

let's compare aggregated volume measurements for the true positive matches

**Mean Difference (MD):** 
$$\text{MD} = \frac{1}{N} \sum_{i=1}^{N} (\text{Allometric Volume}_i - \text{Predicted Volume}_i)$$

**Percent Mean Difference:** 
$$\%\text{MD} = \frac{\text{MD}}{\text{Mean}(\text{Predicted Volume})} \times 100$$

```{r}
vol_agg_df_temp <- 
  ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::ungroup() %>% 
  dplyr::summarise(
    mean_diff = mean(gt_volume_m3-pred_volume_m3)
    , sd_diff = sd(gt_volume_m3-pred_volume_m3)
    , mean_gt_volume_m3 = mean(gt_volume_m3,na.rm = T)
    , mean_pred_volume_m3 = mean(pred_volume_m3,na.rm = T)
  ) %>% 
  dplyr::mutate(
    pct_mean_diff = mean_diff/mean_pred_volume_m3
  )
```

what did we get?

```{r}
vol_agg_df_temp %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    value = 
      dplyr::case_when(
        stringr::str_starts(name, "pct_") ~ scales::percent(value, accuracy = 0.1)
        , T ~ scales::comma(value, accuracy = 0.1)
      )
  ) %>% 
  kableExtra::kbl(
    caption = "comparison of aggregated allometric field volume and predicted volume"
    , col.names = c("metric", "value")
  ) %>% 
  kableExtra::kable_styling()
```

we'll dig into the MD shortly but before we move on let's focus on the percent mean difference. We calcualted a %MD of `r scales::percent(vol_agg_df_temp$pct_mean_diff, accuracy = 0.1)` which indicates a major systematic difference where the allometric field volume is, on average, `r scales::percent(vol_agg_df_temp$pct_mean_diff, accuracy = 0.1)` larger than our CHM-predicted volume. This large relative difference shows how much the geometric assumptions inflate the volume compared to the irregular volumes measured by our remote sensing-based method.

let's make a Bland-Altman plot to compare the two measurement methods. this plot uses the average of the two measurements (approximate size) on the x-axis and the difference (bias) between the two measurements on the y-axis

```{r}
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>%
  dplyr::ungroup() %>% 
  # calc needed metrics
  dplyr::mutate(
    mean_vol = (gt_volume_m3+pred_volume_m3)/2
    , diff_vol = (gt_volume_m3-pred_volume_m3) # match the order used in vol_agg_df_temp
    , scale_diff = ifelse(diff_vol < 0, -abs(diff_vol) / abs(min(diff_vol)), diff_vol / max(diff_vol))
  ) %>% 
  # ggplot() + geom_point(aes(x=diff_vol,y=0, color=scale_diff)) + scale_color_gradient2(mid = "gray", midpoint = 0, low = "red", high = "blue")
  # plot
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = mean_vol, y = diff_vol)
  ) +
  ggplot2::geom_hline(yintercept = 0, color = "black", lwd = 1.2) +
  # mean difference (bias)
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff
    , linetype = "dashed", color = "blue", lwd = 1
  ) +
  # upper limit
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff
    , linetype = "dotted", color = "red", lwd = 1
  ) +
  # lower limit
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff
    , linetype = "dotted", color = "red", lwd = 1
  ) +
  # annotations
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff
    , label = latex2exp::TeX(
      paste0(
        "mean difference (bias): "
        , scales::comma(vol_agg_df_temp$mean_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = -0.5
    , hjust = 1
    , color = "blue"
    , size = 4
    , parse = TRUE
  ) +
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff
    , label = latex2exp::TeX(
      paste0(
        "+1.96 SD: "
        , scales::comma(vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = -0.5
    , hjust = 1
    , color = "red"
    , size = 4
    , parse = TRUE
  ) +
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff
    , label = latex2exp::TeX(
      paste0(
        "-1.96 SD: "
        , scales::comma(vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = 1.5
    , hjust = 1
    , color = "red"
    , size = 4
    , parse = TRUE
  ) +
  # points
  ggplot2::geom_point(mapping = ggplot2::aes(color = scale_diff), size = 1.9, alpha = 0.8) +
  ggplot2::scale_color_steps2(mid = "gray", midpoint = 0) +
  ggplot2::labs(
    subtitle = "Bland-Altman plot: allometric field volume vs predicted volume"
    , x = latex2exp::TeX("mean volume ($m^3$)")
    , y = latex2exp::TeX("difference (allometric - predicted volume $m^3$)")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(legend.position = "none")
```

That's a lot of plotting to show that the mean difference is `r scales::comma(vol_agg_df_temp$mean_diff, accuracy = 0.01)` m^3^. Points falling outside the 95% interval on the plot are instances of significant disagreement between the two volume measurements for those specific data points. These outliers indicate that, for a particular pile, the difference between the allometric field volume and the predicted volume is unusually large, suggesting a potential failure in either the CHM segmentation process, the quality of the original field measurements, the geometric shape assumption, or a combination thereof. We should investigate these extreme disagreements further to see what is happening

before we do that, let's use a paired t-test to determine if the mean difference (MD) between the allometric field volume and the predicted volume is statistically significant (i.e. significantly different from zero)

```{r}
# is the mean difference between the two volumes significantly different from zero
ttest_temp <- t.test(
  ground_truth_prediction_match_ans %>% 
    dplyr::filter(match_grp == "true positive") %>% 
    dplyr::pull(gt_volume_m3)
  , ground_truth_prediction_match_ans %>% 
    dplyr::filter(match_grp == "true positive") %>% 
    dplyr::pull(pred_volume_m3)
  , paired = TRUE
)
ttest_temp
```

that's neat, the test gave us the same mean difference (MD) of `r scales::comma(vol_agg_df_temp$mean_diff, accuracy = 0.01)` m^3^ that we calculated above. also, the p-value of `r scales::comma(ttest_temp$p.value, accuracy = 0.00001)` is less than 0.05, meaning we should reject the null hypothesis that the true mean difference is zero. this confirms that the systematic difference (or bias) we observed where allometric volume is larger than our predicted volume is statistically significant and not due to random chance.

#### Extreme Volume Disagreements

let's investigate the extreme disagreements further to see what is happening 

```{r}
bad_vol_df_temp <- 
  ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>%
  dplyr::ungroup() %>% 
  # calc needed metrics
  dplyr::mutate(
    diff_vol = (gt_volume_m3-pred_volume_m3) # match the order used in vol_agg_df_temp
  ) %>% 
  dplyr::filter(
    diff_vol < (vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff)
    | diff_vol > (vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff)
  ) %>% 
  dplyr::left_join(
    slash_piles_polys %>% 
      sf::st_drop_geometry() %>% 
      dplyr::select(pile_id, comment) %>% 
      dplyr::rename(pile_type = comment)
    , by = "pile_id"
  )
# what are the differences?
bad_vol_df_temp %>% 
  dplyr::select(
    pile_id, pile_type
    , gt_height_m, pred_height_m, diff_height_m
    , gt_diameter_m, pred_diameter_m, diff_field_diameter_m
    , gt_volume_m3, pred_volume_m3
  ) %>% 
  dplyr::mutate(
    dplyr::across(
      .cols = -c(pile_id,pile_type)
      , .fns = ~ scales::comma(.x,accuracy=0.01)
    )
  ) %>% 
  kableExtra::kbl(
    caption = "Volume measurement outliers: comparison of ground truth and predicted piles"
    # , col.names = c("metric", "value")
  ) %>% 
  kableExtra::kable_styling(font_size = 9.8)
```

All these instances of extreme volume disagreement at the PSINF ponderosa pine training site are specific to mechanical piles and it looks like the field-measured height is much different than the CHM height. This suggests that the optimal `max_ht_m` parameter determined during sensitivity and statistical testing was set too low to properly include the upper extent of these taller machine piles. The parameter was optimized this way because the training data featured a much higher frequency of smaller, shorter hand piles, causing the statistical tuning to be inherently biased toward those dominant smaller objects. This finding highlights the difficulty of creating a single-stage process capable of simultaneously detecting distinct pile forms such as small hand piles and larger machine piles. A workaround for this challenge is to execute the pile detection process in two stages: the first parameterized specifically for the expected size, form, and spectral signature of the hand piles, and a second stage parameterized for the expectations of the larger machine piles and then combine the predictions into a final prediction set.

```{r, fig.height=9.2}
bad_vol_df_temp %>% 
  dplyr::select(
    pile_id
    , gt_height_m, pred_height_m
    , gt_diameter_m, pred_diameter_m
    , gt_volume_m3, pred_volume_m3
  ) %>% 
  tidyr::pivot_longer(
    cols = -c(pile_id)
    , names_to = "metric"
    , values_to = "value"
  ) %>% 
  dplyr::mutate(
    which_data = dplyr::case_when(
        stringr::str_starts(metric,"gt_") ~ "ground 'truth'"
        , stringr::str_starts(metric,"pred_") ~ "prediction"
        , T ~ "error"
      ) %>% 
      ordered()
    , pile_metric = metric %>% 
      stringr::str_remove("(_rmse|_rrmse|_mean|_mape)$") %>% 
      stringr::str_extract("(paraboloid_volume|volume|area|height|diameter)") %>% 
      factor(
        ordered = T
        , levels = c(
          "volume"
          , "paraboloid_volume"
          , "area"
          , "height"
          , "diameter"
        )
        , labels = c(
          "Volume (m3)"
          , "Volume paraboloid"
          , "Area (m2)"
          , "Height (m)"
          , "Diameter (m)"
        )
      )
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = which_data, y = value, label = scales::comma(value,accuracy=0.1), group = pile_id)
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_point(mapping = ggplot2::aes(color = which_data), alpha = 0.8, size = 2.5) +
  ggplot2::scale_color_manual(values = c("blue","brown")) +
  ggplot2::geom_text(
    vjust = -0.25
    , show.legend = FALSE
  ) +
  ggplot2::facet_grid(rows = dplyr::vars(pile_metric), scales = "free_y") +
  ggplot2::scale_y_continuous(labels = scales::comma, expand = ggplot2::expansion(mult = c(0.05,.32))) +
  ggplot2::labs(
    x = "", y = "", color = ""
    , subtitle = "Volume measurement outliers: comparison of measurements"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , strip.text = ggplot2::element_text(size = 11, color = "black", face = "bold")
  ) 
```

RGB with the predicted piles (brown) and the ground-truth piles (blue)

```{r, fig.height=10.8}
# plot RGB
plts_temp <-
  1:nrow(bad_vol_df_temp) %>% 
  purrr::map(function(x){
    dta <- bad_vol_df_temp %>% dplyr::slice(x)
    gt <- slash_piles_polys %>% dplyr::filter(pile_id==dta$pile_id)
    pr <- final_predicted_slash_piles %>% dplyr::filter(pred_id==dta$pred_id)
    #plt
    ortho_plt_fn(my_ortho_rast=ortho_rast, stand=sf::st_union(gt,pr), buffer=6) +
      ggplot2::geom_sf(data = gt, fill = NA, color = "blue", lwd = 0.6) +
      ggplot2::geom_sf(data = pr, fill = NA, color = "brown", lwd = 0.5)
  })
# combine
patchwork::wrap_plots(
  plts_temp
  , ncol = 2
)
ggplot2::ggsave("../data/PFDP_Data/bad_vol.jpg", height = 8.5, width = 8.5)
```

just looking at the RGB, the pile footprints are in good alignment

let's look at the CHM

```{r, fig.height=10.5, fig.width=8.5}
# cloud2raster_ans$chm_rast %>% 
#   terra::clamp(upper = structural_params_settings$max_ht_m, lower = 0, values = F) %>% 
#   terra::plot()
# bad_vol_df_temp %>% dplyr::glimpse()
# plot RGB + CHM
plts_temp <-
  1:nrow(bad_vol_df_temp) %>% 
  # sample(1) %>% 
  purrr::map(function(x){
    dta <- bad_vol_df_temp %>% dplyr::slice(x)
    gt <- slash_piles_polys %>% dplyr::filter(pile_id==dta$pile_id)
    pr <- final_predicted_slash_piles %>% dplyr::filter(pred_id==dta$pred_id)
    #plt
    ortho_plt_fn(my_ortho_rast=ortho_rast, stand=sf::st_union(gt,pr), buffer=6) +
      ggplot2::geom_tile(
        data = cloud2raster_ans$chm_rast %>% 
          terra::crop(
            sf::st_union(gt,pr) %>% 
              sf::st_transform(terra::crs(cloud2raster_ans$chm_rast)) %>% 
              terra::vect()
          ) %>% 
          terra::mask(
            sf::st_union(gt,pr) %>% 
              sf::st_transform(terra::crs(cloud2raster_ans$chm_rast)) %>% 
              terra::vect()
          ) %>% 
          # slice the chm below our desired height
          # this is what slash_pile_detect_watershed() does
          terra::clamp(upper = structural_params_settings$max_ht_m, lower = 0, values = F) %>%
          terra::as.data.frame(xy=T) %>% 
          dplyr::rename(f=3)
        , mapping = ggplot2::aes(x=x,y=y,fill=f)
        , alpha = 0.5
      ) +
      ggplot2::scale_fill_viridis_c(option = "plasma", na.value = "gray",name = "CHM (m)") +
      ggplot2::geom_sf(data = gt, fill = NA, color = "blue", lwd = 0.6) +
      ggplot2::geom_sf(data = pr, fill = NA, color = "brown", lwd = 0.5) +
      ggplot2::labs(
        subtitle = paste0(
          "GT ht: ", round(dta$gt_height_m,1)
          , " | Pred ht: ", round(dta$pred_height_m,1)
          , "\nGT dia: ", round(dta$gt_diameter_m,1)
          , " | Pred dia: ", round(dta$pred_diameter_m,1)
        )
      ) +
      ggplot2::theme(legend.text = ggplot2::element_text(size = 6),legend.title = ggplot2::element_text(size = 6))
  })
# plts_temp
# combine
patchwork::wrap_plots(
  plts_temp
  , ncol = 2
)
```

The predicted diameters generally align well with the field-measured diameters. However, the field-measured heights for some piles appear unusually high compared to the CHM-derived data.

Our pile detection methodology uses a specific height slice of the CHM based on the `max_ht_m` parameter. This process then keeps only predicted piles that meet the height threshold across the *majority of their area*. The `convexity_pct` parameter functions to filter out predictions where excessive pile top area was removed by the CHM slicing and acts as a safeguard against misclassifying tall objects (like trees) as piles that would have the majority of their top removed by the slicing.

After filtering for expected height, our detection methodology calculates the predicted height as the maximum CHM cell value within the expected height range (i.e. up to the `max_ht_m` setting), which effectively caps any actual height values above this threshold. For our training data, we assumed a maximum height of `r scales::comma(structural_params_settings$max_ht_m,accuracy=0.01)`m. The field data contains heights up to `r scales::comma(max(bad_vol_df_temp$gt_height_m),accuracy=0.01)`m for these volume outliers, which would only be expected of the largest machine piles. In these extreme cases, the significant volume difference observed in the outliers is magnified by both potentially incorrect field measurements and the systematic capping of the predicted height.

To mitigate the risk of vastly underpredicted heights for these tall outliers, one can increase the `convexity_pct` parameter. Making this filtering more strict will remove the outlier height predictions and improve height quantification accuracy for true positive matches, but will likely reduce the detection rate as these piles become false negatives (omissions). Alternatively, one could increase the `max_ht_m` setting to be less restrictive with the height filtering; this would have the effect of estimating a more accurate height value for these tall piles but would potentially introduce new false positive predictions that are actually trees (or yurts?). If spectral data is available to filter these after structural segmentation, then this might be the preferred path to improve height quantification accuracy while maintaining detection accuracy. The choice ultimately depends on whether the user prioritizes detection rate or height quantification accuracy.

There is precedent for smoothing the height of piles from aerial data point cloud data. Trofymow et al. ([2013](https://scholar.google.com/scholar?cluster=3556811352833339764&hl=en&as_sdt=0,6)) calculated pile height as the 95th percentile height of the height-normalized points within the pile polygon. This method was chosen after their preliminary analysis showed it best excluded points from isolated logs extending above the pile while retaining the main pile structure. Our method achieves a functionally similar result for the largest piles by capping the maximum CHM value and smoothing out extreme height measurements.

we can look a the full CHM without capping the height to see how the field-measured values compare to the unfiltered CHM height profile within the pile footprint

```{r, fig.height=10.5, fig.width=8.5}
# cloud2raster_ans$chm_rast %>% 
#   terra::clamp(upper = structural_params_settings$max_ht_m, lower = 0, values = F) %>% 
#   terra::plot()
# bad_vol_df_temp %>% dplyr::glimpse()
# plot RGB + CHM
plts_temp <-
  1:nrow(bad_vol_df_temp) %>% 
  # sample(1) %>% 
  purrr::map(function(x){
    dta <- bad_vol_df_temp %>% dplyr::slice(x)
    gt <- slash_piles_polys %>% dplyr::filter(pile_id==dta$pile_id)
    pr <- final_predicted_slash_piles %>% dplyr::filter(pred_id==dta$pred_id)
    #plt
    ortho_plt_fn(my_ortho_rast=ortho_rast, stand=sf::st_union(gt,pr), buffer=6) +
      ggplot2::geom_tile(
        data = cloud2raster_ans$chm_rast %>% 
          terra::crop(
            sf::st_union(gt,pr) %>% 
              sf::st_transform(terra::crs(cloud2raster_ans$chm_rast)) %>% 
              terra::vect()
          ) %>% 
          terra::mask(
            sf::st_union(gt,pr) %>% 
              sf::st_transform(terra::crs(cloud2raster_ans$chm_rast)) %>% 
              terra::vect()
          ) %>% 
          # DON'T slice the chm below our desired height
          # terra::clamp(upper = structural_params_settings$max_ht_m, lower = 0, values = F) %>%
          terra::as.data.frame(xy=T) %>% 
          dplyr::rename(f=3)
        , mapping = ggplot2::aes(x=x,y=y,fill=f)
        , alpha = 0.5
      ) +
      ggplot2::scale_fill_viridis_c(option = "plasma", na.value = "gray",name = "CHM (m)", breaks = scales::breaks_extended(n=7)) +
      ggplot2::geom_sf(data = gt, fill = NA, color = "blue", lwd = 0.6) +
      ggplot2::geom_sf(data = pr, fill = NA, color = "brown", lwd = 0.5) +
      ggplot2::labs(
        subtitle = paste0(
          "GT ht: ", round(dta$gt_height_m,1)
          , " | Pred ht: ", round(dta$pred_height_m,1)
          , "\nGT dia: ", round(dta$gt_diameter_m,1)
          , " | Pred dia: ", round(dta$pred_diameter_m,1)
        )
      ) +
      ggplot2::theme(legend.text = ggplot2::element_text(size = 6),legend.title = ggplot2::element_text(size = 6))
  })
# plts_temp
# combine
patchwork::wrap_plots(
  plts_temp
  , ncol = 2
)


```

Visual inspection of the unfiltered CHM (note the CHM scale range) within the footprint of these volume outliers confirms that the field-measured values are misaligned with the CHM profile by at least 1m for some of the most extreme outliers, supporting the theory that the significant volume and height differences observed are magnified by potentially incorrect field measurements.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```


#### Allometric Volume Comparison

The volume comparison [immediately above](#training_detect_final_volcomp) compared the allometric field volume using a geometric shape assumption (paraboloid) with the predicted volume based on the irregular CHM elevation profile. Thus, the volume difference included the impact of the shape assumption. In an attempt to remove the influence of comparing a regular shape (field measured) to an irregular shape (predicted volume), we'll perform a comparison analysis where both volume calculations are based on the same paraboloid shape assumption. This should provide insight into the differences related only to the predicted versus field-measured maximum dimensions of height and diameter.

In the following section, we compare volume predictions against field measured data where both volume calculations utilize the same paraboloid shape assumption. For this analysis, we'll update the predicted volume only using the following volume definitions: 

* **field-measured piles** 
  - **volume** assumes a paraboloid shape, with volume calculated using the field-measured diameter (as the width) and height. we'll refer to this as "Allometric Field Volume" to indicate the field measurement is derived using a shape assumption.
* **predicted piles** 
  - **volume** assumes a paraboloid shape, with volume calculated using the predicted diameter (as the width) and height. we'll refer to this as "Allometric Predicted Volume" to indicate the measurement is derived using a shape assumption.
  
We would generally expect that the allometric field volume will not be uniformly larger or smaller than the predicted allometric volume, as the volume difference will be the net result of two competing influences: the observed `r ifelse(agg_ground_truth_match_ans$diff_field_diameter_m_mean>=0,"overestimation","underestimation")` of diameter (`r scales::comma(agg_ground_truth_match_ans$diff_field_diameter_m_mean,accuracy=0.01)` m mean error) and the observed `r ifelse(agg_ground_truth_match_ans$diff_height_m_mean>=0,"overestimation","underestimation")` of height (`r scales::comma(agg_ground_truth_match_ans$diff_height_m_mean,accuracy=0.01)` m mean error) propagating through the volume calculation formula.

let's first update the predicted volume to calculate the allometric predicted volume where the volume formula for a paraboloid is:
  
$$
V = \frac{1}{8}\pi \cdot width^2 \cdot height
$$
we expect the allometric predicted volume to be larger than the predicted volume calculated based on the irregular elevation profile in the CHM because assuming a geometric shape smooths all the voids and irregularities ( which more accurately reflect reality ;) )

```{r}
ground_truth_prediction_match_ans <- ground_truth_prediction_match_ans %>% 
  dplyr::mutate(
    pred_allom_volume_m3 = (1/8) * pi * (pred_diameter_m^2) * pred_height_m
  )
# summarize it
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::select(gt_volume_m3, pred_volume_m3, pred_allom_volume_m3) %>% 
  summary()
```

nice, the allometric predicted volume is larger than the predicted volume calculated based on the irregular elevation profile in the CHM 

```{r}
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::mutate(diff_volume_m3 = gt_volume_m3 - pred_allom_volume_m3) %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(y = gt_volume_m3, x = pred_allom_volume_m3)) +
  ggplot2::geom_abline(lwd = 1.5) +
  ggplot2::geom_point(color = "navy") +
  ggplot2::geom_smooth(method = "lm", se=F, color = "tomato", linetype = "dashed") +
  ggplot2::scale_color_viridis_c(option = "mako", direction = -1, alpha = 0.8) +
  ggplot2::scale_x_continuous(limits = c(0, max( max(ground_truth_prediction_match_ans$pred_allom_volume_m3,na.rm=T), max(ground_truth_prediction_match_ans$gt_volume_m3,na.rm=T) ) )) +
  ggplot2::scale_y_continuous(limits = c(0, max( max(ground_truth_prediction_match_ans$pred_allom_volume_m3,na.rm=T), max(ground_truth_prediction_match_ans$gt_volume_m3,na.rm=T) ) )) +
  ggplot2::labs(
    y = latex2exp::TeX("allometric field volume $m^3$")
    , x = latex2exp::TeX("allometric predicted volume $m^3$")
    # , color = "image-field\ndiameter diff."
    , subtitle = latex2exp::TeX("bulk volume ($m^3$) comparison")
  ) +
  ggplot2::theme_light()
```

the differences between the predictions for the larger field measured piles are pulling the slope of the line upwards but for the smaller piles, the volume differences are clustered around the line of equality

before we compare the volume measurements in aggregate, let's look at the distributions

```{r}
vol_df_temp <- 
  ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::select(pile_id,gt_volume_m3,pred_allom_volume_m3) %>% 
  tidyr::pivot_longer(cols = -c(pile_id)) %>% 
  dplyr::mutate(
    name = factor(
      name
      , ordered = T
      , levels = c("gt_volume_m3","pred_allom_volume_m3")
      , labels = c(
        "allometric field volume"
        , "allometric predicted volume"
      )
    )
  ) 
# plot dist
vol_df_temp %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = value, color = name, fill = name)) +
  ggplot2::geom_density(mapping = ggplot2::aes(y=ggplot2::after_stat(scaled)), alpha = 0.7) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  harrypotter::scale_fill_hp_d(option = "lunalovegood") +
  ggplot2::scale_y_continuous(NULL,breaks=NULL) +
  ggplot2::labs(
    color="",fill="",x=latex2exp::TeX("allometric volume $m^3$")
    , subtitle = latex2exp::TeX("Allometric bulk volume ($m^3$) comparison of distributions")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "top"
  )
```

nice, the distributions overlap much more than the volume comparison between the allometric field value and the irregular, CHM-derived prediction

slope plots are neat too

```{r}
vol_df_temp %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = name, y = value, group = pile_id)
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_point(mapping = ggplot2::aes(color = name), alpha = 0.7, size = 2.5) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  ggplot2::labs(
    color=""
    , y = latex2exp::TeX("allometric volume $m^3$")
    , x = ""
    , subtitle = latex2exp::TeX("Allometric bulk volume ($m^3$) comparison at the pile level")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.title = ggplot2::element_text(size = 10)
    , axis.text = ggplot2::element_text(size = 10)
  )
```

what if we only look at the smaller piles?

```{r}
vol_df_temp %>% 
  dplyr::filter(
    value < quantile(vol_df_temp$value, probs = 0.938)
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = name, y = value, group = pile_id)
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_point(mapping = ggplot2::aes(color = name), alpha = 0.7, size = 2.5) +
  harrypotter::scale_color_hp_d(option = "lunalovegood") +
  ggplot2::labs(
    color=""
    , y = latex2exp::TeX("allometric volume $m^3$")
    , x = ""
    , subtitle = latex2exp::TeX("Allometric bulk volume ($m^3$) comparison at the pile level for the smaller piles")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.title = ggplot2::element_text(size = 10)
    , axis.text = ggplot2::element_text(size = 10)
  )
```

there is much more variability in this slope plot than the volume comparison between the allometric field value and the irregular, CHM-derived prediction

let's compare aggregated volume measurements for the true positive matches

```{r}
vol_agg_df_temp <- 
  ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>% 
  dplyr::ungroup() %>% 
  dplyr::summarise(
    mean_diff = mean(gt_volume_m3-pred_allom_volume_m3)
    , sd_diff = sd(gt_volume_m3-pred_allom_volume_m3)
    , mean_gt_volume_m3 = mean(gt_volume_m3,na.rm = T)
    , mean_pred_allom_volume_m3 = mean(pred_allom_volume_m3,na.rm = T)
  ) %>% 
  dplyr::mutate(
    pct_mean_diff = mean_diff/mean_pred_allom_volume_m3
  )
```

what did we get?

```{r}
vol_agg_df_temp %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  dplyr::mutate(
    value = 
      dplyr::case_when(
        stringr::str_starts(name, "pct_") ~ scales::percent(value, accuracy = 0.1)
        , T ~ scales::comma(value, accuracy = 0.1)
      )
  ) %>% 
  kableExtra::kbl(
    caption = "comparison of aggregated allometric field volume and allometric predicted volume"
    , col.names = c("metric", "value")
  ) %>% 
  kableExtra::kable_styling()
```

we'll dig into the MD shortly but before we move on let's focus on the percent mean difference. We calcualted a %MD of `r scales::percent(vol_agg_df_temp$pct_mean_diff, accuracy = 0.1)` which indicates a slight systematic difference where the allometric field volume is, on average, `r scales::percent(vol_agg_df_temp$pct_mean_diff, accuracy = 0.1)` larger than our predicted allometric volume.

let's make a Bland-Altman plot to compare the two measurement methods. this plot uses the average of the two measurements (approximate size) on the x-axis and the difference (bias) between the two measurements on the y-axis

```{r}
ground_truth_prediction_match_ans %>% 
  dplyr::filter(match_grp=="true positive") %>%
  dplyr::ungroup() %>% 
  # calc needed metrics
  dplyr::mutate(
    mean_vol = (gt_volume_m3+pred_allom_volume_m3)/2
    , diff_vol = (gt_volume_m3-pred_allom_volume_m3) # match the order used in vol_agg_df_temp
    , scale_diff = ifelse(diff_vol < 0, -abs(diff_vol) / abs(min(diff_vol)), diff_vol / max(diff_vol))
  ) %>% 
  # ggplot() + geom_point(aes(x=diff_vol,y=0, color=scale_diff)) + scale_color_gradient2(mid = "gray", midpoint = 0, low = "red", high = "blue")
  # plot
  ggplot2::ggplot(
    mapping = ggplot2::aes(x = mean_vol, y = diff_vol)
  ) +
  ggplot2::geom_hline(yintercept = 0, color = "black", lwd = 1.2) +
  # mean difference (bias)
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff
    , linetype = "dashed", color = "blue", lwd = 1
  ) +
  # upper limit
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff
    , linetype = "dotted", color = "red", lwd = 1
  ) +
  # lower limit
  ggplot2::geom_hline(
    yintercept = vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff
    , linetype = "dotted", color = "red", lwd = 1
  ) +
  # annotations
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff
    , label = latex2exp::TeX(
      paste0(
        "mean difference (bias): "
        , scales::comma(vol_agg_df_temp$mean_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = -0.5
    , hjust = 1
    , color = "blue"
    , size = 4
    , parse = TRUE
  ) +
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff
    , label = latex2exp::TeX(
      paste0(
        "+1.96 SD: "
        , scales::comma(vol_agg_df_temp$mean_diff+1.96*vol_agg_df_temp$sd_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = -0.5
    , hjust = 1
    , color = "red"
    , size = 4
    , parse = TRUE
  ) +
  ggplot2::annotate(
    "text"
    , x = Inf
    , y = vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff
    , label = latex2exp::TeX(
      paste0(
        "-1.96 SD: "
        , scales::comma(vol_agg_df_temp$mean_diff-1.96*vol_agg_df_temp$sd_diff, accuracy = 0.01)
        , " $m^3$"
      )
      , output = "character"
    )
    , vjust = 1.5
    , hjust = 1
    , color = "red"
    , size = 4
    , parse = TRUE
  ) +
  # points
  ggplot2::geom_point(mapping = ggplot2::aes(color = scale_diff), size = 1.9, alpha = 0.8) +
  ggplot2::scale_color_steps2(mid = "gray", midpoint = 0) +
  ggplot2::labs(
    subtitle = "Bland-Altman plot: allometric field volume vs allometric predicted volume"
    , x = latex2exp::TeX("mean allometric volume ($m^3$)")
    , y = latex2exp::TeX("difference (allometric field - allometric predicted volume $m^3$)")
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(legend.position = "none")
```

That's a lot of plotting to show that the mean difference is `r scales::comma(vol_agg_df_temp$mean_diff, accuracy = 0.01)` m^3^. This is a significant improvement over our comparison between the allometric field value and the irregular, CHM-derived prediction

let's use a paired t-test to determine if the mean difference (MD) between the allometric field volume and the allometric predicted volume is statistically significant (i.e. significantly different from zero)

```{r}
# is the mean difference between the two volumes significantly different from zero
ttest_temp <- t.test(
  ground_truth_prediction_match_ans %>% 
    dplyr::filter(match_grp == "true positive") %>% 
    dplyr::pull(gt_volume_m3)
  , ground_truth_prediction_match_ans %>% 
    dplyr::filter(match_grp == "true positive") %>% 
    dplyr::pull(pred_allom_volume_m3)
  , paired = TRUE
)
ttest_temp
```

Wow, the p-value of `r scales::comma(ttest_temp$p.value, accuracy = 0.001)` is greater than 0.05 (even 0.10), meaning we *fail* to reject the null hypothesis that the true mean difference is zero. That is, the mean difference between the field-measured volume and the predicted volume is not different than zero when both are forced to use the same paraboloid shape assumption. This contrasts sharply with the previously observed major systematic difference between the allometric field volume and the predicted irregular, CHM-derived volume. The non-significant difference strongly suggests that the major volume discrepancy noted earlier was primarily due to the geometric irregularity of the actual piles, and not to systematic prediction bias in our CHM-derived height and diameter measurements.

### Stand-level Aggregation

before we leave, let's summarize the measurement values of the predictions (true positive and false positive) and the ground truth data (true positive and false negative) over the entire stand (this is similar to a basal area comparison in a forest inventory)

```{r}
sum_df_temp <-
  ground_truth_prediction_match_ans %>% 
  dplyr::mutate(gt_allom_volume_m3 = gt_volume_m3) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(-c(pred_id)) %>% 
  dplyr::summarise(
    dplyr::across(
      .cols = tidyselect::starts_with("gt_") | tidyselect::starts_with("pred_")
      , ~sum(.x,na.rm=T)
    )
  ) %>% 
  tidyr::pivot_longer(
    cols = dplyr::everything()
    , names_to = "metric"
    , values_to = "value"
  ) %>% 
  dplyr::mutate(
    which_data = dplyr::case_when(
        stringr::str_starts(metric,"gt_") ~ "ground truth"
        , stringr::str_starts(metric,"pred_") ~ "prediction"
        , T ~ "error"
      ) %>% 
      ordered()
    , pile_metric = metric %>% 
      stringr::str_remove("(_rmse|_rrmse|_mean|_mape)$") %>% 
      stringr::str_extract("(allom_volume|volume|area|height|diameter)") %>% 
      factor(
        ordered = T
        , levels = c(
          "height"
          , "diameter"
          , "area"
          , "allom_volume"
          , "volume"
        )
        , labels = c(
          "Height (m)"
          , "Diameter (m)"
          , "Area (m2)"
          , "Allometric Volume (m3)"
          , "Irregular Volume (m3)"
        )
      )
  ) %>% 
  dplyr::group_by(pile_metric) %>% 
  dplyr::arrange(pile_metric,which_data) %>% 
  dplyr::mutate(
    pct_diff = (value-dplyr::lag(value))/dplyr::lag(value) 
  ) %>% 
  dplyr::ungroup()
  # dplyr::filter(pile_metric!="Irregular Volume (m3)")
```

plot

```{r, fig.height=9.6}
# plot it
sum_df_temp %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    stand_id=1
    , lab = paste0(
      scales::comma(value,accuracy=0.1)
      , dplyr::case_when(
        is.na(pct_diff) ~ ""
        , T ~ paste0(
          "\n"
          , ifelse(pct_diff<0,"-","+")
          ,scales::percent(abs(pct_diff),accuracy=0.1)
        )
      )
    )
  ) %>% 
  ggplot2::ggplot(
    mapping = ggplot2::aes(
      x = which_data
      , y = value
      , label = lab
      , group = stand_id
    )
  ) +
  ggplot2::geom_line(key_glyph = "point", alpha = 0.7, color = "gray", lwd = 1.1) +
  ggplot2::geom_col(mapping = ggplot2::aes(fill = which_data), alpha = 1, width = 0.4) +
  ggplot2::scale_color_manual(values = c("blue","brown")) +
  ggplot2::scale_fill_manual(values = c("blue","brown")) +
  ggplot2::geom_text(
    vjust = -0.25
  ) +
  ggplot2::facet_wrap(facets = dplyr::vars(pile_metric), scales = "free_y", axes = "all_x") +
  ggplot2::scale_y_continuous(labels = scales::comma, expand = ggplot2::expansion(mult = c(0,.3)), breaks = NULL) +
  ggplot2::labs(
    x = "", y = ""
    , subtitle = "Comparison of aggregated measurements at the stand level"
  ) +
  ggplot2::theme_light() +
  ggplot2::theme(
    legend.position = "none"
    , axis.text.x = ggplot2::element_text(size = 11, color = "black", face = "bold")
    , strip.text = ggplot2::element_text(size = 11, color = "black", face = "bold")
    , panel.grid = ggplot2::element_blank()
  ) 
```

table it

```{r}
sum_df_temp %>% 
  dplyr::select(pile_metric, which_data, value, pct_diff) %>% 
  dplyr::mutate(
    value = scales::comma(value,accuracy=0.1)
    , pct_diff = scales::percent(pct_diff,accuracy=0.1)
  ) %>% 
  kableExtra::kbl(
    caption = "Comparison of aggregated measurements at the stand level"
    , col.names = c(
      ".", "measurement source"
      , "stand-level total", "% difference"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(list = ls()[grep("brms_",ls())])
remove(
  best_balanced_accuracy_combos, best_balanced_accuracy_combos_chm
  , cloud2raster_ans, final_predicted_slash_piles
  , ground_truth_prediction_match_ans, optimal_param_settings
  , ortho_rast, param_combos_ranked, param_combos_spectral_ranked
  , slash_pile_detect_watershed_ans, structural_params_settings
  , agg_ground_truth_match_ans
)
gc()
```
